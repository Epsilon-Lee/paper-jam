
- [Invariant Language Modeling](https://arxiv.org/pdf/2110.08413.pdf), Oct. 16 2021, `language model`
  - To alleviate problems from spurious correlation, poor ood generalization, biases
  - By using IRM (invariant risk minimization), the paper implement a game-theoretic variant of IRM to train subsets of the LM in a round-robin fashion
  - Focus on how they evaluate the trained model!
- [Training Dynamics for Text Summarization Models](https://arxiv.org/pdf/2110.08370.pdf), Oct. 15 2021, `learning dynamics` `summarization`
  - they study what the model learns at different stages of its fune-tuning process
  - hallucinations are learned at later stage of training
- [Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models](https://arxiv.org/pdf/2110.07736.pdf), Oct. 14 2021, `spurious correlation`
  - How to identify shortcuts?
- [Visual Representation Learning Does Not Generalize Strongly within The Same Domain](https://arxiv.org/pdf/2107.08221.pdf), Oct. 4 `analysis` `generalization`
- [Controllable Semantic Parsing via Retrieval Augmentation](https://arxiv.org/pdf/2110.08458.pdf), `emnlp2021` `semantic parsing`
- [Probing as Quantifying the Inductive Bias of Pre-trained Representations](https://arxiv.org/pdf/2110.08388.pdf), Oct. 15 2021, `analysis`
- [Towards Understanding the Data Dependency of Mixup-Style Training](https://arxiv.org/pdf/2110.07647.pdf), Oct. 14 2021 `mixup` `analysis`
  -  "despite seeing very few true data points during training, mixup seems to still minimize empirical risk and exhibit better generalization and robustness"
  -  "this paper investigate how mixup achieve these benefits in the help of the properties of the training data"
  -  margin of mixup classifier
- [Certified Patch Robustness via Smoothed Vision Transformers](https://arxiv.org/pdf/2110.07719.pdf), Oct. 11 2021 `certified robustness`
- [Meta-learning via Language Model In-context Tuning](https://arxiv.org/pdf/2110.07814.pdf), Oct. 15 2021 yet another `prompting`-based scenario.
