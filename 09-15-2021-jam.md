
- [Rationales for Sequential Predictions](https://arxiv.org/pdf/2109.06387.pdf), Keyon Vafa et al. `emnlp2021` `interpretability` `nmt` `combinatorial optimization`
  
  - *Rationales*: subset of context that ...;
  - combinatorial optimization formulation of rationale finding: the best rationale is the smallest subset of input tokens that could predict the same prediction as the orignal ones;
  - how to measure faithfulness?


- [Type of Out-of-Distribution Texts and How to Detect Them](https://arxiv.org/pdf/2109.06827.pdf), Udit Arora et al. `emnlp2021` `ood issue` `analysis`
  
  - Motivation is "there is little consensus on formal def. of OOD examples";
  - Propose a categorization of OOD instances according to ***background shift*** or ***semantic shift***
  - Methods like *calibration* and density estimation for *OOD detection* are evaluated over 14 datasets


- [Uncertainty-Aware Machine Translation Evaluation](https://arxiv.org/pdf/2109.06352.pdf), Taisiya Glushkova et al. `emnlp2021` `nmt` `evaluation`
  
  - Motivation: old works of evaluation metrics are ***point estimation***, "which provides limitd knowledge at segment level"
  - Monte Carlo dropout and deep ensembles are used for collect groups of predictions for uncertainty estimation
  - (no reference) uncertainty-aware quality estimation can locate translation errors


- [Compression, Transduction and Creation: A Unified Framework for Evaluating NLG](https://arxiv.org/pdf/2109.06379.pdf), Mingkai Deng et al. `emnlp2021` `evaluation` `nlg`

  - ***information alignment*** as a technique is used for different NLG tasks, namely, summarization, style transfer and dialog
  - a good point for writing a paper, great as a survey paper

- [The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation](https://arxiv.org/pdf/2109.06835.pdf), Marzena Karpinska et al. `emnlp2021` `evaluation` `nlg`

  - Meta-research on other (45) papers' evaluation part of open-ended text generation
  - Rethinking the faithfulness of text generation by GPT-2 and other big LMs

- [Phrase-BERT Improved Phrase Embeddings from BERT with an Application to Corpus Exploration](https://arxiv.org/pdf/2109.06304.pdf), Shufan Wang et al. `emnlp2021` `sentence embedding` `compositionality` `dataset analysis`

  - Motivation: old phrase-embedding-from-BERT methods lacks compositionality
  - They design a contrastive fine-tuning objective function
  - Phrase-BERT embeddings can be integrated into a neural topic model

- [A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space](https://arxiv.org/pdf/2109.06324.pdf), Alex Jones et al. `multilinguality` `representation analysis` `cross-lingual alignment`

  - What is cross-linguality?
  - What are the for intrinsic measures of cross-linguality?


- [Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation](https://arxiv.org/pdf/2109.06253.pdf), Ivan Provikov et al. `arXiv` `nmt inference`

  - Identification of *dataset length bias* as a key factor for large-beam issue
  - They propose a Data Augmentatio methods aka. multi-sentence resampling to mitigate such problem

- [Improving Zero-shot Cross-lingual Transfer between Closely Related Languages by injecting Character-level Noise], Noemi Aepli et al. `emnlp2021` `zero-shot` `cross-lingual transfer`

- [STraTA: Self-Training with Task Augmentation for Better Few-shot Learning](https://arxiv.org/pdf/2109.06270.pdf), Tu Vu et al. `emnlp2021` `few-shot` `fune-tuning`

  - This paper drives my interest due to this sentence in its abstract "STraTA with only 8 training examples per class achieves comparable results to standard fine-tuning with 67K training examples"

- [LM-Critic: Language Models for Unsupervised Grammatical Error Correction](https://arxiv.org/pdf/2109.06822.pdf), Michihiro Yasunaga et al. `emnlp2021` `Grammar Error Correction` `unsupervised learning`

  - Use a pre-trained LM as critic to judge and estabilish the grammaticality of a sentence
