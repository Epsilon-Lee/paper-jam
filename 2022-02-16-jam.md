
- [From data to functa: Your data point is a function and you should treat it like one](https://arxiv.org/pdf/2201.12204.pdf), Jan. 28 2022 `multimodal`
- [Datamodels: Predicting Predictions from Training Data](https://arxiv.org/pdf/2202.00622.pdf), Feb. 1 2022 `knowledge tracing`
- [Co-training Improves Prompt-based Learning for Large Language Models](https://arxiv.org/pdf/2202.00828.pdf), Feb. 2 `prompt-based learning`
- [Typical Decoding for Natural Language Generation](https://arxiv.org/pdf/2202.00666.pdf), Feb. 10 2022 `text generation` `decoding`
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990.pdf), Feb. 4 2022 `foundation model` `data curation` `training`

- [How does unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis](https://arxiv.org/pdf/2201.08514.pdf), Jan. 25 2022 `self-training` `semi-supervised learning`
- [If your data distribution shifts: use self-learning](https://openreview.net/pdf?id=1oEvY1a67c1), `iclr2022` rejected
- [For self-supervised learning, Rationality implies generalization, provably](https://arxiv.org/abs/2010.08508), Oct. 16 2020 `self-supervised learning`
