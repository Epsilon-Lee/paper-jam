### Survey

- [Which BERT? A Survey Organizing Contextualized Encoders](https://arxiv.org/pdf/2010.00854.pdf), Oct. 2 2020. `emnlp2020`

### Hyper-parameter/Training revisited

- [The MultiBERTs: BERT Reproductions for Robustness Analysis](https://openreview.net/forum?id=K0E_F0gFDgA), `iclr2022 submit`

### Few-shot learning

- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf), Oct. 2021, `T0++`
- [MetaICL: Learning to Learn In Context](https://arxiv.org/pdf/2110.15943.pdf), Oct. 29 2021. [code](https://github.com/facebookresearch/MetaICL).
- [ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](https://openreview.net/forum?id=Vzh1BFUCiIX), `iclr22 submit`
- [Quantifying Adaptability in Pre-trained Language Models with 500 Tasks](https://arxiv.org/pdf/2112.03204.pdf), Dec. 6 2021. Jacob Andreas et al.

### Language-specific PTM

- [GottBERT: a pure German Language Model](https://arxiv.org/abs/2012.02110), Dec. 3 2020.
