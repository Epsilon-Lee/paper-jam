### Pretraining-based NMT

- [Language Models are Good Translators](https://arxiv.org/pdf/2106.13627.pdf), Jun. 25.
- [Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation](https://aclanthology.org/2021.emnlp-main.132.pdf), `emnlp2021`
- [BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation](https://aclanthology.org/2021.emnlp-main.534.pdf), `emnlp2021` [code](https://github.com/fe1ixxu/BiBERT).
- [DEEP: DEnoising Entity Pre-training for Neural Machine Translation](https://arxiv.org/pdf/2111.07393.pdf), Neubig et al. `emnlp2021`


### Data Augmentation for NMT

- [mixSeq: A Simple Data Augmentation Method for Neural Machine Translation](https://aclanthology.org/2021.iwslt-1.23.pdf), IWSLT 2021.
- [BITEXTEDIT: Automatic Bitext Editing for Improved Low-Resource Machine Translation](https://arxiv.org/pdf/2111.06787.pdf), Nov. 12 2021.

### Char NMT

- [Why donâ€™t people use character-level machine translation?](https://arxiv.org/pdf/2110.08191.pdf), `analysis`
- [Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models](https://arxiv.org/pdf/2110.12552.pdf), `evaluation under noise`.

### Multilingual NMT

- [Alternative Input Signals Ease Transfer in Multilingual Machine Translation](https://arxiv.org/pdf/2110.07804.pdf), Oct. 15 2021 `analysis` of transfer ability of multilingual nmt models
- [Breaking Down Multilingual Machine Translation](https://arxiv.org/pdf/2110.08130.pdf), Oct. 15 `analysis` plus improvement methods
- [Tricks for Training Sparse Translation Models](https://arxiv.org/pdf/2110.08246.pdf), Oct. 15 `multitask learning` `unbalanced learning`
- [Multilingual Neural Machine Translation: Can Linguistic Hierarchies Help?](https://arxiv.org/pdf/2110.07816.pdf), Oct. 15, proposed a new training paradigm taking into account the language hierarchy, handling `negative transfer`
- [Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters](https://arxiv.org/abs/2110.09574), Oct. 18 2021. `wmt2021`
- [Continual Learning in Multilingual NMT via Language-Specific Embeddings](https://arxiv.org/abs/2110.10478), Oct. 20 2021. `wmt2021`

### Multilinguality

- [Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference](https://arxiv.org/pdf/2110.03742.pdf), `emnlp2021`
- [Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings](https://arxiv.org/pdf/2110.02887.pdf), `emnlp2021`
- [When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer](https://arxiv.org/pdf/2110.14782.pdf), Oct. 27, `emnlp2021`

### Efficiency

- [ABC: Attention with Bounded-Memory Control](https://arxiv.org/pdf/2110.02488.pdf), `deepmind`

### Low-resource

- [The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation](https://arxiv.org/pdf/2110.03036.pdf), `pruning` `emnlp2021`
- [Phrase-level Active Learning for Neural Machine Translation](https://arxiv.org/abs/2106.11375#), Jun. 21 2021.

### Scaling Up

- [Scaling Laws for Neural Machine Translation](https://arxiv.org/pdf/2109.07740.pdf), Google.
- [Data and Parameter Scaling Laws for Neural Machine Translation](https://aclanthology.org/2021.emnlp-main.478.pdf), JHU, `emnlp2021`

### Quality Estimation

- [Levenshtein Training for Word-level Quality Estimation](https://arxiv.org/pdf/2109.05611.pdf), Shuoyang Ding et al.
- [Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation](https://arxiv.org/pdf/2109.07141.pdf), Alibaba Group.
- [Practical Perspectives on Quality Estimation for Machine Translation](https://arxiv.org/pdf/2005.03519.pdf), CMU & Google Inc. May 2 2020.

### Translationese

- [Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification](https://arxiv.org/pdf/2109.07604.pdf).

### Training Strategy

- [Improving Neural Machine Translation by Bidirectional Training](https://arxiv.org/pdf/2109.07780.pdf), Liang Ding et al.

### Analysis and Evaluation

- [Relations between comprehensibility and adequacy errors in machine translation output](https://aclanthology.org/2020.conll-1.19.pdf), `error analysis`.
- [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://aclanthology.org/2021.acl-long.562.pdf), `acl2021`.
- [Translation Transformers Rediscover Inherent Data Domains](https://arxiv.org/pdf/2109.07864.pdf), 2021.
- [On the Limits of Minimal Pairs in Contrastive Evaluation](https://arxiv.org/pdf/2109.07465.pdf), 2021
- [On Neurons Invariant To Sentence Structural Changes in Neural Machine Translation](https://arxiv.org/pdf/2110.03067.pdf), `iclr`, 2021.
- [Understanding the Impact of UGC Specificities on Translation Quality](https://arxiv.org/pdf/2110.12551.pdf), Oct. 24 2021.
- [On the Limits of Minimal Pairs in Contrastive Evaluation](https://aclanthology.org/2021.blackboxnlp-1.5.pdf), `blackboxnlp 2021`
- [Variance-Aware Machine Translation Test Sets](https://arxiv.org/pdf/2111.04079.pdf), Nov. 7 2021. `nips2021`

### Discourse

- [When Does Translation Require Context? A Data-driven, Multilingual Exploration](https://arxiv.org/pdf/2109.07446.pdf).

### New Paradigm

- [Nearest Neighbor Machine Translation](https://arxiv.org/pdf/2010.00710.pdf), `iclr 2021`
