
- [Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations](https://arxiv.org/pdf/2110.12088.pdf), Oct. 22 2021. `learning under noise`
- [THE EFFICIENCY MISNOMER](https://arxiv.org/pdf/2110.12894.pdf), Oct. 25 2021. `evaluation`
- [GNN-LM: LANGUAGE MODELING BASED ON GLOBAL CONTEXTS VIA GNN](https://arxiv.org/pdf/2110.08743.pdf), `language model`
- [Text Counterfactuals via Latent Optimization and Shapley-Guided Search](https://arxiv.org/pdf/2110.11589.pdf), `counterfactual` `adversarial` `interpretability` `emnlp2021`
- [BRAXLINES: Fast and Interactive Toolkit for RL-driven Behavior Engineering beyond Reward Maximization](https://arxiv.org/pdf/2110.04686.pdf), Oct. 10 2021, `toolkit`
- [Behavioral experiments for understanding catastrophic forgetting](https://arxiv.org/pdf/2110.10570.pdf), Oct. 22 2021 `continual learning`.
- [Shaking the foundations: delusions in sequence models for interaction and control](https://arxiv.org/pdf/2110.10819.pdf), 
- [AUTONLU: DETECTING, ROOT-CAUSING, AND FIXING NLU MODEL ERRORS](https://arxiv.org/pdf/2110.06384.pdf), Oct. 12 2021Facebook, `model debugging` `toolkit` design
- [Hierarchical Transformers Are More Efficient Language Models](https://arxiv.org/pdf/2110.13711.pdf), Oct. 26 2021. `language model`
- [AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain](https://arxiv.org/pdf/2110.13434.pdf), Oct. 26 2021. `tokenization`
- [Quantifying Epistemic Uncertainty in Deep Learning](https://arxiv.org/pdf/2110.12122.pdf), Oct. 23 2021. `uncertainty`
- [Alignment Attention by Matching Key and Query Distributions](https://arxiv.org/pdf/2110.12567.pdf), Oct. 2021. `inductive bias` `attention` `nips2021`
