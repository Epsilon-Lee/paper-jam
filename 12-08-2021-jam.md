
- [NL-Augmenter: A Framework for Task-Sensitive Natural Language Transformation](https://arxiv.org/pdf/2112.02721.pdf), Dec. 6 2021. [GEM-benchmark](https://gem-benchmark.com/).
  - This is like a behavior test suite focusing on the robustness of model behavior with respect to input manipulation/transformation.
- [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf), Dec. 1 2021, Anthropic.
- [Lossy Compression for Lossless Prediction](https://arxiv.org/pdf/2106.10800.pdf), Oct. 6 2021 `nips2021`
- [Causal Distillation for Language Models](https://arxiv.org/pdf/2112.02505.pdf), Dec. 5 2021 `emnlp2021`
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf), Oct. 16 2021 `iclr2022 submit`
  - Propose low-rank-parameterized update matrices to reduce computational cost of fine-tuning.
- [Initializing New Word Embeddings for Pretrained Language Models](https://github.com/john-hewitt/embed-init), a blogpost by John Hewitt.
- [Towards Interactive Language Modeling](https://maartjeth.github.io/assets/documents/interactive_language_modeling.pdf), from Meta.
- [Learning To Retrieve Prompts for In-Context Learning](https://www.cs.tau.ac.il/~ohadr/Learning_to_retrieve_prompts_for_in_context_learning.pdf), Dec. 16 2021.
- [PROMPT WAYWARDNESS: The Curious Case of Discretized Interpretation of Continuous Prompts](https://arxiv.org/pdf/2112.08348.pdf), Dec. 15 2021.
- [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/pdf/2112.06905.pdf), Dec. 13 2021.
- [WebGPT: Browser-assisted question-answering with human feedback](https://cdn.openai.com/WebGPT.pdf), OpenAI `webGPT`
- [Measure and Improve Robustness in NLP Models: A Survey](https://arxiv.org/pdf/2112.08313.pdf), Dec. 15 2021.
- [Transformers Can Do Bayesian Inference](https://arxiv.org/pdf/2112.10510.pdf), Dec. 20 2021.
- [Efficient Large Scale Language Modeling with Mixtures of Experts](https://arxiv.org/pdf/2112.10684.pdf), Dec. 20 2021.
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf), Dec. 8 2021.
