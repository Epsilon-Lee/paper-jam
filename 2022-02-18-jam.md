
- [Should You Mask 15% in Masked Language Modeling?](https://arxiv.org/abs/2202.08005), Feb. 16 2022. `pretraining`
- [ZeroGen: Efficient Zero-shot Learning via Dataset Generation](https://arxiv.org/abs/2202.07922), Feb. 16 2022. `zero-shot`
- [Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation](https://arxiv.org/pdf/2202.07654.pdf), Feb. 2022. `qa evaluation`
- [Saving Dense Retriever from Shortcut Dependency in Conversational Search](https://arxiv.org/pdf/2202.07280.pdf), Feb. 15 2022. `dense retrieval` `dialogue`
- [Impact of Pretraining Term Frequencies on Few-Shot Reasoning](https://arxiv.org/abs/2202.07206), `pretraining`
- [Transformer Memory as a Differentiable Search Index](https://arxiv.org/pdf/2202.06991.pdf), Feb. 16 2022. `dense retrieval`
- [Quantifying Memorization Across Neural Language Model](https://arxiv.org/pdf/2202.07646.pdf), Feb. 15 2022. `memorization` `lm`
- [Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments](https://arxiv.org/pdf/2202.06387.pdf), Feb. 13. 2022.  `scaling law`
- [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/pdf/2202.06539.pdf), Feb. 16 2022. `privacy`
- [MetaShift: A dataset of dataset for evaluating contextual distribution shifts and training conflicts](https://arxiv.org/pdf/2202.06523.pdf), `ood learning` `data-centric`

- [What Does it Mean for a Language Model to Preserve Privacy?](https://arxiv.org/pdf/2202.05520.pdf), Feb. 14 2022. `pretrained lm` `privacy`
- [Survey of Hallucination in Natural Language Generation](https://arxiv.org/pdf/2202.03629.pdf), Feb.  8 2022. `hallucination`
