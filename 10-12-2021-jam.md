
- [Sparse MoEs Meet Efficient Ensembles](https://arxiv.org/pdf/2110.03360.pdf), Google Brain.
- [Which Shortcut Cues Will DNN Choose? A Study from the Parameter-Space Perspective](https://arxiv.org/pdf/2110.03095.pdf), `shortcut` `spurious correlation`.
- [Double Descent in Adversarial Training: An Implicit Label Noise Perspective](https://arxiv.org/pdf/2110.03135.pdf), `learning under noise` `label flipping noise` `double descent`
- [Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks](https://arxiv.org/pdf/2110.03825.pdf), `nips` `robustness`
- [Creating Training Sets via Weak Indirect Supervision](https://arxiv.org/pdf/2110.03484.pdf), `iclr` `weakly-supervised learning`
- [Robustness and reliability when training with noisy labels](https://arxiv.org/pdf/2110.03321.pdf), `learning under noise`
- [Exploiting the Intrinsic Neighborhood Structure for Source-free Domain Adaptation](https://arxiv.org/pdf/2110.04202.pdf), `domain adaptation` `nips`
- [THE INFORMATION GEOMETRY OF UNSUPERVISED REINFORCEMENT LEARNING](https://arxiv.org/pdf/2110.02719.pdf), `reinforcement learning theory`
- [ANOMALY TRANSFORMER: TIME SERIES ANOMALY DETECTION WITH ASSOCIATION DISCREPANCY](https://arxiv.org/pdf/2110.02642.pdf), `ood detection`
- [Exploring the Limits of Large Scale Pre-training](https://arxiv.org/pdf/2110.02095.pdf), Google Research.
- [Noisy Feature Mixup](https://arxiv.org/pdf/2110.02180.pdf), `data augmentation`
- [UNIFYING LIKELIHOOD-FREE INFERENCE WITH BLACK-BOX SEQUENCE DESIGN AND BEYOND](https://arxiv.org/pdf/2110.03372.pdf), `likelihood free inference`

### Theory of contrastive learning

- [The Power of Contrast for Feature Learning: A Theoretical Analysis](https://arxiv.org/pdf/2110.02473.pdf), James Zou's group.
- [Sharp Learning Bounds for Contrastive Unsupervised Representation Learning](https://arxiv.org/pdf/2110.02501.pdf), RIKEN AIP.
