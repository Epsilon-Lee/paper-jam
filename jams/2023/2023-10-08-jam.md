
- [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models](https://browse.arxiv.org/pdf/2310.00902.pdf), Oct. 2 2023.
- [What do larger image classifiers memorise?](https://arxiv.org/pdf/2310.05337.pdf), Oct. 9 2023.
- [Deep concept removal](https://arxiv.org/pdf/2310.05755.pdf), Oct. 9 2023.
- [Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks](https://arxiv.org/pdf/2310.04955.pdf), Oct. 8 2023.
- [On the embedding collapse when scaling up recommendation models](https://arxiv.org/pdf/2310.04400.pdf), Oct. 6 2023.
  - related to tabular deep learning
- [Toward a Foundation Model for Time Series Data](https://arxiv.org/pdf/2310.03916.pdf), Oct. 5 2023.
- [Unlabeled Out-Of-Domain Data Improves Generalization](https://arxiv.org/pdf/2310.00027.pdf), Sep. 29 2023. `theory`.
- [Learning the Efficient Frontier](https://browse.arxiv.org/pdf/2309.15775.pdf), Sep. 27 2023.
- [Monitoring Machine Learning Models: Online Detection of Relevant Deviations](https://browse.arxiv.org/pdf/2309.15187.pdf), Sep. 26 2023.
- [Dataset Quantization](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Dataset_Quantization_ICCV_2023_paper.pdf), `iccv2023`.
- [Hierarchical concept discovery models: a concept pyramid scheme](https://browse.arxiv.org/pdf/2310.02116.pdf), Oct. 3 2023.
- [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains](https://arxiv.org/pdf/2310.07838.pdf), Oct. 17 2023. `theory of distillation`.
- [Model metamers reveal divergent invariances between biological and artificial neural networks](https://www.nature.com/articles/s41593-023-01442-0), 2023. `neuroscience` `robustness`.

### Causal inference

- [Deep backtracking counterfactuals for causally compliant explanations](https://arxiv.org/pdf/2310.07665.pdf), Oct. 11 2023.

### Time-series

- [Prompt-augmented Temporal Point Process for Streaming Event Sequence](https://arxiv.org/pdf/2310.04993.pdf), Oct. 8 2023.
- [iTransformer: Inverted Transformers are effective for time series forcasting](https://arxiv.org/pdf/2310.06625.pdf), Oct. 10 2023.
- [Large language models are zero-shot time series forecasters](https://arxiv.org/pdf/2310.07820.pdf), Oct. 11 2023.

### Trustworthy ml

- [Representation engineering: a top-down approach to ai transparency](https://browse.arxiv.org/pdf/2310.01405.pdf), Oct. 10 2023.
- [Functional trustworthiness of AI systems by statistically valid testing](https://browse.arxiv.org/pdf/2310.02727.pdf), Oct. 4 2023.

### Active learning

- [Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning](https://openreview.net/pdf?id=vcHwQyNBjW), `tmlr2023`.

### GFlowNet

- [Pre-Training and Fine-Tuning Generative Flow Networks](https://browse.arxiv.org/pdf/2310.03419.pdf), Oct. 5 2023.

### PU Learning

- [Beyond myopia: learning from positive and unlabeled data through holistic predictive trends](https://arxiv.org/pdf/2310.04078.pdf), Oct. 6 2023.

### Calibration and uncertainty

- [Measuring calibration in deep learning](https://openreview.net/pdf?id=r1la7krKPS), `iclr2020`.
- [Classifier calibration: a survey on how to assess and improve predicted class probabilities](https://arxiv.org/pdf/2112.10327.pdf), Feb. 16 2023.

### Weakly supervised learning

- [Binary Classification with Confidence Difference](https://arxiv.org/pdf/2310.05632.pdf), Oct. 9 2023.

### Continual learning

- [Towards guarantees for parameter isolation in continual learning](https://browse.arxiv.org/pdf/2310.01165.pdf), Oct. 2 2023.

### Meta-learning

- [Making Scalable Meta Learning Practical](https://arxiv.org/pdf/2310.05674.pdf), Oct. 9 2023. `data selection` as application.

### Representation learning

- [Understanding transferable representation learning and zero-shot transfer in CLIP](https://browse.arxiv.org/pdf/2310.00927.pdf), Oct. 2 2023.
- [Discrete, compositional, and symbolic representations through attractor dynamics](https://browse.arxiv.org/pdf/2310.01807.pdf), Oct. 3 2023.
- [Identifying Representations for Intervention Extrapolation](https://arxiv.org/pdf/2310.04295.pdf), Oct. 6 2023. `causal representation learning`.
- [Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks](https://arxiv.org/pdf/2310.03789.pdf), Oct. 5 2023.

### Generalization mystery of nn

- [Memorization with neural nets: going beyond the worst case](https://arxiv.org/pdf/2310.00327.pdf), Sep. 30 2023.
- [Robust nonparametric hypothesis testing to understand variability in training neural networks](https://arxiv.org/pdf/2310.00541.pdf), Oct.1 2023.
- [Adaptivity and modularity for efficient generalization over task diversity](https://arxiv.org/pdf/2310.08866.pdf), Oct. 13 2023.

### Distribution shift

- [Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks](https://arxiv.org/pdf/2107.07455.pdf), Feb. 11 2022.
- [Deep Neural Networks Tend To Extrapolate Predictably](https://browse.arxiv.org/pdf/2310.00873.pdf), Oct. 2 2023. [code](https://github.com/katiekang1998/cautious_extrapolation).

### LLMs and beyond

- [Borges and AI](https://browse.arxiv.org/pdf/2310.01425.pdf), Oct. 4 2023. `Leon Bottou & Bernhard Scholkopf`.
- [Fusing models with complementary expertise](https://browse.arxiv.org/pdf/2310.01542.pdf), Oct. 2 2023.
  - [Model Fusion via Optimal Transport](https://arxiv.org/abs/1910.05653), Oct. 12 2019. `nips2020`.
- [AdaMerging: Adaptive model merging for multi-task learning](https://browse.arxiv.org/pdf/2310.02575.pdf), Oct. 4 2023.
- [Linear attention is (maybe) all you need](https://browse.arxiv.org/pdf/2310.01082.pdf), Oct. 2 2023.
- [Farzi Data: Autoregressive data distillation](https://arxiv.org/pdf/2310.09983.pdf), Oct. 15 2023.
- [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/).
- [Tokenizer choice for LLM training: negligible or crucial](https://arxiv.org/pdf/2310.08754.pdf), Oct. 12 2023.
- [Reasoning or reciting? Exploring the capabilities and limitatioins of langauge models throught counterfactual tasks](https://arxiv.org/pdf/2307.02477.pdf), Aug. 1 2023.
- [The efficiency misnomer](https://arxiv.org/pdf/2110.12894.pdf), Mar. 16 2022.
- [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714), Oct. 5 2023.

#### Understand generalization

- [Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?](https://arxiv.org/abs/2310.09562), Oct. 14 2023.

#### Alignment

- [PARL: A Unified Framework for Policy Alignment in Reinforcement Learning](https://arxiv.org/abs/2308.02585), Aug. 3 2023.

#### Hallucination

- [Automatic hallucination assessment for aligned large language models via transferable adversarial attacks](https://arxiv.org/pdf/2310.12516.pdf), Oct. 19 2023.
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043), Jul. 27 2023.

#### Reasoning

- [Counting and Algorithmic Generalization with Transformers](https://arxiv.org/pdf/2310.08661.pdf), Oct. 12 2023. [code](https://github.com/SimonOuellette35/CountingWithTransformers).

#### Decoding

- [Amortized intractable inference in large langauge models](https://arxiv.org/pdf/2310.04363.pdf), Oct. 6 2023. [code](https://github.com/GFNOrg/gfn-lm-tuning).
- [The consensus game: language model generation via equilibrium search](https://arxiv.org/pdf/2310.09139.pdf), Oct. 13 2023.

#### Knowledge fusion via model fusion

- [Transformer fusion with optimal transport](https://arxiv.org/pdf/2310.05719.pdf), Oct. 9 2023.
- [Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes](https://proceedings.mlr.press/v139/lucas21a/lucas21a.pdf), `icml2021`.
- [AutoMix: Automatically Mixing Language Models](https://arxiv.org/pdf/2310.12963.pdf), Oct. 19 2023.

#### Scaling law

- [A Neural Scaling Law from the Dimension of the Data Manifold](https://browse.arxiv.org/pdf/2004.10802.pdf), Apr. 22 2020.
- [A neural scaling law from lottery ticket ensembling](https://browse.arxiv.org/pdf/2310.02258.pdf), Oct. 3 2023.
- [Can a student LLM perform as well as it's teacher?](https://browse.arxiv.org/pdf/2310.02421.pdf), Oct. 3 2023.
- [xVal: a continuous number encoding for large language models](https://browse.arxiv.org/pdf/2310.02989.pdf), Oct. 4 2023.

#### Emergence

- [Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task](https://arxiv.org/pdf/2310.09336.pdf), Oct. 13 2023.
- [Understanding the effects of RLHF on LLM generalization and diversity](https://arxiv.org/pdf/2310.06452.pdf), Oct. 10 2023.

#### Mechanistic interpretability

- [Junk DNA hypothesis: a task-centric angle of LLM pre-training weights through sparsity](https://browse.arxiv.org/pdf/2310.02277.pdf), Sep. 29 2023.
- [Discovering knowledge-critical subnetworks in pretrained language models](https://browse.arxiv.org/pdf/2310.03084.pdf), Oct. 4 2023.
- [Episodic memory theory for the mechanistic interpretation of recurrent neural networks](https://browse.arxiv.org/pdf/2310.02430.pdf), Oct. 3 2023.
- [From language modeling to instruction following: understanding the behavior shift in LLMs after instruction tuning](https://arxiv.org/pdf/2310.00492.pdf), Sep. 30 2023.
- [Logical Languages Accepted by Transformer Encoders with Hard Attention](https://arxiv.org/pdf/2310.03817.pdf), Oct. 5 2023.
- [Attribution Patching Outperforms Automated Circuit Discovery](https://arxiv.org/pdf/2310.10348.pdf), Oct. 16 2023.
- [Circuit component reuse across tasks in Transformer language models](https://arxiv.org/pdf/2310.08744.pdf), Oct. 12 2023.
- [Sparse autoencoders find highly interpretable features in large langauge models](https://arxiv.org/pdf/2309.08600.pdf), Oct. 4 2023.
- [How connectivity structure shapes rich and lazy learning in neural circuits(https://arxiv.org/pdf/2310.08513.pdf), Oct. 21 2023.
- [Attribution Patching Outperforms Automated Circuit Discovery](https://arxiv.org/abs/2310.10348), Oct. 16 2023.
- [Measuring Feature Sparsity in Language Models](https://arxiv.org/pdf/2310.07837.pdf), Oct. 13 2023.
- [Seeking neural nuggets: Knowledge transfer in large language models from a parametric perspective](https://arxiv.org/pdf/2310.11451.pdf), Oct. 17 2023.

#### In-context learning

- [Linear transformers are secretly fast weight programmers](https://arxiv.org/pdf/2102.11174.pdf), Jun. 9 2021.
- [A modern self-referential weight matrix that learns to modify itself](https://proceedings.mlr.press/v162/irie22b/irie22b.pdf), `icml2022`.
- [Transformers Learn In-Context by Gradient Descent](https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf), `icml2023`.
- [Understanding in-context learning via supportive pretraining data](https://arxiv.org/pdf/2306.15091.pdf), Jun. 26 2023.
- [Understanding in-context learning in Transformers and LLMs by learning to learn discrete functions](https://browse.arxiv.org/pdf/2310.03016.pdf), Oct. 4 2023.
- [In-Context Convergence of Transformers](https://arxiv.org/pdf/2310.05249.pdf), Oct. 8 2023.
- [A meta-learning perspective on Transformers for causal language modeling](https://arxiv.org/pdf/2310.05884.pdf), Oct. 9 2023.
  - _" explicating an inner optimization process "_
- [Understanding prompt engineering may not require rethinking generalization](https://arxiv.org/pdf/2310.03957.pdf), Oct. 6 2023.
- [Transformers as algorithms: generalization and stability in in-context learning](https://arxiv.org/pdf/2301.07067.pdf), Feb. 6 2023.
- [What's the magic word? A control theory of LLM prompting](https://arxiv.org/pdf/2310.04444.pdf), Oct. 10 2023.
- [The Expresssive Power of Transformers with Chain of Thought](https://arxiv.org/abs/2310.07923), Oct. 11 2023.

#### Benchmarks

- [NLPBench: Evaluating LLMs on solving NLP problems](https://browse.arxiv.org/pdf/2309.15630.pdf), Oct. 8 2023.
- [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2305.01210), May 2 2023.
- [What does the OpenLLM Leaderboard measure?](https://hub.zenoml.com/report/a13x/What%20does%20the%20OpenLLM%20Leaderboard%20measure%3F), `blogpost`.

#### Applications

- [AutoCast++: Enhancing world event prediction with zero-shot ranking-based context retrieval](https://browse.arxiv.org/pdf/2310.01880.pdf), Oct. 3 2023.

---

### Codebase

- [llama-classification](https://github.com/sh0416/llama-classification).
- [small-text](https://github.com/webis-de/small-text/tree/main).
