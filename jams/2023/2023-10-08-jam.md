
- [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models](https://browse.arxiv.org/pdf/2310.00902.pdf), Oct. 2 2023.

### Distribution shift

- [Deep Neural Networks Tend To Extrapolate Predictably](https://browse.arxiv.org/pdf/2310.00873.pdf), Oct. 2 2023. [code](https://github.com/katiekang1998/cautious_extrapolation).

### LLMs and beyond

- [Borges and AI](https://browse.arxiv.org/pdf/2310.01425.pdf), Oct. 4 2023. `Leon Bottou & Bernhard Scholkopf`.
- [Fusing models with complementary expertise](https://browse.arxiv.org/pdf/2310.01542.pdf), Oct. 2 2023.

#### Scaling law

- [A Neural Scaling Law from the Dimension of the Data Manifold](https://browse.arxiv.org/pdf/2004.10802.pdf), Apr. 22 2020.
- [A neural scaling law from lottery ticket ensembling](https://browse.arxiv.org/pdf/2310.02258.pdf), Oct. 3 2023.
- [Can a student LLM perform as well as it's teacher?](https://browse.arxiv.org/pdf/2310.02421.pdf), Oct. 3 2023.
- [xVal: a continuous number encoding for large language models](https://browse.arxiv.org/pdf/2310.02989.pdf), Oct. 4 2023.

#### Mechanistic interpretability

- [Junk DNA hypothesis: a task-centric angle of LLM pre-training weights through sparsity](https://browse.arxiv.org/pdf/2310.02277.pdf), Sep. 29 2023.
- [Discovering knowledge-critical subnetworks in pretrained language models](https://browse.arxiv.org/pdf/2310.03084.pdf), Oct. 4 2023.

#### In-context learning

- [Understanding in-context learning in Transformers and LLMs by learning to learn discrete functions](https://browse.arxiv.org/pdf/2310.03016.pdf), Oct. 4 2023.
