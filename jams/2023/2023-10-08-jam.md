
- [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models](https://browse.arxiv.org/pdf/2310.00902.pdf), Oct. 2 2023.
- [What do larger image classifiers memorise?](https://arxiv.org/pdf/2310.05337.pdf), Oct. 9 2023.
- [Deep concept removal](https://arxiv.org/pdf/2310.05755.pdf), Oct. 9 2023.
- [Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks](https://arxiv.org/pdf/2310.04955.pdf), Oct. 8 2023.

### Weakly supervised learning

- [Binary Classification with Confidence Difference](https://arxiv.org/pdf/2310.05632.pdf), Oct. 9 2023.

### Meta-learning

- [Making Scalable Meta Learning Practical](https://arxiv.org/pdf/2310.05674.pdf), Oct. 9 2023. `data selection` as application.

### Representation learning

- [Understanding transferable representation learning and zero-shot transfer in CLIP](https://browse.arxiv.org/pdf/2310.00927.pdf), Oct. 2 2023.
- [Discrete, compositional, and symbolic representations through attractor dynamics](https://browse.arxiv.org/pdf/2310.01807.pdf), Oct. 3 2023.

### Distribution shift

- [Deep Neural Networks Tend To Extrapolate Predictably](https://browse.arxiv.org/pdf/2310.00873.pdf), Oct. 2 2023. [code](https://github.com/katiekang1998/cautious_extrapolation).

### LLMs and beyond

- [Borges and AI](https://browse.arxiv.org/pdf/2310.01425.pdf), Oct. 4 2023. `Leon Bottou & Bernhard Scholkopf`.
- [Fusing models with complementary expertise](https://browse.arxiv.org/pdf/2310.01542.pdf), Oct. 2 2023.

#### Knowledge fusion via model fusion

- [Transformer fusion with optimal transport](https://arxiv.org/pdf/2310.05719.pdf), Oct. 9 2023.

#### Scaling law

- [A Neural Scaling Law from the Dimension of the Data Manifold](https://browse.arxiv.org/pdf/2004.10802.pdf), Apr. 22 2020.
- [A neural scaling law from lottery ticket ensembling](https://browse.arxiv.org/pdf/2310.02258.pdf), Oct. 3 2023.
- [Can a student LLM perform as well as it's teacher?](https://browse.arxiv.org/pdf/2310.02421.pdf), Oct. 3 2023.
- [xVal: a continuous number encoding for large language models](https://browse.arxiv.org/pdf/2310.02989.pdf), Oct. 4 2023.

#### Mechanistic interpretability

- [Junk DNA hypothesis: a task-centric angle of LLM pre-training weights through sparsity](https://browse.arxiv.org/pdf/2310.02277.pdf), Sep. 29 2023.
- [Discovering knowledge-critical subnetworks in pretrained language models](https://browse.arxiv.org/pdf/2310.03084.pdf), Oct. 4 2023.
- [Episodic memory theory for the mechanistic interpretation of recurrent neural networks](https://browse.arxiv.org/pdf/2310.02430.pdf), Oct. 3 2023.

#### In-context learning

- [Understanding in-context learning in Transformers and LLMs by learning to learn discrete functions](https://browse.arxiv.org/pdf/2310.03016.pdf), Oct. 4 2023.
- [In-Context Convergence of Transformers](https://arxiv.org/pdf/2310.05249.pdf), Oct. 8 2023.
- [A meta-learning perspective on Transformers for causal language modeling](https://arxiv.org/pdf/2310.05884.pdf), Oct. 9 2023.
  - _" explicating an inner optimization process "_

#### Applications

- [AutoCast++: Enhancing world event prediction with zero-shot ranking-based context retrieval](https://browse.arxiv.org/pdf/2310.01880.pdf), Oct. 3 2023.
