
- [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models](https://browse.arxiv.org/pdf/2310.00902.pdf), Oct. 2 2023.
  - [Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs](https://arxiv.org/pdf/2303.08114.pdf), Mar. 14 2023.
- [What do larger image classifiers memorise?](https://arxiv.org/pdf/2310.05337.pdf), Oct. 9 2023.
- [Deep concept removal](https://arxiv.org/pdf/2310.05755.pdf), Oct. 9 2023.
- [Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks](https://arxiv.org/pdf/2310.04955.pdf), Oct. 8 2023.
- [On the embedding collapse when scaling up recommendation models](https://arxiv.org/pdf/2310.04400.pdf), Oct. 6 2023.
  - related to tabular deep learning
- [Unlabeled Out-Of-Domain Data Improves Generalization](https://arxiv.org/pdf/2310.00027.pdf), Sep. 29 2023. `theory`.
- [Learning the Efficient Frontier](https://browse.arxiv.org/pdf/2309.15775.pdf), Sep. 27 2023.
- [Monitoring Machine Learning Models: Online Detection of Relevant Deviations](https://browse.arxiv.org/pdf/2309.15187.pdf), Sep. 26 2023.
- [Dataset Quantization](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Dataset_Quantization_ICCV_2023_paper.pdf), `iccv2023`.
- [Hierarchical concept discovery models: a concept pyramid scheme](https://browse.arxiv.org/pdf/2310.02116.pdf), Oct. 3 2023.
- [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains](https://arxiv.org/pdf/2310.07838.pdf), Oct. 17 2023. `theory of distillation`.
- [Model metamers reveal divergent invariances between biological and artificial neural networks](https://www.nature.com/articles/s41593-023-01442-0), 2023. `neuroscience` `robustness`.
- [Microscaling Data Formats for Deep Learning](https://arxiv.org/pdf/2310.10537.pdf), Oct. 19 2023.
- [Data Cleaning and Machine Learning: A Systematic Literature Review](https://browse.arxiv.org/pdf/2310.01765.pdf), 

### Interpretability

- [Stabilizing Estimates of Shapley Values with Control Variates](https://arxiv.org/pdf/2310.07672.pdf), Oct. 11 2023.
- [Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates](https://arxiv.org/pdf/2310.04352.pdf), Oct. 6 2023.
- [Interpretability is in the mind of the beholder: A causal framework for human-interpretable representation learning](https://arxiv.org/pdf/2309.07742.pdf), Sep. 14 2023.

### Causal inference

- [Deep backtracking counterfactuals for causally compliant explanations](https://arxiv.org/pdf/2310.07665.pdf), Oct. 11 2023.
- [Causal-structure driven augmentations for text ood generalization](https://arxiv.org/pdf/2310.12803.pdf), Oct. 19 2023.

### Time-series

- [Prompt-augmented Temporal Point Process for Streaming Event Sequence](https://arxiv.org/pdf/2310.04993.pdf), Oct. 8 2023.
- [iTransformer: Inverted Transformers are effective for time series forcasting](https://arxiv.org/pdf/2310.06625.pdf), Oct. 10 2023.
- [Large language models are zero-shot time series forecasters](https://arxiv.org/pdf/2310.07820.pdf), Oct. 11 2023.
- [Toward a Foundation Model for Time Series Data](https://arxiv.org/pdf/2310.03916.pdf), Oct. 5 2023.

### Trustworthy ml

- [Representation engineering: a top-down approach to ai transparency](https://browse.arxiv.org/pdf/2310.01405.pdf), Oct. 10 2023.
- [Functional trustworthiness of AI systems by statistically valid testing](https://browse.arxiv.org/pdf/2310.02727.pdf), Oct. 4 2023.

### Active learning

- [Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning](https://openreview.net/pdf?id=vcHwQyNBjW), `tmlr2023`.

### GFlowNet

- [Pre-Training and Fine-Tuning Generative Flow Networks](https://browse.arxiv.org/pdf/2310.03419.pdf), Oct. 5 2023.

### PU Learning

- [Beyond myopia: learning from positive and unlabeled data through holistic predictive trends](https://arxiv.org/pdf/2310.04078.pdf), Oct. 6 2023.

### Calibration and uncertainty

- [Measuring calibration in deep learning](https://openreview.net/pdf?id=r1la7krKPS), `iclr2020`.
- [Classifier calibration: a survey on how to assess and improve predicted class probabilities](https://arxiv.org/pdf/2112.10327.pdf), Feb. 16 2023.

### Weakly supervised learning

- [Binary Classification with Confidence Difference](https://arxiv.org/pdf/2310.05632.pdf), Oct. 9 2023.

### Continual learning

- [Towards guarantees for parameter isolation in continual learning](https://browse.arxiv.org/pdf/2310.01165.pdf), Oct. 2 2023.

### Meta-learning

- [Making Scalable Meta Learning Practical](https://arxiv.org/pdf/2310.05674.pdf), Oct. 9 2023. `data selection` as application.

### Representation learning

- [Understanding transferable representation learning and zero-shot transfer in CLIP](https://browse.arxiv.org/pdf/2310.00927.pdf), Oct. 2 2023.
- [Discrete, compositional, and symbolic representations through attractor dynamics](https://browse.arxiv.org/pdf/2310.01807.pdf), Oct. 3 2023.
- [Identifying Representations for Intervention Extrapolation](https://arxiv.org/pdf/2310.04295.pdf), Oct. 6 2023. `causal representation learning`.
- [Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks](https://arxiv.org/pdf/2310.03789.pdf), Oct. 5 2023.
- [Introspective Deep Metric Learning](https://arxiv.org/pdf/2309.09982.pdf), Sep. 11 2023.

### Generalization mystery of nn

- [Memorization with neural nets: going beyond the worst case](https://arxiv.org/pdf/2310.00327.pdf), Sep. 30 2023.
- [Robust nonparametric hypothesis testing to understand variability in training neural networks](https://arxiv.org/pdf/2310.00541.pdf), Oct.1 2023.
- [Adaptivity and modularity for efficient generalization over task diversity](https://arxiv.org/pdf/2310.08866.pdf), Oct. 13 2023.
- [Why Does Sharpness-Aware Minimization Generalize Better Than SGD?](https://arxiv.org/pdf/2310.07269.pdf), Oct. 11 2023.
- [A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment for Imbalanced Learning](https://arxiv.org/pdf/2310.04752.pdf), Oct. 7 2023.

### Distribution shift

- [Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks](https://arxiv.org/pdf/2107.07455.pdf), Feb. 11 2022.
- [Deep Neural Networks Tend To Extrapolate Predictably](https://browse.arxiv.org/pdf/2310.00873.pdf), Oct. 2 2023. [code](https://github.com/katiekang1998/cautious_extrapolation).
- [A Survey of Heterogeneous Transfer Learning](https://arxiv.org/pdf/2310.08459.pdf), Oct. 15 2023.
- [GDL-DS: A benchmark for geometric deep learning under distribution shifts](https://arxiv.org/pdf/2310.08677.pdf), Oct. 12 2023.
- [Distributionally robust post-hoc classifiers under prior shifts](https://openreview.net/pdf?id=3KUfbI9_DQE), `iclr2023`.

### Old works of Stefan Wager

- [Teaching Statistics at Google Scale](https://browse.arxiv.org/pdf/1508.01278.pdf), Aug. 16 2015.
  - _"despite the deepening connections between engineering and modern data science, we argue that  training in classical statistical concepts plays a central role in preparing students to solve Google-scale problems"_
- [Data Augmentation via Levy Processes](https://browse.arxiv.org/pdf/1603.06340.pdf), Mar. 21 2016.

### Machine learning on graphs

- [Efficiently visualizing large graphs](https://arxiv.org/pdf/2310.11186.pdf), Oct. 17 2023. [code](https://github.com/Charlie-XIAO/Large-graph-visualization).

---

### LLMs and beyond

- [Borges and AI](https://browse.arxiv.org/pdf/2310.01425.pdf), Oct. 4 2023. `Leon Bottou & Bernhard Scholkopf`.
- [Fusing models with complementary expertise](https://browse.arxiv.org/pdf/2310.01542.pdf), Oct. 2 2023.
  - [Model Fusion via Optimal Transport](https://arxiv.org/abs/1910.05653), Oct. 12 2019. `nips2020`.
- [AdaMerging: Adaptive model merging for multi-task learning](https://browse.arxiv.org/pdf/2310.02575.pdf), Oct. 4 2023.
- [Linear attention is (maybe) all you need](https://browse.arxiv.org/pdf/2310.01082.pdf), Oct. 2 2023.
- [Farzi Data: Autoregressive data distillation](https://arxiv.org/pdf/2310.09983.pdf), Oct. 15 2023.
- [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/).
- [Tokenizer choice for LLM training: negligible or crucial](https://arxiv.org/pdf/2310.08754.pdf), Oct. 12 2023.
- [Reasoning or reciting? Exploring the capabilities and limitatioins of langauge models throught counterfactual tasks](https://arxiv.org/pdf/2307.02477.pdf), Aug. 1 2023.
- [The efficiency misnomer](https://arxiv.org/pdf/2110.12894.pdf), Mar. 16 2022.
- [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714), Oct. 5 2023.
- [To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing](https://arxiv.org/abs/2310.07715), Oct. 11 2023.
- [Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf), Oct. 10 2023.
- [Frozen Transformers in language models are effective visual encoder layers](https://arxiv.org/pdf/2310.12973.pdf), Oct. 19 2023.

#### Math ability

- [LLEMMA: An open language model for mathematics](https://arxiv.org/pdf/2310.10631.pdf), Oct. 16 2023.

#### Understand generalization

- [Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?](https://arxiv.org/abs/2310.09562), Oct. 14 2023.

#### Alignment

- [PARL: A Unified Framework for Policy Alignment in Reinforcement Learning](https://arxiv.org/abs/2308.02585), Aug. 3 2023.
- [Secrets of RLHF in Large Language Models Part I: PPO](https://arxiv.org/pdf/2307.04964.pdf), Jul. 18 2023.
- [SALMON: Self-Alignment with Principle-Following Reward Models](https://arxiv.org/abs/2310.05910), Oct. 9 2023.
- [Emptying the ocean with a spoon: Should we edit models?](https://arxiv.org/abs/2310.11958), Oct. 18 2023.
- [An Emulator for Fine-Tuning Large Language Models using Small Language Models](https://arxiv.org/pdf/2310.12962.pdf), Oct. 19 2023.
- [Gain wisdom from setbacks: Aligning LLMs via mistake analysis](https://arxiv.org/pdf/2310.10477.pdf), Oct. 16 2023.

#### Evaluation

- [Pseudointelligence: A Unifying Framework for Language Model Evaluation](https://arxiv.org/pdf/2310.12135.pdf), Oct. 18 2023.
- [Systematic Assessment of Factual Knowledge in Large Language Models](https://arxiv.org/pdf/2310.11638.pdf), Oct. 18 2023.

#### Hallucination

- [Automatic hallucination assessment for aligned large language models via transferable adversarial attacks](https://arxiv.org/pdf/2310.12516.pdf), Oct. 19 2023.
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043), Jul. 27 2023.
- [The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models](https://arxiv.org/pdf/2310.11877.pdf), Oct. 18 2023.

#### Reasoning

- [Counting and Algorithmic Generalization with Transformers](https://arxiv.org/pdf/2310.08661.pdf), Oct. 12 2023. [code](https://github.com/SimonOuellette35/CountingWithTransformers).
- [Large language models can learn rules](https://arxiv.org/pdf/2310.07064.pdf), Oct. 10 2023.

#### Decoding

- [Amortized intractable inference in large langauge models](https://arxiv.org/pdf/2310.04363.pdf), Oct. 6 2023. [code](https://github.com/GFNOrg/gfn-lm-tuning).
- [The consensus game: language model generation via equilibrium search](https://arxiv.org/pdf/2310.09139.pdf), Oct. 13 2023.

#### Knowledge fusion via model fusion

- [Transformer fusion with optimal transport](https://arxiv.org/pdf/2310.05719.pdf), Oct. 9 2023.
- [Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes](https://proceedings.mlr.press/v139/lucas21a/lucas21a.pdf), `icml2021`.
- [AutoMix: Automatically Mixing Language Models](https://arxiv.org/pdf/2310.12963.pdf), Oct. 19 2023.
- [Model merging by uncertainty-based gradient matching](https://arxiv.org/pdf/2310.12808.pdf), Oct. 19 2023.

#### Scaling law

- [A Neural Scaling Law from the Dimension of the Data Manifold](https://browse.arxiv.org/pdf/2004.10802.pdf), Apr. 22 2020.
- [A neural scaling law from lottery ticket ensembling](https://browse.arxiv.org/pdf/2310.02258.pdf), Oct. 3 2023.
- [Can a student LLM perform as well as it's teacher?](https://browse.arxiv.org/pdf/2310.02421.pdf), Oct. 3 2023.
- [xVal: a continuous number encoding for large language models](https://browse.arxiv.org/pdf/2310.02989.pdf), Oct. 4 2023.

#### Emergence

- [Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task](https://arxiv.org/pdf/2310.09336.pdf), Oct. 13 2023.
- [Understanding the effects of RLHF on LLM generalization and diversity](https://arxiv.org/pdf/2310.06452.pdf), Oct. 10 2023.

#### Mechanistic interpretability

- [Junk DNA hypothesis: a task-centric angle of LLM pre-training weights through sparsity](https://browse.arxiv.org/pdf/2310.02277.pdf), Sep. 29 2023.
- [Discovering knowledge-critical subnetworks in pretrained language models](https://browse.arxiv.org/pdf/2310.03084.pdf), Oct. 4 2023.
- [Episodic memory theory for the mechanistic interpretation of recurrent neural networks](https://browse.arxiv.org/pdf/2310.02430.pdf), Oct. 3 2023.
- [From language modeling to instruction following: understanding the behavior shift in LLMs after instruction tuning](https://arxiv.org/pdf/2310.00492.pdf), Sep. 30 2023.
- [Logical Languages Accepted by Transformer Encoders with Hard Attention](https://arxiv.org/pdf/2310.03817.pdf), Oct. 5 2023.
- [Attribution Patching Outperforms Automated Circuit Discovery](https://arxiv.org/pdf/2310.10348.pdf), Oct. 16 2023.
- [Circuit component reuse across tasks in Transformer language models](https://arxiv.org/pdf/2310.08744.pdf), Oct. 12 2023.
- [Sparse autoencoders find highly interpretable features in large langauge models](https://arxiv.org/pdf/2309.08600.pdf), Oct. 4 2023.
- [How connectivity structure shapes rich and lazy learning in neural circuits(https://arxiv.org/pdf/2310.08513.pdf), Oct. 21 2023.
- [Attribution Patching Outperforms Automated Circuit Discovery](https://arxiv.org/abs/2310.10348), Oct. 16 2023.
- [Measuring Feature Sparsity in Language Models](https://arxiv.org/pdf/2310.07837.pdf), Oct. 13 2023.
- [Seeking neural nuggets: Knowledge transfer in large language models from a parametric perspective](https://arxiv.org/pdf/2310.11451.pdf), Oct. 17 2023.
- [On the representational capacity of recurrent neural language models](https://arxiv.org/pdf/2310.12942.pdf), Oct. 19 2023.
- [Rigorously assessing natural language explanations of neurons](https://arxiv.org/pdf/2309.10312.pdf), Sep. 19 2023.
- [The Locality and Symmetry of Positional Encodings](https://arxiv.org/pdf/2310.12864.pdf), Oct. 19 2023.
- [Simple mechanisms for representing, indexing and manipulating concepts](https://arxiv.org/abs/2310.12143), Oct. 19 2023.
- [Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks](https://arxiv.org/pdf/2310.07711.pdf), Oct. 11 2023.
- [NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations](https://arxiv.org/pdf/2310.07184.pdf), Oct. 17 2023.
- [LMs as Counterfactual Explanation Modules: Can ChatGPT Explain Black-box Text Classifiers?](https://browse.arxiv.org/pdf/2309.13340.pdf), Sep. 23 2023.

#### Data-centric interpretability

- [A State-Vector Framework for Dataset Effects](https://arxiv.org/pdf/2310.10955.pdf), Oct. 17 2023.

#### In-context learning

- [Linear transformers are secretly fast weight programmers](https://arxiv.org/pdf/2102.11174.pdf), Jun. 9 2021.
- [A modern self-referential weight matrix that learns to modify itself](https://proceedings.mlr.press/v162/irie22b/irie22b.pdf), `icml2022`.
- [Transformers Learn In-Context by Gradient Descent](https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf), `icml2023`.
- [Understanding in-context learning via supportive pretraining data](https://arxiv.org/pdf/2306.15091.pdf), Jun. 26 2023.
- [Understanding in-context learning in Transformers and LLMs by learning to learn discrete functions](https://browse.arxiv.org/pdf/2310.03016.pdf), Oct. 4 2023.
- [In-Context Convergence of Transformers](https://arxiv.org/pdf/2310.05249.pdf), Oct. 8 2023.
- [A meta-learning perspective on Transformers for causal language modeling](https://arxiv.org/pdf/2310.05884.pdf), Oct. 9 2023.
  - _" explicating an inner optimization process "_
- [Understanding prompt engineering may not require rethinking generalization](https://arxiv.org/pdf/2310.03957.pdf), Oct. 6 2023.
- [Transformers as algorithms: generalization and stability in in-context learning](https://arxiv.org/pdf/2301.07067.pdf), Feb. 6 2023.
- [What's the magic word? A control theory of LLM prompting](https://arxiv.org/pdf/2310.04444.pdf), Oct. 10 2023.
- [The Expresssive Power of Transformers with Chain of Thought](https://arxiv.org/abs/2310.07923), Oct. 11 2023.
- [Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning](https://arxiv.org/abs/2305.14160), May 23 2023.
- [Measuring Pointwise V-Usable Information In-Context-ly](https://arxiv.org/pdf/2310.12300.pdf), Oct. 18 2023.

#### Benchmarks

- [NLPBench: Evaluating LLMs on solving NLP problems](https://browse.arxiv.org/pdf/2309.15630.pdf), Oct. 8 2023.
- [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2305.01210), May 2 2023.
- [What does the OpenLLM Leaderboard measure?](https://hub.zenoml.com/report/a13x/What%20does%20the%20OpenLLM%20Leaderboard%20measure%3F), `blogpost`.

#### Applications

- [AutoCast++: Enhancing world event prediction with zero-shot ranking-based context retrieval](https://browse.arxiv.org/pdf/2310.01880.pdf), Oct. 3 2023.
- [Towards foundation models for learning on tabular data](https://arxiv.org/pdf/2310.07338.pdf), Oct. 11 2023.
- [Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models](https://browse.arxiv.org/pdf/2310.00566.pdf), Oct. 1 2023.

#### LLMs for agent

- [Heap: Hierarchical policies for web actions using LLMs](https://arxiv.org/pdf/2310.03720.pdf), Oct. 5 2023.

---

### Old-school nlp

- [Neural Data Augmentation via Example Extrapolation](https://arxiv.org/pdf/2102.01335.pdf), Feb. 2 2021.
- [Text Embeddings Reveal (Almost) As Much As Text](https://arxiv.org/pdf/2310.06816.pdf), Oct. 10 2023. `privacy`.
- [Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications](https://arxiv.org/pdf/2310.12620.pdf), Oct. 19 2023.
- [Backpack Language Models](https://arxiv.org/pdf/2305.16765.pdf), May 26 2023.
  - [Character-level Chinese Backpack Language Models](https://arxiv.org/pdf/2310.12751.pdf), Oct. 19 2023.
- [Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers](https://arxiv.org/pdf/2310.12118.pdf), Oct. 18 2023. [code](https://github.com/cyberiada/cartography-for-compositionality).

---

### Codebase

- [llama-classification](https://github.com/sh0416/llama-classification).
- [small-text](https://github.com/webis-de/small-text/tree/main).
- [llama-2-jax](https://github.com/ayaka14732/llama-2-jax).
- [OpenLLaMA: An Open Reproduction of LLaMA](https://github.com/openlm-research/open_llama).
- [Contextualized.ML](https://contextualized.ml/).



