
- [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models](https://browse.arxiv.org/pdf/2310.00902.pdf), Oct. 2 2023.
- [What do larger image classifiers memorise?](https://arxiv.org/pdf/2310.05337.pdf), Oct. 9 2023.
- [Deep concept removal](https://arxiv.org/pdf/2310.05755.pdf), Oct. 9 2023.
- [Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks](https://arxiv.org/pdf/2310.04955.pdf), Oct. 8 2023.
- [On the embedding collapse when scaling up recommendation models](https://arxiv.org/pdf/2310.04400.pdf), Oct. 6 2023.
  - related to tabular deep learning
- [Toward a Foundation Model for Time Series Data](https://arxiv.org/pdf/2310.03916.pdf), Oct. 5 2023.
- [Unlabeled Out-Of-Domain Data Improves Generalization](https://arxiv.org/pdf/2310.00027.pdf), Sep. 29 2023. `theory`.
- [Learning the Efficient Frontier](https://browse.arxiv.org/pdf/2309.15775.pdf), Sep. 27 2023.
- [Monitoring Machine Learning Models: Online Detection of Relevant Deviations](https://browse.arxiv.org/pdf/2309.15187.pdf), Sep. 26 2023.
- [Dataset Quantization](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Dataset_Quantization_ICCV_2023_paper.pdf), `iccv2023`.
- [Hierarchical concept discovery models: a concept pyramid scheme](https://browse.arxiv.org/pdf/2310.02116.pdf), Oct. 3 2023.

### Time-series

- [Prompt-augmented Temporal Point Process for Streaming Event Sequence](https://arxiv.org/pdf/2310.04993.pdf), Oct. 8 2023.

### Trustworthy ml

- [Representation engineering: a top-down approach to ai transparency](https://browse.arxiv.org/pdf/2310.01405.pdf), Oct. 10 2023.
- [Functional trustworthiness of AI systems by statistically valid testing](https://browse.arxiv.org/pdf/2310.02727.pdf), Oct. 4 2023.

### Active learning

- [Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning](https://openreview.net/pdf?id=vcHwQyNBjW), `tmlr2023`.

### GFlowNet

- [Pre-Training and Fine-Tuning Generative Flow Networks](https://browse.arxiv.org/pdf/2310.03419.pdf), Oct. 5 2023.

### PU Learning

- [Beyond myopia: learning from positive and unlabeled data through holistic predictive trends](https://arxiv.org/pdf/2310.04078.pdf), Oct. 6 2023.

### Calibration and uncertainty

- [Measuring calibration in deep learning](https://openreview.net/pdf?id=r1la7krKPS), `iclr2020`.
- [Classifier calibration: a survey on how to assess and improve predicted class probabilities](https://arxiv.org/pdf/2112.10327.pdf), Feb. 16 2023.

### Weakly supervised learning

- [Binary Classification with Confidence Difference](https://arxiv.org/pdf/2310.05632.pdf), Oct. 9 2023.

### Continual learning

- [Towards guarantees for parameter isolation in continual learning](https://browse.arxiv.org/pdf/2310.01165.pdf), Oct. 2 2023.

### Meta-learning

- [Making Scalable Meta Learning Practical](https://arxiv.org/pdf/2310.05674.pdf), Oct. 9 2023. `data selection` as application.

### Representation learning

- [Understanding transferable representation learning and zero-shot transfer in CLIP](https://browse.arxiv.org/pdf/2310.00927.pdf), Oct. 2 2023.
- [Discrete, compositional, and symbolic representations through attractor dynamics](https://browse.arxiv.org/pdf/2310.01807.pdf), Oct. 3 2023.
- [Identifying Representations for Intervention Extrapolation](https://arxiv.org/pdf/2310.04295.pdf), Oct. 6 2023. `causal representation learning`.
- [Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks](https://arxiv.org/pdf/2310.03789.pdf), Oct. 5 2023.

### Generalization mystery of nn

- [Memorization with neural nets: going beyond the worst case](https://arxiv.org/pdf/2310.00327.pdf), Sep. 30 2023.
- [Robust nonparametric hypothesis testing to understand variability in training neural networks](https://arxiv.org/pdf/2310.00541.pdf), Oct.1 2023.

### Distribution shift

- [Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks](https://arxiv.org/pdf/2107.07455.pdf), Feb. 11 2022.
- [Deep Neural Networks Tend To Extrapolate Predictably](https://browse.arxiv.org/pdf/2310.00873.pdf), Oct. 2 2023. [code](https://github.com/katiekang1998/cautious_extrapolation).

### LLMs and beyond

- [Borges and AI](https://browse.arxiv.org/pdf/2310.01425.pdf), Oct. 4 2023. `Leon Bottou & Bernhard Scholkopf`.
- [Fusing models with complementary expertise](https://browse.arxiv.org/pdf/2310.01542.pdf), Oct. 2 2023.
  - [Model Fusion via Optimal Transport](https://arxiv.org/abs/1910.05653), Oct. 12 2019. `nips2020`.
- [AdaMerging: Adaptive model merging for multi-task learning](https://browse.arxiv.org/pdf/2310.02575.pdf), Oct. 4 2023.
- [Linear attention is (maybe) all you need](https://browse.arxiv.org/pdf/2310.01082.pdf), Oct. 2 2023.
- [Farzi Data: Autoregressive data distillation](https://arxiv.org/pdf/2310.09983.pdf), Oct. 15 2023.

#### Decoding

- [Amortized intractable inference in large langauge models](https://arxiv.org/pdf/2310.04363.pdf), Oct. 6 2023. [code](https://github.com/GFNOrg/gfn-lm-tuning).

#### Knowledge fusion via model fusion

- [Transformer fusion with optimal transport](https://arxiv.org/pdf/2310.05719.pdf), Oct. 9 2023.
- [Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes](https://proceedings.mlr.press/v139/lucas21a/lucas21a.pdf), `icml2021`.

#### Scaling law

- [A Neural Scaling Law from the Dimension of the Data Manifold](https://browse.arxiv.org/pdf/2004.10802.pdf), Apr. 22 2020.
- [A neural scaling law from lottery ticket ensembling](https://browse.arxiv.org/pdf/2310.02258.pdf), Oct. 3 2023.
- [Can a student LLM perform as well as it's teacher?](https://browse.arxiv.org/pdf/2310.02421.pdf), Oct. 3 2023.
- [xVal: a continuous number encoding for large language models](https://browse.arxiv.org/pdf/2310.02989.pdf), Oct. 4 2023.

#### Emergence

- [Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task](https://arxiv.org/pdf/2310.09336.pdf), Oct. 13 2023.

#### Mechanistic interpretability

- [Junk DNA hypothesis: a task-centric angle of LLM pre-training weights through sparsity](https://browse.arxiv.org/pdf/2310.02277.pdf), Sep. 29 2023.
- [Discovering knowledge-critical subnetworks in pretrained language models](https://browse.arxiv.org/pdf/2310.03084.pdf), Oct. 4 2023.
- [Episodic memory theory for the mechanistic interpretation of recurrent neural networks](https://browse.arxiv.org/pdf/2310.02430.pdf), Oct. 3 2023.
- [From language modeling to instruction following: understanding the behavior shift in LLMs after instruction tuning](https://arxiv.org/pdf/2310.00492.pdf), Sep. 30 2023.
- [Logical Languages Accepted by Transformer Encoders with Hard Attention](https://arxiv.org/pdf/2310.03817.pdf), Oct. 5 2023.

#### In-context learning

- [Understanding in-context learning in Transformers and LLMs by learning to learn discrete functions](https://browse.arxiv.org/pdf/2310.03016.pdf), Oct. 4 2023.
- [In-Context Convergence of Transformers](https://arxiv.org/pdf/2310.05249.pdf), Oct. 8 2023.
- [A meta-learning perspective on Transformers for causal language modeling](https://arxiv.org/pdf/2310.05884.pdf), Oct. 9 2023.
  - _" explicating an inner optimization process "_
- [Understanding prompt engineering may not require rethinking generalization](https://arxiv.org/pdf/2310.03957.pdf), Oct. 6 2023.

#### Benchmarks

- [NLPBench: Evaluating LLMs on solving NLP problems](https://browse.arxiv.org/pdf/2309.15630.pdf), Oct. 8 2023.

#### Applications

- [AutoCast++: Enhancing world event prediction with zero-shot ranking-based context retrieval](https://browse.arxiv.org/pdf/2310.01880.pdf), Oct. 3 2023.

---

### Codebase

- [llama-classification](https://github.com/sh0416/llama-classification).
- [small-text](https://github.com/webis-de/small-text/tree/main).
