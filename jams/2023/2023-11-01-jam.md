
- [Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices](https://arxiv.org/pdf/2310.19214.pdf), Oct. 30 2023.
- [Revisiting the Learnability of Apple Tasting](https://arxiv.org/pdf/2310.19064.pdf), Oct. 29 2023.
- [Learning an inventory control policy with general inventory arrival dynamics](https://arxiv.org/pdf/2310.17168.pdf), Oct. 26 2023.
- [MaxEnt loss: Constrained maximum entropy for calibration under out-of-distribution shift](https://arxiv.org/pdf/2310.17159.pdf), Oct. 26 2023.
- [Content Moderation and the Formation of Online Communities: A Theoretical Framework](https://arxiv.org/pdf/2310.10573.pdf), Oct. 16 2023.
- [Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets](https://arxiv.org/pdf/2310.04413.pdf), Oct. 12 2023.
- [Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation](https://arxiv.org/abs/2305.09860), May 2023. `inference` `decoding`.
- [Machine Translation for Nko: Tools, Corpora and Baseline Results](https://arxiv.org/pdf/2310.15612.pdf), Oct. 31 2023.
- [Stochastic gradient descent for Gaussian process done right](https://arxiv.org/pdf/2310.20581.pdf), Oct. 31 2023.
- [Frequency Domain-based Dataset Distillation](https://arxiv.org/pdf/2311.08819.pdf), Nov. 15 2023.
- [Anchor Data Augmentation](https://arxiv.org/pdf/2311.06965.pdf), Nov. 12 2023.
- [Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples](https://arxiv.org/pdf/2311.05538.pdf), Nov. 9 2023.
- [Dirichlet Active Learning](https://arxiv.org/pdf/2311.05501.pdf), Nov. 9 2023.
- [Rare event probability learning by normalizing flows](https://arxiv.org/pdf/2310.19167.pdf), Oct. 29 2023.
- [minimax: Efficient Baselines for Autocurricula in JAX](https://arxiv.org/pdf/2311.12716.pdf), Nov. 21 2023.

### Inductive bias

- [Weight-sharing regularization](https://arxiv.org/pdf/2311.03096.pdf), Nov. 6 2023.
- [Long sequence Hopfield memory](https://arxiv.org/pdf/2306.04532.pdf), Nov. 2 2023.

### Good old nlp

- [TopicGPT: A Prompt-based Topic Modeling Framework](https://arxiv.org/pdf/2311.01449.pdf), Nov. 2 2023.
- [Syntax-semantics interface: An algebric model](https://arxiv.org/pdf/2311.06189.pdf), Nov. 10 2023.
- [Toucan: Token-Aware Character Level Language Modeling](https://arxiv.org/pdf/2311.08620.pdf), Nov. 15 2023.
- [Simple and effective input reformulations for translation](https://arxiv.org/pdf/2311.06696.pdf), Nov. 12 2023.
- [Toucan: Token-Aware Character Level Language Modeling](https://arxiv.org/pdf/2311.08620.pdf), Nov. 15 2023.
- [Attribute Diversity Determines the Systematicity Gap in VQA](https://arxiv.org/pdf/2311.08695.pdf), Nov. 15 2023.
- [Measuring Adversarial Datasets](https://arxiv.org/pdf/2311.03566.pdf), Nov. 6 2023.

### Time series

- [TS-Diffusion: Generating Highly Complex Time Series with Diffusion Models](https://arxiv.org/pdf/2311.03303.pdf), Nov. 6 2023.

### Robustness

- [Spatial-frequency channels, shape bias, and adversarial robustness](https://arxiv.org/pdf/2309.13190.pdf), Sep. 22 2023.

### Calibration and uncertainty

- [Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification](https://arxiv.org/abs/2102.07856), Jul. 19 2021.
- [Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing](https://arxiv.org/pdf/2309.12236.pdf), Sep. 21 2023.
- [Uncertainty in gradient boosting via ensembles](https://arxiv.org/pdf/2006.10562.pdf), Apr. 2021.
- [Quantile extreme gradient boosting for uncertainty quantification](https://arxiv.org/pdf/2304.11732.pdf), Apr. 23 2023.

### Distribution shift

- [Learning optimal classification trees robust to distribution shifts](https://arxiv.org/pdf/2310.17772.pdf), Oct. 26 2023.
- [On the Foundations of Shortcut Learning](https://arxiv.org/pdf/2310.16228.pdf), Oct. 24 2023.
- [Performative Prediction: Past and Future](https://arxiv.org/pdf/2310.16608.pdf), Oct. 25 2023.
- [Meta-(out-of-context) learning in neural networks](https://arxiv.org/pdf/2310.15047.pdf), Oct. 24 2023.
- [Learning to (learn at test time)](https://arxiv.org/pdf/2310.13807.pdf), Oct. 20 2023.
- [Learning useful representations for shifting tasks and distributions](https://proceedings.mlr.press/v202/zhang23b/zhang23b.pdf), `icml2023`.
- [Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization](https://arxiv.org/pdf/2212.10445.pdf), Aug. 9 2023.
- [Handling New Class in Online Label Shift](https://www.lamda.nju.edu.cn/qianyy/paper/ICDM23_NOLS.pdf), 2023.

### Robustness vs generalization

- [Group robust classification: Without any group information](https://arxiv.org/pdf/2310.18555.pdf), Oct. 28 2023.

### Data weigting

- [A challenge in reweighting data with bilevel optimization](https://arxiv.org/pdf/2310.17386.pdf), Oct. 26 2023.
- [How re-sampling helps for long-tail learning](https://arxiv.org/pdf/2310.18236.pdf), Oct. 27 2023.
- [Data optimization in deep learning: A survey](https://arxiv.org/pdf/2310.16499.pdf), Oct. 25 2023.
- [D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning](https://arxiv.org/abs/2310.07931), Oct. 11 2023.

### Representation learning

- [A spectral condition for feature learning](https://arxiv.org/pdf/2310.17813.pdf), Oct. 26 2023.
- [Ghost on the shell: An expressive representation of general 3D shapes](https://arxiv.org/pdf/2310.15168.pdf), Oct. 24 2023.
- [Combating Representation Learning Disparity with Geometric Harmonization](https://arxiv.org/pdf/2310.17622.pdf), Oct. 26 2023.
- [Flow Factorized Representation Learning](https://arxiv.org/pdf/2309.13167.pdf), Sep. 22 2023.
- [Boundary-restricted metric learning](https://link.springer.com/article/10.1007/s10994-023-06380-3), Sep. 20 2023.
- [Contrastive difference predictive coding](https://arxiv.org/pdf/2310.20141.pdf), Oct. 31 2023.
- [Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination](https://arxiv.org/pdf/2311.02960.pdf), Nov. 6 2023.
- [The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning](https://arxiv.org/pdf/2311.02940.pdf), Nov. 6 2023.
- [Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples](https://arxiv.org/pdf/2311.05538.pdf), Nov. 9 2023.
- [Are "hierarchical" visual representations hierarchical?](https://arxiv.org/pdf/2311.05784.pdf), Nov. 9 2023.
- [Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations](https://arxiv.org/pdf/2311.08815.pdf), Nov. 15 2023.
- [Robust Contrastive Learning With Theory Guarantee](https://arxiv.org/pdf/2311.09671.pdf), Nov. 16 2023.
- [Soft matching distance: a metric on neural representations that captures single-neuron tuning](https://arxiv.org/pdf/2311.09466.pdf), Nov. 16 2023.
- [Manifold learning: what, how, and why](https://arxiv.org/pdf/2311.03757.pdf), Nov. 7 2023.

### Causal inference

- [Object-centric architectures enable efficient causal representation learning](https://arxiv.org/pdf/2310.18496.pdf), Oct. 29 2023.
- [Sample complexity boudns for score-matching: Causal discovery and generative modeling](https://arxiv.org/pdf/2310.18123.pdf), Oct. 27 2023.
- [Identifying latent polynomial causal models through the lens of change](https://arxiv.org/pdf/2310.15580.pdf), Oct. 24 2023.
- [Shortcuts for causal discovery of nonlinear models by score matching](https://arxiv.org/pdf/2310.14246.pdf), Oct. 22 2023.
- [CausalCite: A causal formulation of paper citations](https://arxiv.org/pdf/2311.02790.pdf), Nov. 5 2023.

### Generalization mystery

- [A path to simpler model starts with noise](https://arxiv.org/pdf/2310.19726.pdf), Oct. 30 2023. `data-centric`
  - _"Our results explain a key aspect of why simpler models often tend to perform as well as black box models on complex, noisier datasets"_
- [The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data](https://arxiv.org/pdf/2310.19273.pdf), Oct. 30 2023. `uncertainty` `data-centric`.
  - _"we present Memory-Perturbation Equation which relates model's sensitivity to perturbation in its training data"_
- [Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult](https://arxiv.org/pdf/2310.17087.pdf), Oct. 26 2023. `learning dynamics`.
- [On the Neural Tangent Kernel of Equilibrium Models](https://arxiv.org/pdf/2310.14062.pdf), Oct. 21 2023.
- [Understanding the enigma of double descent: An in-depth analysis through the lens of learned feature space](https://arxiv.org/pdf/2310.13572.pdf), Oct. 20 2023.
- [Generalization Bounds for Label Noise Stochastic Gradient Descent](https://arxiv.org/pdf/2311.00274.pdf), Nov. 1 2023.
- [Fortuitous forgetting in connectionist networks](https://arxiv.org/abs/2202.00155), Feb. 1 2022.
- [A Theory of Finite-Width Neural Networks: Generalization, Scaling Laws, and the Loss Landscape](https://infoscience.epfl.ch/record/303463), 2023. `thesis`.
- [It’s an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models](https://arxiv.org/pdf/2310.09250.pdf), Oct. 13 2023.
- [Cliff-learning](https://arxiv.org/pdf/2302.07348.pdf), Jun. 7 2023.
- [Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory](https://arxiv.org/pdf/2310.20360.pdf), Oct. 31 2023.
- [SoK: Memorisation in Machine Learning](https://arxiv.org/pdf/2311.03075.pdf), Nov. 6 2023.

### Continual learning

- [Progressively efficient learning](https://arxiv.org/pdf/2310.13004.pdf), Oct. 13 2023.

### Interpretability

- [Towards a fuller understanding of neurons with clustered compositional explanations](https://arxiv.org/pdf/2310.18443.pdf), Oct. 27 2023.
- [How well do feature-additive explainers explain feature-additive predictors](https://arxiv.org/pdf/2310.18496.pdf), Oct. 27 2023.
- [Sum-of-Parts Models: Faithful Attributions for Groups of Features](https://arxiv.org/pdf/2310.16316.pdf), Oct. 25 2023.
- [Learning Interpretable Rules for Scalable Data Representation and Classification](https://arxiv.org/pdf/2310.14336.pdf), Oct. 30 2023.
- [Local Universal Rule-based Explanations](https://zhuanlan.zhihu.com/p/617305431), Oct. 23 2023.
- [Explaining Interactions Between Text Spans](https://arxiv.org/pdf/2310.13506.pdf), Oct. 20 2023.
- [Intriguing properties of data attribution on diffusion models](https://arxiv.org/pdf/2311.00500.pdf), Nov. 1 2023.
- [Bridging the Human–AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero](https://arxiv.org/pdf/2310.16410.pdf), Oct. 25 2023.
- [On the Interplay between Fairness and Explainability](https://arxiv.org/pdf/2310.16607.pdf), Oct. 25 2023.
- [Measuring Association Between Labels and Free-Text Rationales](https://arxiv.org/pdf/2010.12762.pdf), Aug. 29 2022.
- [Leave-One-Out Distinguishability in Machine Learning](https://arxiv.org/pdf/2309.17310.pdf), Sep. 29 2023.
- [The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance](https://arxiv.org/pdf/2309.13775.pdf), Sep. 26 2023.
- [Neural Networks Are Implicit Decision Trees: The Hierarchical Simplicity Bias](https://arxiv.org/pdf/2311.02622.pdf), Nov. 5 2023.
- [CAFE: Conflict-Aware Feature-wise Explanations](https://arxiv.org/pdf/2310.20363.pdf), Oct. 31 2023.
- [TRIAGE: Characterizing and auditing training data for improved regression](https://arxiv.org/pdf/2310.18970.pdf), Oct. 29 2023.
- [A Simple and Efficient Baseline for Data Attribution on Images](https://arxiv.org/pdf/2311.03386.pdf), Nov. 3 2023. `dataset attribution`.

### Deconstructing nn

- [Equivariant deep weight space alignment](https://arxiv.org/pdf/2310.13397.pdf), Oct. 20 2023.
- [REPAIR: REnormalizing Permuted  Activations for Interpolation Repair](https://arxiv.org/pdf/2211.08403.pdf), Sep. 25 2023.

---

### LLMs

- [LAUGHING HYENA DISTILLERY: Extracting compact recurrences from convolutions](https://arxiv.org/pdf/2310.18780.pdf), Oct. 28 2023.
- [Proving test set contamination in black box language models](https://arxiv.org/pdf/2310.17623.pdf), Oct. 26 2023.
- [POE: Process of Elimination for Multiple Choice Reasoning](https://arxiv.org/pdf/2310.15575.pdf), Oct. 24 2023.
- [Improving generalization in large language models by learning prefix subspaces](https://arxiv.org/pdf/2310.15793.pdf), Oct. 24 2023.
- [Specialist or Generalist? Instruction Tuning for Specific NLP Tasks](https://arxiv.org/pdf/2310.15326.pdf), Oct. 23 2023.
- [Scalable neural network kernels](https://arxiv.org/pdf/2310.13225.pdf), Oct. 20 2023. `architectural bias`.
- [Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?](https://arxiv.org/pdf/2310.17774.pdf), Oct. 26 2023.
- [Meaning and understanding in large language models](https://arxiv.org/pdf/2310.17407.pdf), arXiv 2023.
- [Balancing act: constraining disparate impact in sparse models](https://arxiv.org/pdf/2310.20673.pdf), Oct. 31 2023.
- [First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models](https://arxiv.org/pdf/2311.05020.pdf), Nov. 8 2023.
  - _"We argue that disparities in scale are transient and that researchers can work to reduce them; that data, rather than hardware, is still a bottleneck for many meaningful applications; that meaningful evaluation informed by actual use is still an open problem;a nd that there is still rome for speculative approaches"_
- [Towards Verifiable Text Generation with Symbolic References](https://arxiv.org/pdf/2311.09188.pdf), Nov. 15 2023.
- [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://arxiv.org/pdf/2311.04378.pdf), Nov. 15 2023.
- [Formal Aspects of Language Modeling](https://arxiv.org/pdf/2311.04329.pdf), Nov. 9 2023. `book`.
- [On the Convergence of Encoder-only Shallow Transformers](https://arxiv.org/pdf/2311.01575.pdf), Nov. 2 2023.

#### Learning dynamics

- [The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models](https://arxiv.org/pdf/2311.05928.pdf), Nov. 10 2023.

#### Efficiency and pruning

- [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/pdf/2310.18313.pdf), Oct. 27 2023.
- [Beyond size: How gradients shape pruning decisions in large language models](https://arxiv.org/pdf/2311.04902.pdf), Nov. 8 2023.
- [Parameter-efficient orthogonal finetuning via butterfly factorization](https://arxiv.org/pdf/2311.06243.pdf), Nov. 10 2023.

#### Evaluation

- [LUNA: A model-based univeral analysis framework for large language models](https://arxiv.org/pdf/2310.14211.pdf), Oct. 22 2023.
- [AutoDAN: Automatic and interpretable adversarial attacks oni large language models](https://arxiv.org/pdf/2310.15140.pdf), Oct. 23 2023.
- [The generative AI paradox: "What it can create, it may not understand"](https://arxiv.org/pdf/2311.00059.pdf), Oct. 31 2023.
- [Paper Review: 'Sparks of Artificial General Intelligence: Early experiments with GPT-4'](https://cis.temple.edu/tagit/presentations/Sparks%20of%20AGI%20Early%20Experiments%20with%20GPT4.pdf), Oct. 4 2023.
- [Characterizing Mechanisms for Factual Recall in Language Models](https://arxiv.org/pdf/2310.15910.pdf), Oct. 24 2023.
- [Automatic Evaluation of Generative Models with Instruction Tuning](https://arxiv.org/pdf/2310.20072.pdf), Oct. 30 2023.
- [Show Your Work with Confidence: Confidence Bands for Tuning Curves](https://arxiv.org/pdf/2311.09480.pdf), Nov. 16 2023.

#### Alignment

- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf), Jul. 19 2023.
- [Persons as a way to model truthfulness in language models](https://arxiv.org/pdf/2310.17813.pdf), Oct. 30 2023.
- [Hallucination Detection for Grounded Instruction Generation](https://arxiv.org/pdf/2310.15319.pdf), Oct. 23 2023.
- [Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation](https://arxiv.org/pdf/2310.15746.pdf), Oct. 24 2023.
- [Tree Prompting: Efficient Task Adaptation without Fine-Tuning](https://arxiv.org/pdf/2310.14034.pdf), Oct. 21 2023.
- [Stabilizing RLHF through advantage model and selective rehearsal](https://arxiv.org/pdf/2309.10202.pdf), Sep. 18 2023.
- [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/pdf/2305.03047.pdf), May 4 2023.
- [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226), Oct. 3 2023.
- [The expressive power of low-rank adaptation](https://arxiv.org/pdf/2310.17513.pdf), Oct. 26 2023.
- [Vanishing Gradients in Reinforcement Finetuning of Language Models](https://arxiv.org/pdf/2310.20703.pdf), Oct. 31 2023.
- [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/pdf/2311.03287.pdf), Nov. 6 2023.
- [The alignment problem in context](https://arxiv.org/pdf/2311.02147.pdf), Nov. 3 2023.
- [LLAMAs know what gpts don't show: surrogate models for confidence estimation](https://arxiv.org/pdf/2311.08877.pdf), Nov. 15 2023.
- [Frontier language models are not robust to adversarial arithmetic, or "what do I need to say so you agree 2+2=5?"](https://arxiv.org/pdf/2311.07587.pdf), Nov. 15 2023.
- [Prudent Silence or Foolish Babble? Examining Large Language Models’ Responses to the Unknown](https://arxiv.org/pdf/2311.09731.pdf), Nov. 16 2023.
- [On measuring faithfuilness of natural language explanations](https://arxiv.org/pdf/2311.07466.pdf), Nov. 13 2023.
- [n-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering](https://arxiv.org/pdf/2311.06668.pdf), Nov. 16 2023.
- [Massive editing for LLMs via meta learning](https://arxiv.org/pdf/2311.04661.pdf), Nov. 9 2023.
- [Can LLMs follow simple rules?](https://arxiv.org/pdf/2311.04235.pdf), Nov. 6 2023.

#### Data-centric

- [DoGE: Domain reweigting with generalization estimation](https://arxiv.org/pdf/2310.15393.pdf), Oct. 23 2023.
- [Detecting pretraining from large language models](https://arxiv.org/pdf/2310.16789.pdf), Oct. 25 2023.
- [The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI](https://arxiv.org/pdf/2310.16787.pdf), Oct. 25 2023.
- [DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection](https://arxiv.org/pdf/2310.16776.pdf), Oct. 25 2023.
- [Irreducible Curriculum for Language Model Pretraining](https://arxiv.org/pdf/2310.15389.pdf), Oct. 23 2023.
- [What's in my big data?](https://arxiv.org/pdf/2310.20707.pdf), Oct. 31 2023.
- [Exploring Dataset-Scale Indicators of Data Quality](https://arxiv.org/pdf/2311.04016.pdf), Nov. 7 2023.

#### Reasoning, coding ability

- [LILO: Learning interpretable libraries by compressing and documenting code](https://arxiv.org/pdf/2310.19791.pdf), Oct. 30 2023.

#### Generalization of LLMs

- [When do prompting and prefix-tuning work? A theory of capabilities and limitations](https://arxiv.org/pdf/2310.19698.pdf), Oct. 30 2023.
- [Skill-Mix: A flexible and expandable family of evaluations for AI models](https://arxiv.org/pdf/2310.17567.pdf), Oct. 26 2023.
- [In-context learning dynamics with random binary sequences](https://arxiv.org/pdf/2310.17639.pdf), Oct. 26 2023.
- [Transformers learn high-order optimization methods for in-context learning: A study with linear models](https://arxiv.org/pdf/2310.17086.pdf), Oct. 26 2023.
- [A systematic comparison of syllogistic reasoning in humans and language models](https://arxiv.org/pdf/2311.00445.pdf), Nov. 1 2023.
- [Beyond Words: A Mathematical Framework for Interpreting Large Language Models](https://arxiv.org/pdf/2311.03033.pdf), Nov. 6 2023.
- [Connecting Pre-trained Language Models and Downstream Tasks via Properties of Representations](https://openreview.net/pdf?id=YLOJ4aKAka), `nips2023`.
- [Data Factors for Better Compositional Generalization](https://arxiv.org/pdf/2311.04420.pdf), Nov. 8 2023.

#### Emergence

- [Grokking in Linear Estimators – A Solvable Model that Groks without Understanding](https://arxiv.org/pdf/2310.16441.pdf), Oct. 25 2023.
- [Feature emergence via margin maximization: case studies in algebraic tasks](https://arxiv.org/pdf/2311.07568.pdf), Nov. 13 2023.
- [Understanding Grokking Through A Robustness Viewpoint](https://arxiv.org/pdf/2311.06597.pdf), Nov. 11 2023.
- [A theory for the sparsity emerged in the Forward Forward algorithm](https://arxiv.org/pdf/2311.05667.pdf), Nov. 9 2023.

#### Mechanistic interpretability

- [Codebook features: Sparse and discrete interpretability for neural networks](https://arxiv.org/pdf/2310.17230.pdf), Oct. 26 2023.
- [How do language models bind entities in context](https://arxiv.org/pdf/2310.17191.pdf), Oct. 26 2023.
- [Function vectors in large language models](https://arxiv.org/pdf/2310.15213.pdf), Oct. 23 2023.
- [Is probing all you need? Indicator tasks as an alternative to probing embedding spaces](https://arxiv.org/pdf/2310.15905.pdf), Oct. 24 2023.
- [What algorithms can Transformers learn? A study in length generalization](https://arxiv.org/pdf/2310.16028.pdf), Oct. 24 2023.
- [Understanding the Inner Workings of Language Models Through Representation Dissimilarity](https://arxiv.org/pdf/2310.14993.pdf), Oct. 23 2023.
- [Linear representations of sentiment in large language models](https://arxiv.org/pdf/2310.15154.pdf), Oct. 23 2023.
- [Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number](https://arxiv.org/pdf/2310.15151.pdf), Oct. 23 2023.
- [A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing](https://arxiv.org/pdf/2310.16142.pdf), Oct. 24 2023.
- [Attention Lens](https://arxiv.org/pdf/2310.16270.pdf), Oct. 25 2023.
- [Unraveling Feature Extraction Mechanisms in Neural Networks](https://arxiv.org/pdf/2310.16350.pdf), Oct. 25 2023.
- [Outlier Dimensions Encode Task-Specific Knowledge](https://arxiv.org/pdf/2310.17715.pdf), Oct. 26 2023.
- [Understanding polysemanticity and improved interpretability in neural networks through coding theory](https://www.alignmentjournal.org/assets/pdfs/archive-0.pdf), `alignment forum`. [Monosemanticity](https://www.monosemanticity.com/).
- [Seeking neural nuggets: Knowledg transfer in llms from a parametric perspective](https://arxiv.org/pdf/2310.11451.pdf), Oct. 7 2023.
- [Uncovering Meanings of Embeddings via Partial Orthogonality](https://arxiv.org/pdf/2310.17611.pdf), Oct. 26 2023.
- [Neel Nanda MATS Admissions Procedure](https://docs.google.com/document/d/1p-ggQV3vVWIQuCccXEl1fD0thJOgXimlbBpGk6FI32I/edit#heading=h.m7pqjq9qci8f).
- [Uncovering intermediate variables in Transformers using circuit probing](https://arxiv.org/pdf/2311.04354.pdf), Nov. 17 2023.
- [Future Lens: Anticipating Subsequent Tokens from a Single Hidden State](https://arxiv.org/pdf/2311.04897.pdf), Nov. 8 2023.
- [Locating Cross-Task Sequence Continuation Circuits in Transformers](https://arxiv.org/pdf/2311.04131.pdf), Nov. 7 2023.
- [The Linear Representation Hypothesis and the Geometry of Large Language Models](https://arxiv.org/pdf/2311.03658.pdf), Nov. 7 2023.
- [Mechanistically analyzing the effects of finetuning on procedurally defined tasks](https://arxiv.org/pdf/2311.12786.pdf), Nov. 21 2023.

#### In-context learning

- [Understanding in-context learning in Transformers and LLMs by learning to learn discrete functions](https://arxiv.org/pdf/2310.03016.pdf), Oct. 4 2023.
- [In-context Learning and Gradient Descent Revisited](https://arxiv.org/pdf/2311.09006.pdf), Nov. 15 2023.
- [In-context Learning and Gradient Descent Revisited](https://arxiv.org/pdf/2311.07772.pdf), Nov. 18 2023.

#### Formal language

- [Transformers as Recognizers of Formal Languages: A Survey on Expressivity](https://arxiv.org/pdf/2311.00208.pdf), Nov. 1 2023.
- [Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions](https://arxiv.org/pdf/2211.12316.pdf), Jul. 10 2023.

#### Theory

- [The Expressibility of Polynomial based Attention Scheme](https://arxiv.org/pdf/2310.20051.pdf), Oct. 30 2023.

---

### Codebase

- [FastChat](https://github.com/lm-sys/FastChat).

### Beautifual visualization

- [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/).

### Blogposts

- [Why transformative artificial intelligence is really, really hard to achieve](https://thegradient.pub/why-transformative-artificial-intelligence-is-really-really-hard-to-achieve/), Jun. 26 2023.
- [Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond](https://pytorch.org/blog/inside-the-matrix/?utm_content=265147245&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024), Sep. 25 2023.




