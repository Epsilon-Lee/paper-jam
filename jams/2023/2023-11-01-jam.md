
- [Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices](https://arxiv.org/pdf/2310.19214.pdf), Oct. 30 2023.
- [Revisiting the Learnability of Apple Tasting](https://arxiv.org/pdf/2310.19064.pdf), Oct. 29 2023.
- [Learning an inventory control policy with general inventory arrival dynamics](https://arxiv.org/pdf/2310.17168.pdf), Oct. 26 2023.
- [MaxEnt loss: Constrained maximum entropy for calibration under out-of-distribution shift](https://arxiv.org/pdf/2310.17159.pdf), Oct. 26 2023.
- [Content Moderation and the Formation of Online Communities: A Theoretical Framework](https://arxiv.org/pdf/2310.10573.pdf), Oct. 16 2023.
- [Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets](https://arxiv.org/pdf/2310.04413.pdf), Oct. 12 2023.
- [Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation](https://arxiv.org/abs/2305.09860), May 2023. `inference` `decoding`.

### Robustness

- [Spatial-frequency channels, shape bias, and adversarial robustness](https://arxiv.org/pdf/2309.13190.pdf), Sep. 22 2023.

### Calibration and uncertainty

- [Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification](https://arxiv.org/abs/2102.07856), Jul. 19 2021.
- [Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing](https://arxiv.org/pdf/2309.12236.pdf), Sep. 21 2023.
- [Uncertainty in gradient boosting via ensembles](https://arxiv.org/pdf/2006.10562.pdf), Apr. 2021.
- [Quantile extreme gradient boosting for uncertainty quantification](https://arxiv.org/pdf/2304.11732.pdf), Apr. 23 2023.

### Distribution shift

- [Learning optimal classification trees robust to distribution shifts](https://arxiv.org/pdf/2310.17772.pdf), Oct. 26 2023.
- [On the Foundations of Shortcut Learning](https://arxiv.org/pdf/2310.16228.pdf), Oct. 24 2023.
- [Performative Prediction: Past and Future](https://arxiv.org/pdf/2310.16608.pdf), Oct. 25 2023.
- [Meta-(out-of-context) learning in neural networks](https://arxiv.org/pdf/2310.15047.pdf), Oct. 24 2023.
- [Learning to (learn at test time)](https://arxiv.org/pdf/2310.13807.pdf), Oct. 20 2023.

### Robustness vs generalization

- [Group robust classification: Without any group information](https://arxiv.org/pdf/2310.18555.pdf), Oct. 28 2023.

### Data weigting

- [A challenge in reweighting data with bilevel optimization](https://arxiv.org/pdf/2310.18123.pdf), Oct. 26 2023.
- [How re-sampling helps for long-tail learning](https://arxiv.org/pdf/2310.18236.pdf), Oct. 27 2023.
- [Data optimization in deep learning: A survey](https://arxiv.org/pdf/2310.16499.pdf), Oct. 25 2023.
- [D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning](https://arxiv.org/abs/2310.07931), Oct. 11 2023.

### Representation learning

- [A spectral condition for feature learning](https://arxiv.org/pdf/2310.17813.pdf), Oct. 26 2023.
- [Ghost on the shell: An expressive representation of general 3D shapes](https://arxiv.org/pdf/2310.15168.pdf), Oct. 24 2023.
- [Combating Representation Learning Disparity with Geometric Harmonization](https://arxiv.org/pdf/2310.17622.pdf), Oct. 26 2023.
- [Flow Factorized Representation Learning](https://arxiv.org/pdf/2309.13167.pdf), Sep. 22 2023.
- [Boundary-restricted metric learning](https://link.springer.com/article/10.1007/s10994-023-06380-3), Sep. 20 2023.

### Causal inference

- [Object-centric architectures enable efficient causal representation learning](https://arxiv.org/pdf/2310.18496.pdf), Oct. 29 2023.
- [Sample complexity boudns for score-matching: Causal discovery and generative modeling](https://arxiv.org/pdf/2310.18123.pdf), Oct. 27 2023.
- [Identifying latent polynomial causal models through the lens of change](https://arxiv.org/pdf/2310.15580.pdf), Oct. 24 2023.
- [Shortcuts for causal discovery of nonlinear models by score matching](https://arxiv.org/pdf/2310.14246.pdf), Oct. 22 2023.

### Generalization mystery

- [A path to simpler model starts with noise](https://arxiv.org/pdf/2310.19726.pdf), Oct. 30 2023. `data-centric`
  - _"Our results explain a key aspect of why simpler models often tend to perform as well as black box models on complex, noisier datasets"_
- [The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data](https://arxiv.org/pdf/2310.19273.pdf), Oct. 30 2023. `uncertainty` `data-centric`.
  - _"we present Memory-Perturbation Equation which relates model's sensitivity to perturbation in its training data"_
- [Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult](https://arxiv.org/pdf/2310.17087.pdf), Oct. 26 2023. `learning dynamics`.
- [On the Neural Tangent Kernel of Equilibrium Models](https://arxiv.org/pdf/2310.14062.pdf), Oct. 21 2023.
- [Understanding the enigma of double descent: An in-depth analysis through the lens of learned feature space](https://arxiv.org/pdf/2310.13572.pdf), Oct. 20 2023.
- [Generalization Bounds for Label Noise Stochastic Gradient Descent](https://arxiv.org/pdf/2311.00274.pdf), Nov. 1 2023.
- [Fortuitous forgetting in connectionist networks](https://arxiv.org/abs/2202.00155), Feb. 1 2022.
- [A Theory of Finite-Width Neural Networks: Generalization, Scaling Laws, and the Loss Landscape](https://infoscience.epfl.ch/record/303463), 2023. `thesis`.
- [It’s an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models](https://arxiv.org/pdf/2310.09250.pdf), Oct. 13 2023.
- [Cliff-learning](https://arxiv.org/pdf/2302.07348.pdf), Jun. 7 2023.

### Continual learning

- [Progressively efficient learning](https://arxiv.org/pdf/2310.13004.pdf), Oct. 13 2023.

### Interpretability

- [Towards a fuller understanding of neurons with clustered compositional explanations](https://arxiv.org/pdf/2310.18443.pdf), Oct. 27 2023.
- [How well do feature-additive explainers explain feature-additive predictors](https://arxiv.org/pdf/2310.18496.pdf), Oct. 27 2023.
- [Sum-of-Parts Models: Faithful Attributions for Groups of Features](https://arxiv.org/pdf/2310.16316.pdf), Oct. 25 2023.
- [Learning Interpretable Rules for Scalable Data Representation and Classification](https://arxiv.org/pdf/2310.14336.pdf), Oct. 30 2023.
- [Local Universal Rule-based Explanations](https://zhuanlan.zhihu.com/p/617305431), Oct. 23 2023.
- [Explaining Interactions Between Text Spans](https://arxiv.org/pdf/2310.13506.pdf), Oct. 20 2023.
- [Intriguing properties of data attribution on diffusion models](https://arxiv.org/pdf/2311.00500.pdf), Nov. 1 2023.
- [Bridging the Human–AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero](https://arxiv.org/pdf/2310.16410.pdf), Oct. 25 2023.
- [On the Interplay between Fairness and Explainability](https://arxiv.org/pdf/2310.16607.pdf), Oct. 25 2023.
- [Measuring Association Between Labels and Free-Text Rationales](https://arxiv.org/pdf/2010.12762.pdf), Aug. 29 2022.
- [Leave-One-Out Distinguishability in Machine Learning](https://arxiv.org/pdf/2309.17310.pdf), Sep. 29 2023.
- [The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance](https://arxiv.org/pdf/2309.13775.pdf), Sep. 26 2023.

### Deconstructing nn

- [Equivariant deep weight space alignment](https://arxiv.org/pdf/2310.13397.pdf), Oct. 20 2023.
- [REPAIR: REnormalizing Permuted  Activations for Interpolation Repair](https://arxiv.org/pdf/2211.08403.pdf), Sep. 25 2023.

---

### LLMs

- [LAUGHING HYENA DISTILLERY: Extracting compact recurrences from convolutions](https://arxiv.org/pdf/2310.18780.pdf), Oct. 28 2023.
- [Proving test set contamination in black box language models](https://arxiv.org/pdf/2310.17623.pdf), Oct. 26 2023.
- [POE: Process of Elimination for Multiple Choice Reasoning](https://arxiv.org/pdf/2310.15575.pdf), Oct. 24 2023.
- [Improving generalization in large language models by learning prefix subspaces](https://arxiv.org/pdf/2310.15793.pdf), Oct. 24 2023.
- [Specialist or Generalist? Instruction Tuning for Specific NLP Tasks](https://arxiv.org/pdf/2310.15326.pdf), Oct. 23 2023.
- [Scalable neural network kernels](https://arxiv.org/pdf/2310.13225.pdf), Oct. 20 2023. `architectural bias`.
- [Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?](https://arxiv.org/pdf/2310.17774.pdf), Oct. 26 2023.
- [Meaning and understanding in large language models](https://arxiv.org/pdf/2310.17407.pdf), arXiv 2023.

#### Efficiency

- [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/pdf/2310.18313.pdf), Oct. 27 2023.

#### Evaluation

- [LUNA: A model-based univeral analysis framework for large language models](https://arxiv.org/pdf/2310.14211.pdf), Oct. 22 2023.
- [AutoDAN: Automatic and interpretable adversarial attacks oni large language models](https://arxiv.org/pdf/2310.15140.pdf), Oct. 23 2023.
- [The generative AI paradox: "What it can create, it may not understand"](https://arxiv.org/pdf/2311.00059.pdf), Oct. 31 2023.
- [Paper Review: 'Sparks of Artificial General Intelligence: Early experiments with GPT-4'](https://cis.temple.edu/tagit/presentations/Sparks%20of%20AGI%20Early%20Experiments%20with%20GPT4.pdf), Oct. 4 2023.

#### Alignment

- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf), Jul. 19 2023.
- [Persons as a way to model truthfulness in language models](https://arxiv.org/pdf/2310.17813.pdf), Oct. 30 2023.
- [Hallucination Detection for Grounded Instruction Generation](https://arxiv.org/pdf/2310.15319.pdf), Oct. 23 2023.
- [Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation](https://arxiv.org/pdf/2310.15746.pdf), Oct. 24 2023.
- [Tree Prompting: Efficient Task Adaptation without Fine-Tuning](https://arxiv.org/pdf/2310.14034.pdf), Oct. 21 2023.
- [Stabilizing RLHF through advantage model and selective rehearsal](https://arxiv.org/pdf/2309.10202.pdf), Sep. 18 2023.
- [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/pdf/2305.03047.pdf), May 4 2023.
- [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226), Oct. 3 2023.
- [The expressive power of low-rank adaptation](https://arxiv.org/pdf/2310.17513.pdf), Oct. 26 2023.

#### Data-centric

- [DoGE: Domain reweigting with generalization estimation](https://arxiv.org/pdf/2310.15393.pdf), Oct. 23 2023.
- [Detecting pretraining from large language models](https://arxiv.org/pdf/2310.16789.pdf), Oct. 25 2023.
- [The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI](https://arxiv.org/pdf/2310.16787.pdf), Oct. 25 2023.
- [DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection](https://arxiv.org/pdf/2310.16776.pdf), Oct. 25 2023.

#### Reasoning, coding ability

- [LILO: Learning interpretable libraries by compressing and documenting code](https://arxiv.org/pdf/2310.19791.pdf), Oct. 30 2023.

#### Generalization of LLMs

- [When do prompting and prefix-tuning work? A theory of capabilities and limitations](https://arxiv.org/pdf/2310.19698.pdf), Oct. 30 2023.
- [Skill-Mix: A flexible and expandable family of evaluations for AI models](https://arxiv.org/pdf/2310.17567.pdf), Oct. 26 2023.
- [In-context learning dynamics with random binary sequences](https://arxiv.org/pdf/2310.17639.pdf), Oct. 26 2023.
- [Transformers learn high-order optimization methods for in-context learning: A study with linear models](https://arxiv.org/pdf/2310.17086.pdf), Oct. 26 2023.
- [A systematic comparison of syllogistic reasoning in humans and language models](https://arxiv.org/pdf/2311.00445.pdf), Nov. 1 2023.

#### Emergence

- [Grokking in Linear Estimators – A Solvable Model that Groks without Understanding](https://arxiv.org/pdf/2310.16441.pdf), Oct. 25 2023.

#### Mechanistic interpretability

- [Codebook features: Sparse and discrete interpretability for neural networks](https://arxiv.org/pdf/2310.17230.pdf), Oct. 26 2023.
- [How do language models bind entities in context](https://arxiv.org/pdf/2310.17191.pdf), Oct. 26 2023.
- [Function vectors in large language models](https://arxiv.org/pdf/2310.15213.pdf), Oct. 23 2023.
- [Is probing all you need? Indicator tasks as an alternative to probing embedding spaces](https://arxiv.org/pdf/2310.15905.pdf), Oct. 24 2023.
- [What algorithms can Transformers learn? A study in length generalization](https://arxiv.org/pdf/2310.16028.pdf), Oct. 24 2023.
- [Understanding the Inner Workings of Language Models Through Representation Dissimilarity](https://arxiv.org/pdf/2310.14993.pdf), Oct. 23 2023.
- [Linear representations of sentiment in large language models](https://arxiv.org/pdf/2310.15154.pdf), Oct. 23 2023.
- [Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number](https://arxiv.org/pdf/2310.15151.pdf), Oct. 23 2023.
- [A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing](https://arxiv.org/pdf/2310.16142.pdf), Oct. 24 2023.
- [Attention Lens](https://arxiv.org/pdf/2310.16270.pdf), Oct. 25 2023.
- [Unraveling Feature Extraction Mechanisms in Neural Networks](https://arxiv.org/pdf/2310.16350.pdf), Oct. 25 2023.
- [Outlier Dimensions Encode Task-Specific Knowledge](https://arxiv.org/pdf/2310.17715.pdf), Oct. 26 2023.
- [Understanding polysemanticity and improved interpretability in neural networks through coding theory](https://www.alignmentjournal.org/assets/pdfs/archive-0.pdf), `alignment forum`. [Monosemanticity](https://www.monosemanticity.com/).
- [Seeking neural nuggets: Knowledg transfer in llms from a parametric perspective](https://arxiv.org/pdf/2310.11451.pdf), Oct. 7 2023.
- [Uncovering Meanings of Embeddings via Partial Orthogonality](https://arxiv.org/pdf/2310.17611.pdf), Oct. 26 2023.
- [Neel Nanda MATS Admissions Procedure](https://docs.google.com/document/d/1p-ggQV3vVWIQuCccXEl1fD0thJOgXimlbBpGk6FI32I/edit#heading=h.m7pqjq9qci8f).

#### Formal language

- [Transformers as Recognizers of Formal Languages: A Survey on Expressivity](https://arxiv.org/pdf/2311.00208.pdf), Nov. 1 2023.

---

### Codebase

- [FastChat](https://github.com/lm-sys/FastChat).

### Beautifual visualization

- [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/).

### Blogposts

- [Why transformative artificial intelligence is really, really hard to achieve](https://thegradient.pub/why-transformative-artificial-intelligence-is-really-really-hard-to-achieve/), Jun. 26 2023.
- [Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond](https://pytorch.org/blog/inside-the-matrix/?utm_content=265147245&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024), Sep. 25 2023.

