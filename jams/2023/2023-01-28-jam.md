
- [The impact of digital advertising on turnout during the 2020 US presidential election: evidence from a massive campaign-level field](https://solomonmg.github.io/pdf/acronymNHB.pdf), Nov. 2022.
- [The Impossibility of Parallelizing Boosting](https://arxiv.org/abs/2301.09627), Jan. 23 2023.
- [On the Importance of Noise Scheduling for Diffusion Models](https://arxiv.org/abs/2301.10972), Jan. 26 2023.
- [Knowledge Augmented Methods for NLP (KnowledgeNLP-AAAIâ€™23)](https://knowledge-nlp.github.io/aaai2023/publications.html).
  - Accepted papers.
- [openai: new and improved embedding model](https://openai.com/blog/new-and-improved-embedding-model/).
- [The Semantic Scholar Open Data Platform](https://arxiv.org/pdf/2301.10140.pdf), Jan. 24 2023.
  - Builds the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper authorship edges, and 2.4B+ citation edges.
- [New Insights into Multi-Calibration](https://arxiv.org/pdf/2301.08837.pdf), Jan. 21 2023. `fairness` `theory`.
- [Extracting Training Data from Diffusion Models](https://arxiv.org/abs/2301.13188), Jan. 30 2023. [tweet](https://twitter.com/Eric_Wallace_/status/1620449934863642624?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email).
- [A Survey of Machine Unlearning](https://arxiv.org/pdf/2209.02299.pdf), Oct. 21 2022.
- [GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks](https://openreview.net/forum?id=rqq6Dh8t4d), `iclr2023`.
- [Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://arxiv.org/pdf/2301.08243.pdf), Jan. 19 2023. LeCun's JEPA arch. for self-supervised learning?
  - I-JEPA (Image-based Joint Embedding Predictive Architecture)
  - _"we find I-JEPA to be highly scalable"_
  - _"we train a ViT-Huge/16 on ImageNet using 32 A100 GPUs in under 38 hours to achieve strong downstream performance across a wide range of tasks requiring various levels of abstraction, from linear classification to object counting and depth prediction."_
- [Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?](https://arxiv.org/pdf/1912.01094.pdf), relationship between fairness and accuracy. Dec. 2019.
- [Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons](https://arxiv.org/pdf/2301.11270.pdf), Jan. 30 2023.
- [Scaling laws for single-agent reinforcement learning](https://arxiv.org/pdf/2301.13442.pdf), Jan. 31 2023. `scaling law`.
- [PAC Prediction Sets for Large Language Models of Code](https://www.seas.upenn.edu/~akhakhar/pacsetllm.pdf), `icml2022`.

### Evaluation paradigm

- [Evaluation Examples Are Not Equally Informative: How Should That Change NLP Leaderboards?](https://aclanthology.org/2021.acl-long.346.pdf), `acl2021`.
- [Machine Learning Testing: Survey, Landscapes and Horizons](https://arxiv.org/pdf/1906.10742.pdf), Dec. 2019.

### Online learning

- [Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks](https://openreview.net/forum?id=gUTKBS34Q5c), `tmlr2023`. [tweet](https://twitter.com/ElaheArani/status/1620483890564718592).
  - Deep neural networks (DNNs) are often trained with the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be computationally inefficient, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce \textit{Learn, Unlearn, and Relearn (LURE)} an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalizable features. We show that our training paradigm provides consistent performance gains across datasets in both classification and few-shot settings. We further show that it leads to more robust and well-calibrated models.
- [A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning](https://www.sciencedirect.com/science/article/pii/S089360802300014X?via%3Dihub), March 2023.

### Data-centric

- [Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP](https://arxiv.org/pdf/2110.03618.pdf), Oct. 19 2021. `independent causal mechanism`.
- [A streamlined approach to online linguistic surveys](https://link.springer.com/article/10.1007/s11049-015-9305-9), 2016. `crowdsourcing`.

### Multilinguality

- [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472), Jan. 25 2023. [how to use](https://twitter.com/LiangDavis/status/1618738467315531777).

### Generative models

- [Awesome Normalizing Flows](https://github.com/janosh/awesome-normalizing-flows).
  - A list of awesome resources for understanding and applying normalizing flows (NF): a relatively simple yet powerful new tool in statistics for constructing expressive probability distributions from simple base distributions using a chain (flow) of trainable smooth bijective transformations (diffeomorphisms).

### Text Generation

- [On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation](https://arxiv.org/abs/2205.16001), Nov. 20 2022 `evaluation`.

### LLMs

- [DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature](https://arxiv.org/abs/2301.11305), Jan. 26 2023.
- [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226), Jan. 28 2023.
- [A list of papers on scaling laws](https://docs.google.com/spreadsheets/d/1XHU0uyCojH6daSWEq9d1SHnlrQVW7li8iqBMasawMns/edit#gid=0), Jan. 28 2023.
- [ExaRanker: Explanation-Augmented Neural Ranker](https://arxiv.org/abs/2301.10521), Jan. 25 2023. `neural ir`.
- [Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again](https://arxiv.org/abs/2203.08410), Nov. 2022. `ie`.
  - [Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning](https://openreview.net/forum?id=9EAQVEINuum), `iclr2023`. `ie` `ner`.
- [Large language models generate functional protein sequences across diverse families](https://www.nature.com/articles/s41587-022-01618-2), `nature biotechnology`. Jan. 26 2023.
  - _"The model was trained on 280 million protein sequences from >19,000 families and is augmented with control tags specifying protein properties."_
  - _"Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4%."_
- [Extremely Small BERT Models from Mixed-Vocabulary Training](https://arxiv.org/abs/1909.11687), Feb. 6. 2021. [tweet](https://twitter.com/DavidMezzetti/status/1619316525328105472), a discussion on efficiency tradeoff. `efficient nlp`.
- [MegaBlocks](https://github.com/stanford-futuredata/megablocks). `efficient nlp`.
  - MegaBlocks is a light-weight library for mixture-of-experts (MoE) training. The core of the system is efficient "dropless-MoE" ([dMoE](https://github.com/stanford-futuredata/megablocks/blob/main/megablocks/layers/dmoe.py), [paper](https://arxiv.org/abs/2211.15841)) and standard [MoE](https://github.com/stanford-futuredata/megablocks/blob/main/megablocks/layers/moe.py) layers.
  - MegaBlocks is built on top of [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), where we support data, expert and pipeline parallel training of MoEs. We're working on extending more frameworks to support MegaBlocks.
- [Call for Papers - The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus](https://arxiv.org/pdf/2301.11796.pdf), Jan. 27 2023.
- [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652), Jan. 30 2023. [tweet](https://twitter.com/WeijiaShi2/status/1620497381962977281).
- [Efficient Scaling Transformer Inference](https://arxiv.org/pdf/2211.05102.pdf), Nov. 9 2022. `efficient nlp`.
- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416), Dec. 6 2022. `scaling-up`. [tweet](https://twitter.com/ShayneRedford/status/1620805305801261058).
- [Looped Transformers as Programmable Computers](https://twitter.com/DimitrisPapail/status/1620834409275609088), 30 Jan 2023.
- [Benchmarking Large Language Models for News Summarization](https://arxiv.org/pdf/2301.13848.pdf), Jan. 31 2023.
- [Predictability and Surprise in Large Generative Models](https://arxiv.org/pdf/2202.07785.pdf), Oct. 3 2022. `calibration`.
- [Language Models (Mostly) Know What They Know](https://arxiv.org/pdf/2207.05221.pdf), Nov. 21 2022. [tweet](https://twitter.com/AnthropicAI/status/1547250801130713090). `calibration`.
- [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465), Mar. 22 2022. [tweet](https://twitter.com/RazRazcle/status/1622393593930612736).
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629), Oct. 6 2022. [tweet](https://twitter.com/RazRazcle/status/1622033232232611841).
- [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://arxiv.org/abs/2210.10341), Oct. 19 2022. [github](https://github.com/microsoft/BioGPT). [tweet](https://twitter.com/katieelink/status/1622635429202898944).
- [Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery](https://arxiv.org/abs/2302.03668), Feb. 7 2023. [tweet](Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery).
- [Exploring the Benefits of Training Expert Language Models over Instruction Tuning](https://arxiv.org/abs/2302.03202), Feb. 7 2023. [tweet](https://twitter.com/jang_yoel/status/1623169024489328640).

#### Finetuning

- [Downstream Datasets Make Surprisingly Good Pretraining Corpora](https://arxiv.org/abs/2209.14389), Sep. 28 2022.

#### Products

- [multion.ai](https://multion.ai/). Don't browse the web alone. Use MultiÂ·ON to get things done! The world's first AI Web Co-Pilot powered by ChatGPT.
- [meru](https://www.usemeru.com/). You're an API Call Away from AI. For optimal speed, pricing, and scalability, embed Meru's managed API endpoints directly into your application. Unlock the power of Generative AI.
- [Ghostwrite: ChatGPT Email Assistant](https://chrome.google.com/webstore/detail/ghostwrite-chatgpt-email/fbjnnjochaopepfjpngghafgnafebkjh?hl=en&authuser=0&twclid=24e9qdqilyu5n62d17oipt8o48).
- [tabnine](https://www.tabnine.com/), AI assistant for software developers. Code faster with whole-line & full-function code completions.
- [ActGPT - chatbot that controls browser](https://github.com/yihui-he/ActGPT), chatbot does what you ask, like open Google search, post a Tweet, etc.

### Blogposts

- [Just know stuff. (Or, how to achieve success in a machine learning PhD.)](https://kidger.site/thoughts/just-know-stuff/), Jan. 28 2023.
- [From-0-to-Research-Scientist-resources-guide](https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide).
  - Detailed and tailored guide for undergraduate students or anybody want to dig deep into the field of AI with solid foundation.
- [intro_machine_learning](https://github.com/PrincetonUniversity/intro_machine_learning).
  - This mini-course will provide a comprehensive introduction to machine learning. Part 1 will briefly overview the full machine learning process and cover introductory concepts such as what is machine learning and why is it used. Popular software libraries will be discussed. Attendees will begin working hands-on in Part 2 to train simple machine learning models. Part 3 covers model evaluation and refinement. Artificial neural networks are introduced during Part 4. The mini-course concludes with a hackathon during Part 5 where participants will work on a small, end-to-end machine learning project chosen from one of multiple domains.
- [cs197](https://www.cs197.seas.harvard.edu/). [course book](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#).
- [Copilot Internals](https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals).
- [Tweet: Yandex source code leakage](https://twitter.com/alex_buraks/status/1618988134850785280).
  - _"The most interesting part for SEO community is: the list of all 1922 ranking factors used in the search algorithm "_
  - [YANDEX SERVICES SOURCE CODE LEAK](https://arseniyshestakov.com/2023/01/26/yandex-services-source-code-leak/).
- [Tweet: Random quick note on Transformer block unification.](https://twitter.com/karpathy/status/1619500957196484609), Jan. 29 2023.
- [Podcast: Is scaling all you need for AI Large Language Models? Scaling laws and the Inverse Scaling Challenge](https://www.youtube.com/watch?v=ppPUznLDqMY), Jan. 27 2023.
- [Blogpost: Mini Blog Post 15: The illusion of doing nothing](https://www.neelnanda.io/blog/mini-blog-post-15-the-illusion-of-doing-nothing).
- [Blogpost: On the architecture of torchdynamo](https://docs.google.com/document/d/13K03JN4gkbr40UMiW4nbZYtsw8NngQwrTRnL3knetGM/edit#), internal pytorch building block and compiler.
- [Techniques for label conditioning in Gaussian DDPMs](https://beckham.nz/2023/01/27/ddpms_guidance.html), Jan. 27 2023.
- [Trends in Training Dataset Sizes](https://epochai.org/blog/trends-in-training-dataset-sizes), Sep. 20 2022.
  - We collected over 200 notable ML models and estimated their training dataset size.
  - Vision and language datasets have historically grown at 0.1 and 0.2 orders of magnitude (OOMs) per year, respectively.
  - There seems to be some transition around 2014-2015, after which training datasets became much bigger and (in the case of language) smaller datasets disappeared. This might be just an artefact of our small sample size.
  - We also provide trends for games, speech, recommendation and drawing, but since our sample size is very small in these domains we would advise some level of scepticism.
- [A PhD in Numbers](https://davidstutz.de/a-phd-in-numbers/), Jan. 8 2023.
  - Conducting PhD research can be a long endeavor, involving much more than the publications listed on Google Scholar. As I recently submitted my thesis, in this article, I look back on my time as PhD researcher in terms of numbers. This way, I hope to shed some light on what a PhD can look like in terms of everyday work.
- [How Nvidiaâ€™s CUDA Monopoly In Machine Learning Is Breaking - OpenAI Triton And PyTorch 2.0](https://www.semianalysis.com/p/nvidiaopenaitritonpytorch), Jan. 16 2023.


### Others

- [Tweet: 5 main ideas/components that make NeRFs so effective](https://twitter.com/cwolferesearch/status/1620155302674055169).
  





