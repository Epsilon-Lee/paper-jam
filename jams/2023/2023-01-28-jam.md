
- [The Impossibility of Parallelizing Boosting](https://arxiv.org/abs/2301.09627), Jan. 23 2023.
- [On the Importance of Noise Scheduling for Diffusion Models](https://arxiv.org/abs/2301.10972), Jan. 26 2023.
- [Knowledge Augmented Methods for NLP (KnowledgeNLP-AAAI’23)](https://knowledge-nlp.github.io/aaai2023/publications.html).
  - Accepted papers.
- [openai: new and improved embedding model](https://openai.com/blog/new-and-improved-embedding-model/).
- [The Semantic Scholar Open Data Platform](https://arxiv.org/pdf/2301.10140.pdf), Jan. 24 2023.
  - Builds the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper authorship edges, and 2.4B+ citation edges.
- [New Insights into Multi-Calibration](https://arxiv.org/pdf/2301.08837.pdf), Jan. 21 2023. `fairness` `theory`.
- [Extracting Training Data from Diffusion Models](https://arxiv.org/abs/2301.13188), Jan. 30 2023. [tweet](https://twitter.com/Eric_Wallace_/status/1620449934863642624?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email).
- [A Survey of Machine Unlearning](https://arxiv.org/pdf/2209.02299.pdf), Oct. 21 2022.
- [GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks](https://openreview.net/forum?id=rqq6Dh8t4d), `iclr2023`.

### Online learning

- [Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks](https://openreview.net/forum?id=gUTKBS34Q5c), `tmlr2023`. [tweet](https://twitter.com/ElaheArani/status/1620483890564718592).
  - Deep neural networks (DNNs) are often trained with the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be computationally inefficient, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce \textit{Learn, Unlearn, and Relearn (LURE)} an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalizable features. We show that our training paradigm provides consistent performance gains across datasets in both classification and few-shot settings. We further show that it leads to more robust and well-calibrated models.
- [A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning](https://www.sciencedirect.com/science/article/pii/S089360802300014X?via%3Dihub), March 2023.

### Multilinguality

- [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472), Jan. 25 2023. [how to use](https://twitter.com/LiangDavis/status/1618738467315531777).

### Generative models

- [Awesome Normalizing Flows](https://github.com/janosh/awesome-normalizing-flows).
  - A list of awesome resources for understanding and applying normalizing flows (NF): a relatively simple yet powerful new tool in statistics for constructing expressive probability distributions from simple base distributions using a chain (flow) of trainable smooth bijective transformations (diffeomorphisms).

### Text Generation

- [On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation](https://arxiv.org/abs/2205.16001), Nov. 20 2022 `evaluation`.

### LLMs

- [DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature](https://arxiv.org/abs/2301.11305), Jan. 26 2023.
- [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226), Jan. 28 2023.
- [A list of papers on scaling laws](https://docs.google.com/spreadsheets/d/1XHU0uyCojH6daSWEq9d1SHnlrQVW7li8iqBMasawMns/edit#gid=0), Jan. 28 2023.
- [ExaRanker: Explanation-Augmented Neural Ranker](https://arxiv.org/abs/2301.10521), Jan. 25 2023. `neural ir`.
- [Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again](https://arxiv.org/abs/2203.08410), Nov. 2022. `ie`.
  - [Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning](https://openreview.net/forum?id=9EAQVEINuum), `iclr2023`. `ie` `ner`.
- [Large language models generate functional protein sequences across diverse families](https://www.nature.com/articles/s41587-022-01618-2), `nature biotechnology`. Jan. 26 2023.
  - _"The model was trained on 280 million protein sequences from >19,000 families and is augmented with control tags specifying protein properties."_
  - _"Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4%."_
- [Extremely Small BERT Models from Mixed-Vocabulary Training](https://arxiv.org/abs/1909.11687), Feb. 6. 2021. [tweet](https://twitter.com/DavidMezzetti/status/1619316525328105472), a discussion on efficiency tradeoff. `efficient nlp`.
- [MegaBlocks](https://github.com/stanford-futuredata/megablocks). `efficient nlp`.
  - MegaBlocks is a light-weight library for mixture-of-experts (MoE) training. The core of the system is efficient "dropless-MoE" ([dMoE](https://github.com/stanford-futuredata/megablocks/blob/main/megablocks/layers/dmoe.py), [paper](https://arxiv.org/abs/2211.15841)) and standard [MoE](https://github.com/stanford-futuredata/megablocks/blob/main/megablocks/layers/moe.py) layers.
  - MegaBlocks is built on top of [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), where we support data, expert and pipeline parallel training of MoEs. We're working on extending more frameworks to support MegaBlocks.
- [Call for Papers - The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus](https://arxiv.org/pdf/2301.11796.pdf), Jan. 27 2023.
- [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652), Jan. 30 2023. [tweet](https://twitter.com/WeijiaShi2/status/1620497381962977281).

#### Products

- [multion.ai](https://multion.ai/). Don't browse the web alone. Use Multi·ON to get things done! The world's first AI Web Co-Pilot powered by ChatGPT.
- [meru](https://www.usemeru.com/). You're an API Call Away from AI. For optimal speed, pricing, and scalability, embed Meru's managed API endpoints directly into your application. Unlock the power of Generative AI.
- [Ghostwrite: ChatGPT Email Assistant](https://chrome.google.com/webstore/detail/ghostwrite-chatgpt-email/fbjnnjochaopepfjpngghafgnafebkjh?hl=en&authuser=0&twclid=24e9qdqilyu5n62d17oipt8o48).
- [tabnine](https://www.tabnine.com/), AI assistant for software developers. Code faster with whole-line & full-function code completions.
- [ActGPT - chatbot that controls browser](https://github.com/yihui-he/ActGPT), chatbot does what you ask, like open Google search, post a Tweet, etc.

### Blogposts

- [Just know stuff. (Or, how to achieve success in a machine learning PhD.)](https://kidger.site/thoughts/just-know-stuff/), Jan. 28 2023.
- [From-0-to-Research-Scientist-resources-guide](https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide).
  - Detailed and tailored guide for undergraduate students or anybody want to dig deep into the field of AI with solid foundation.
- [intro_machine_learning](https://github.com/PrincetonUniversity/intro_machine_learning).
  - This mini-course will provide a comprehensive introduction to machine learning. Part 1 will briefly overview the full machine learning process and cover introductory concepts such as what is machine learning and why is it used. Popular software libraries will be discussed. Attendees will begin working hands-on in Part 2 to train simple machine learning models. Part 3 covers model evaluation and refinement. Artificial neural networks are introduced during Part 4. The mini-course concludes with a hackathon during Part 5 where participants will work on a small, end-to-end machine learning project chosen from one of multiple domains.
- [cs197](https://www.cs197.seas.harvard.edu/). [course book](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#).
- [Copilot Internals](https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals).
- [Tweet: Yandex source code leakage](https://twitter.com/alex_buraks/status/1618988134850785280).
  - _"The most interesting part for SEO community is: the list of all 1922 ranking factors used in the search algorithm "_
  - [YANDEX SERVICES SOURCE CODE LEAK](https://arseniyshestakov.com/2023/01/26/yandex-services-source-code-leak/).
- [Tweet: Random quick note on Transformer block unification.](https://twitter.com/karpathy/status/1619500957196484609), Jan. 29 2023.
- [Podcast: Is scaling all you need for AI Large Language Models? Scaling laws and the Inverse Scaling Challenge](https://www.youtube.com/watch?v=ppPUznLDqMY), Jan. 27 2023.
- [Blogpost: Mini Blog Post 15: The illusion of doing nothing](https://www.neelnanda.io/blog/mini-blog-post-15-the-illusion-of-doing-nothing).
- [Blogpost: On the architecture of torchdynamo](https://docs.google.com/document/d/13K03JN4gkbr40UMiW4nbZYtsw8NngQwrTRnL3knetGM/edit#), internal pytorch building block and compiler.
- [Techniques for label conditioning in Gaussian DDPMs](https://beckham.nz/2023/01/27/ddpms_guidance.html), Jan. 27 2023.

### Others

- [Tweet: 5 main ideas/components that make NeRFs so effective](https://twitter.com/cwolferesearch/status/1620155302674055169).





