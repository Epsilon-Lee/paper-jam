
- [Mixture Proportion Estimation Beyond Irreducibility](https://arxiv.org/pdf/2306.01253.pdf), Jun. 2 2023.
- [DocFormerv2: Local Features for Document Understanding](https://arxiv.org/pdf/2306.01733.pdf), Jun. 2 2023.
- [A Data-Driven Measure of Relative Uncertainty for Misclassification Detection](https://arxiv.org/pdf/2306.01710.pdf), Jun. 2 2023. `uncertainty` `trustworthy`.
- [Black-box anomaly attribution](https://arxiv.org/pdf/2305.18440.pdf), May 29 2023.
- [Doubly Robust Self-Training](https://arxiv.org/pdf/2306.00265.pdf), Jun. 1 2023.
- [When Does Optimizing a Proper Loss Yield Calibration?](https://arxiv.org/pdf/2305.18764.pdf), May 30 2023. `uncertainty`.
- [Soft Merging of Experts with Adaptive Routing](https://arxiv.org/pdf/2306.03745.pdf), Jun. 6 2023. `model merging`.
- [Time Interpret: a Unified Model Interpretability Library for Time Series](https://arxiv.org/pdf/2306.02968.pdf), Jun. 6 2023. `interpretability`. [code](https://github.com/josephenguehard/time_interpret).
- [Active Learning from the Web](https://arxiv.org/pdf/2210.08205.pdf), Feb. 10 2023. [code](https://github.com/joisino/seafaring).
- [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking](https://arxiv.org/pdf/2306.05426.pdf), Jun. 8 2023. `exposure bias`.
- [Feature Programming for Multivariate Time Series Prediction](https://arxiv.org/pdf/2306.06252.pdf), Jun. 9 2023.
- [Transformer Taxonomy (the last lit review)](https://kipp.ly/blog/transformer-taxonomy/), Mar. 30 2023.
- [RankFormer: Listwise Learning-to-Rank Using Listwide Labels](https://arxiv.org/pdf/2306.05808.pdf), Jun. 9 2023. `learning to rank`.
- [One-sided Matrix Completion from Two Observations Per Row](https://arxiv.org/pdf/2306.04049.pdf), Jun. 6 2023. `matrix completion`.
- [A brief review of hypernetworks in deep learning](https://arxiv.org/pdf/2306.06955.pdf), Jun. 12 2023.
- [Generating Synthetic Datasets by Interpolating along Generalized Geodesics](https://arxiv.org/pdf/2306.06866.pdf), Jun. 12 2023.
- [Proximity-Informed Calibration for Deep Neural Networks](https://arxiv.org/pdf/2306.04590.pdf), Jun. 7 2023. `calibration` `proximity bias`
  - proximity bias, where models tend to be more overconfident in low proximity data (i.e. data lying in the sparse region of the data distribution)
  - _"we examine the problem over 504 pretrained ImageNet models"_
  - _"Models tend to overfit more heavily on low proximity samples than on high proximity samples"_
  - _"ProCal is effective in addressing proximity bias and improving calibration on balanced, long-tail, and distribution-shift settings"_
- [Exploiting Intent Evolution in E-commercial Query Recommendation](https://assets.amazon.science/73/1b/ed83221c4ba7914b93a8af32fb0d/exploiting-intent-evolution-in-e-commercial-query-recommendation.pdf), `kdd2023`.
- [WIZMAP: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings](https://arxiv.org/pdf/2306.09328.pdf), Jun. 15 2023. [code](https://github.com/poloclub/wizmap).
- [Block-State Transformers](https://arxiv.org/pdf/2306.09539.pdf), Jun. 15 2023. `ssm` for `lm`, `jax`.
- [On achieving optimal adversarial test error](https://arxiv.org/pdf/2306.07544.pdf), Jun. 13 2023. `adversarial training`.
- [“Private Prediction Strikes Back!” Private Kernelized Nearest Neighbors with Individual Renyi Filter](https://arxiv.org/pdf/2306.07381.pdf), Jun. 12 2023. `Yu-Xiang Wang`.
- [A Unified Model and Dimension for Interactive Estimation](https://arxiv.org/pdf/2306.06184.pdf), Jun. 9 2023. `interactive learning`.
- [An Overview of Catastrophic AI Risks](https://arxiv.org/pdf/2306.12001.pdf), Jun. 21 2023. `ai safety`.
- [Scaling MLPs: A Tale of Inductive Bias](https://arxiv.org/pdf/2306.13575.pdf), Jun. 23 2023.

### Dataset selection

- [GIO: Gradient Information Optimization for Training Dataset Selection](https://arxiv.org/pdf/2306.11670.pdf), Jun. 20 2023.
- [Towards Sustainable Learning: Coresets for Data-efficient Deep Learning](https://arxiv.org/pdf/2306.01244.pdf), Jun. 2 2023. `coreset`.
  
### AUC optimization

- [Does it pay to optimize AUC?](https://arxiv.org/pdf/2306.01528.pdf), Jun. 2 2023.
- [LibAUC: A Deep Learning Library for X-Risk Optimization](https://arxiv.org/pdf/2306.03065.pdf), Jun. 5 2023. `direct optimization`. [code](https://github.com/Optimization-AI/LibAUC).

### Clustering

- [End-to-end Differentiable Clustering with Associative Memories](https://arxiv.org/pdf/2306.03209.pdf), Jun. 5 2023.
- [Interpretable Deep Clustering](https://arxiv.org/pdf/2306.04785.pdf), Jun. 7 2023.

### MLware engineering

- [Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models](https://arxiv.org/pdf/2306.04529.pdf), Jun. 7 2023.

### Inductive bias

- [Cycle Consistency Driven Object Discovery](https://arxiv.org/pdf/2306.02204.pdf), Jun. 3 2023.

### Hyperparameter search

- [Optimizing Millions of Hyperparameters by Implicit Differentiation](http://proceedings.mlr.press/v108/lorraine20a/lorraine20a.pdf), `aistats2020`.
- [Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels](https://arxiv.org/pdf/2306.03968.pdf), Jun. 6 2023.

### Grow model to get specific properties

- [GrowNet](https://arxiv.org/pdf/2002.07971.pdf), 2020. `nips2020 rejected`.
- [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196), 2017. `citation: 6k+`.
- [Understanding Progressive Training Through the Framework of Randomized Coordinate Descent](https://arxiv.org/pdf/2306.03626.pdf), Jun. 6 2023.

### Generative model new frontiers

- [Conditionally Strongly Log-Concave Generative Models](https://arxiv.org/pdf/2306.00181.pdf), May 31 2023.

### RLHF theory

- [How to Query Human Feedback Efficiently in RL?](https://arxiv.org/pdf/2305.18505.pdf), May 29 2023.
~~- [Random Feedback Alignment Algorithms to train Neural Networks: Why do they Align?](https://arxiv.org/pdf/2306.02325.pdf), Jun. 4 2023.~~
  - This is not RLHF but a method to train NNs beyond backpropagation.

### Learning dynamics and generalization of neural networks

- [Benign Overfitting in Deep Neural Networks under Lazy Training](https://arxiv.org/pdf/2305.19377.pdf), May 30 2023.
- [Quantifying Overfitting: Evaluating Neural Network Performance through Analysis of Null Space](https://arxiv.org/pdf/2305.19424.pdf), May 30 2023.
- [On Emergence of Clean-Priority Learning in Early Stopped Neural Networks](https://arxiv.org/pdf/2306.02533.pdf), Jun. 5 2023.
- [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning](https://arxiv.org/pdf/2306.04815.pdf), Jun. 7 2023.
- [Benchmarking Neural Network Training Algorithms](https://arxiv.org/pdf/2306.07179.pdf), Jun. 12 2023.
- [Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning](https://arxiv.org/pdf/2306.08590.pdf), Jun. 14 2023.
- [Understanding optimization of deep learning](https://arxiv.org/pdf/2306.09338.pdf), Jun. 15 2023.
- [Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok](https://arxiv.org/pdf/2306.13253.pdf), Jun. 23 2023.
- [The Inductive Bias of Flatness Regularization for Deep Matrix Factorization](https://arxiv.org/pdf/2306.13239.pdf), Jun. 22 2023.

#### Other characteristics of neural networks

- [Exact Count of Boundary Pieces of ReLU Classifiers: Towards the Proper Complexity Measure for Classification](https://arxiv.org/pdf/2306.08805.pdf), Jun. 15 2023.

### Data augmentation

- [Scaling up semi-supervised learning with unconstrained unlabeled data](https://arxiv.org/pdf/2306.01222.pdf), Jun. 2 2023. `UnMixMatch`.

### Data structure

- [EEL: Efficiently Encoding Lattices for Reranking](https://arxiv.org/pdf/2306.00947.pdf), Jun. 1 2023.

### Building blocks of neural architecture

- [Centered Self-Attention Layers](https://arxiv.org/pdf/2306.01610.pdf), Jun. 2 2023.

### Deep survival analysis

- [An Effective Meaningful Way to Evaluate Survival Models](https://arxiv.org/pdf/2306.01196.pdf), Jun. 1 2023.
- [Deep Learning for Survival Analysis](https://humboldt-wi.github.io/blog/research/information_systems_1920/group2_survivalanalysis/#deeplearning_sa), Feb. 6 2020.

### Continual/multitask learning

- [OMNI: Open-endedness via Models of human Notions of Interestingness](https://arxiv.org/pdf/2306.01711.pdf), Jun. 2 2023.
- [Resolving Interference When Merging Models](https://arxiv.org/pdf/2306.01708.pdf), Jun. 2 2023.
- [GateON: an unsupervised method for large scale continual learning](https://arxiv.org/pdf/2306.01690.pdf), Jun. 2 2023.
- [Improved Active Multi-Task Representation Learning via Lasso](https://arxiv.org/pdf/2306.02556.pdf), Jun. 5 2023.
- [Continual Learning in Linear Classification on Separable Data](https://arxiv.org/pdf/2306.03534.pdf), Jun. 6 2023. `continual learning theory`.
- [A Probabilistic Framework for Modular Continual Learning](https://arxiv.org/pdf/2306.06545.pdf), Jun. 11 2023.
- [Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning](https://openreview.net/pdf?id=SykskBAkZL), `icml2023`.

### Adversarial training

- [Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training](https://arxiv.org/pdf/2306.01271.pdf), Jun. 2 2023.

### Community detection

- [Semi-supervised community detection via structural similarity metrics](https://arxiv.org/pdf/2306.01089.pdf), Jun. 1 2023.

### Distribution shift

- [Measuring the Robustness of Natural Language Processing Models to Domain Shifts](https://arxiv.org/pdf/2306.00168.pdf), May 31 2023.
- [ELSA: Efficient Label Shift Adaptation through the Lens of Semiparametric Models](https://arxiv.org/pdf/2305.19123.pdf), May 30 2023.
- [Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms](https://arxiv.org/pdf/2305.19570.pdf), May 31 2023.
- [Deep into The Domain Shift: Transfer Learning through Dependence Regularization](https://arxiv.org/pdf/2305.19499.pdf), May 31 2023.
- [An Adaptive Method for Weak Supervision with Drifting Data](https://arxiv.org/pdf/2306.01658.pdf), Jun. 2 2023.
- [On Pitfalls of Test-Time Adaptation](https://arxiv.org/pdf/2306.03536.pdf), Jun. 6 2023.
- [Partial Identifiability for Domain Adaptation](https://arxiv.org/pdf/2306.06510.pdf), Jun. 10 2023.
- [Explaining Predictive Uncertainty with Information Theoretic Shapley Values](https://arxiv.org/pdf/2306.05724.pdf), Jun. 9 2023. `shapley values` `distribution shift`.
- [On Minimizing the Impact of Dataset Shifts on Actionable Explanations](https://arxiv.org/pdf/2306.06716.pdf), Jun. 11 2023.
- [Taxonomy-Structured Domain Adaptation](https://arxiv.org/pdf/2306.07874.pdf), Jun. 13 2023.
- [Kalman Filter for Online Classification of Non-Stationary Data](https://arxiv.org/pdf/2306.08448.pdf), Jun. 14 2023.

### Old-school nlp

- [Back to patterns: efficient Japanese morphological analysis with feature-sequence trie](https://arxiv.org/pdf/2305.19045.pdf), May 30 2023.
- [Deriving Language Models from Masked Language Models](https://arxiv.org/pdf/2305.15501.pdf), May 24 2023.
- [PromptNER: Prompting For Named Entity Recognition](https://arxiv.org/pdf/2305.15444.pdf), May 24 2023.
- [Linear Classifier: An Often-Forgotten Baseline for Text Classification](https://arxiv.org/pdf/2306.07111.pdf), Jun. 12 2023.

### Representation learning

- [On the impact of activation and normalization in obtaining isometric embeddings at initialization](https://arxiv.org/pdf/2305.18399.pdf), May 28 2023.
- [Rethinking Weak Supervision in Helping Contrastive Learning](https://arxiv.org/pdf/2306.04160.pdf), Jun. 7 2023.
- [FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy](https://arxiv.org/pdf/2306.05268.pdf), Jun. 8 2023.
- [On the Joint Interaction of Models, Data, and Features](https://arxiv.org/pdf/2306.04793.pdf), Jun. 7 2023.
- [Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://arxiv.org/pdf/2301.08243.pdf), Apr. 13 2023.

### Transfer learning

- [Resolving Interference When Merging Models](https://arxiv.org/pdf/2306.01708.pdf), Jun. 2 2023.

### LLMs

- [Let’s Verify Step by Step](https://arxiv.org/pdf/2305.20050.pdf), May 31 2023.
- [Test-Time Training on Nearest Neighbors for Large Language Models](https://arxiv.org/pdf/2305.18466.pdf), May 29 2023.
  - [AdANNS: A Framework for Adaptive Semantic Search](https://arxiv.org/pdf/2305.19435.pdf), May 30 2023. `fast ann search`.
- [Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://arxiv.org/pdf/2305.04388.pdf), May 7 2023.
- [Did ChatGPT cheat on your test?](https://hitz-zentroa.github.io/lm-contamination/blog/).
- [Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions](https://arxiv.org/pdf/2306.02224.pdf), Jun. 4 2023. [code](https://github.com/Significant-Gravitas/Auto-GPT). 
- [Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference](https://arxiv.org/abs/2304.04947), Apr. 11 2023.
- [Can Large Language Models Infer Causation from Correlation?](https://arxiv.org/pdf/2306.05836.pdf), Jun. 9 2023.
- [FinGPT: Open-Source Financial Large Language Models](https://arxiv.org/pdf/2306.06031.pdf), Jun. 9 2023. [website](https://ai4finance-foundation.github.io/FinNLP/).
- [Turning large language models into cognitive models](https://arxiv.org/pdf/2306.03917.pdf), Jun. 6 2023.
- [Large Language Models Sometimes Generate Purely Negatively-Reinforced Text](https://arxiv.org/pdf/2306.07567.pdf), Jun. 16 2023.
- [Long-range Language Modeling with Self-retrieval](https://arxiv.org/pdf/2306.13421.pdf), Jun. 23 2023.
- [System-Level Natural Language Feedback](https://arxiv.org/pdf/2306.13588.pdf), Jun. 23 2023.

#### Factuality

- [Editing Factual Knowledge in Language Models](https://aclanthology.org/2021.emnlp-main.522.pdf), `emnlp2021`.
- [Self-consistency improves chain of thought reasoning in language models](https://arxiv.org/pdf/2203.11171.pdf), Mar. 7 2023.
- [Measuring and Modifying Factual Knowledge in Large Language Models](https://arxiv.org/pdf/2306.06264.pdf), Jun. 9 2023.
- [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/abs/2306.03341), Jun. 7 2023.
- [A Mathematical Abstraction for Balancing the Trade-off Between Creativity and Reality in Large Language Models](https://arxiv.org/pdf/2306.02295.pdf), Jun. 4 2023.
- [Evaluating Superhuman Models with Consistency Checks](https://arxiv.org/pdf/2306.09983.pdf), Jun. 16 2023.

#### Instruction tuning

- [AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/pdf/2010.15980.pdf), Nov. 7 2020.
- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf), Dec. 6 2022.
- [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751), Jun. 7 2023.
- [One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning](https://arxiv.org/pdf/2306.07967.pdf), Jun. 13 2023.
- [Full-parameter fine-tuning for large language models with limited resources](https://arxiv.org/pdf/2306.09782.pdf), Jun. 16 2023.

#### Pre-training

- [On “Scientific Debt” in NLP: A Case for More Rigour in Language Model Pre-Training Research](https://arxiv.org/pdf/2306.02870.pdf), Jun. 5 2023.

#### LLMs for DBs

- [ChatDB: Augmenting LLMs with databases as their symbolic memory](https://arxiv.org/pdf/2306.03901.pdf), Jun. 7 2023.

#### Distillation

- [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/pdf/2306.02707.pdf), Ju. 5 2023.

#### Reasoning ability

- [Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models—and Disappeared in GPT-4]
- [Tart: A plug-and-play Transformer module for task-agnostic reasoning](https://arxiv.org/pdf/2306.07536.pdf), Jun. 13 2023.
- [Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views](https://arxiv.org/pdf/2306.09841.pdf), Jun. 16 2023.

#### Math ability

- [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/pdf/2306.01337.pdf), Jun. 2 2023.
- [Evaluating Language Models for Mathematics through Interactions](https://arxiv.org/pdf/2306.01694.pdf), Jun. 2 2023.

#### Programming ability and algorithm learning

- [Learning Transformer Programs](https://arxiv.org/pdf/2306.01128.pdf), Jun. 1 2023.
- [Beam Tree Recursive Cells](https://arxiv.org/pdf/2305.19999.pdf), Jun. 1 2023.
- [Natural Language Commanding via Program Synthesis](https://arxiv.org/pdf/2306.03460.pdf), Jun. 6 2023.

#### Data analysis via LLMs

- [Column Type Annotation using ChatGPT](https://arxiv.org/pdf/2306.00745.pdf), Jun. 1 2023.

#### Text generation via LLMs

- [An Invariant Learning Characterization of Controlled Text Generation](https://arxiv.org/pdf/2306.00198.pdf), May 31 2023.
- [Structured Voronoi Sampling](https://arxiv.org/pdf/2306.03061.pdf), Jun. 5 2023.

#### Benchmark

- [BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer](https://arxiv.org/pdf/2305.14857.pdf), May 24 2023.
- [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models](https://arxiv.org/pdf/2306.04757.pdf), Jun. 11 2023.

#### Understanding LLMs

- [Large Language Models Are Not Abstract Reasoners](https://arxiv.org/pdf/2305.19555.pdf), May 31 2023.
- [What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization](https://arxiv.org/pdf/2305.19420.pdf), May 30 2023.
- [Examining the Emergence of Deductive Reasoning in Generative Language Models](https://arxiv.org/pdf/2306.01009.pdf), May 31 2023. `reasoning`.
- [On the Role of Attention in Prompt-tuning](https://arxiv.org/pdf/2306.03435.pdf), Jun. 6 2023.
  - _"softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model"_

##### Understanding transformers

- [The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles](https://arxiv.org/pdf/2306.01705.pdf), Jun. 2 2023. `understanding transformers`.
- [White-Box Transformers via Sparse Rate Reduction](https://arxiv.org/pdf/2306.01129.pdf), Jun. 1 2023.
- [Transformers learn to implement preconditioned gradient descent for in-context learning](https://arxiv.org/pdf/2306.00297.pdf), Jun. 1 2023.
- [Representational Strengths and Limitations of Transformers](https://arxiv.org/pdf/2306.02896.pdf), Jun. 5 2023.
- [Memorization Capacity of Multi-Head Attention in Transformers](https://arxiv.org/pdf/2306.02010.pdf), Jun. 3 2023.
- [Birth of a Transformer: A Memory Viewpoint](https://arxiv.org/pdf/2306.00802.pdf), Jun. 1 2023.
- [Transformers learn through gradual rank increase](https://arxiv.org/pdf/2306.07042.pdf), Jun. 12 2023.
- [Trained Transformers Learn Linear Models In-Context](https://arxiv.org/pdf/2306.09927.pdf), Jun. 16 2023.

#### Mechanistic interpretability

- [Neuron to Graph: Interpreting Language Model Neurons at Scale](https://arxiv.org/pdf/2305.19911.pdf), May 31 2023.
- [Low-Complexity Probing via Finding Subnetworks](https://arxiv.org/pdf/2104.03514.pdf), Apr. 8 2021.

#### Applications

- [Decision-Oriented Dialogue for Human–AI Collaboration](https://arxiv.org/pdf/2305.20076.pdf), Jun. 1 2023.
- [Knowledge Base Question Answering for Space Debris Queries](https://arxiv.org/pdf/2305.19734.pdf), May 31 2023.

### Benchmark and codebase

- [Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX](https://arxiv.org/pdf/2306.09884.pdf), Jun. 16 2023. `jax` `rl` `benchmark`.
- [LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning](https://arxiv.org/pdf/2306.09910.pdf), Jun. 16 2023. `pytorch` `label-efficient learning` `benchmark`. [code](https://github.com/EfficientTraining/LabelBench).
- [OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection](https://arxiv.org/pdf/2306.09301.pdf), Jun. 15 2023. [code](https://github.com/Jingkang50/OpenOOD).
- [Small-Text: Active Learning for Text Classification in Python](https://aclanthology.org/2023.eacl-demo.11.pdf), `eacl2023`. [code](https://github.com/webis-de/small-text).

### AutoML toolkits

- All the links below are from [awesome-automl](https://github.com/windmaple/awesome-AutoML).
- [auto-sklearn](https://github.com/automl/auto-sklearn).
- [optuna](https://github.com/optuna/optuna).
- [auto-pytorch](https://github.com/automl/Auto-PyTorch).

### Must read

- [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644), Jun. 20 2023.
- [A Simple and Effective Pruning Approach for Large Language Models](https://arxiv.org/pdf/2306.11695.pdf), Jun. 20 2023.
- [Mass-Producing Failures of Multimodal Systems with Language Models](https://arxiv.org/pdf/2306.12105.pdf), Jun. 21 2023.
- [FFCV: Accelerating Training by Removing Data Bottlenecks](https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf), `cvpr2023`.
[code](https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf).
- [Conformal Language Modeling](https://arxiv.org/pdf/2306.10193.pdf), Jun. 16 2023.
- [Can Neural Network Memorization Be Localized?](https://openreview.net/pdf?id=Pbaiy3fRCt), `icml2023`.
- [Faith and Fate: Limits of Transformers on Compositionality](https://arxiv.org/pdf/2305.18654.pdf), Jun. 1 2023.

