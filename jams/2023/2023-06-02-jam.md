
- [Mixture Proportion Estimation Beyond Irreducibility](https://arxiv.org/pdf/2306.01253.pdf), Jun. 2 2023.
- [Towards Sustainable Learning: Coresets for Data-efficient Deep Learning](https://arxiv.org/pdf/2306.01244.pdf), Jun. 2 2023. `coreset`.
- [DocFormerv2: Local Features for Document Understanding](https://arxiv.org/pdf/2306.01733.pdf), Jun. 2 2023.
- [A Data-Driven Measure of Relative Uncertainty for Misclassification Detection](https://arxiv.org/pdf/2306.01710.pdf), Jun. 2 2023. `uncertainty` `trustworthy`.
- [Black-box anomaly attribution](https://arxiv.org/pdf/2305.18440.pdf), May 29 2023.
- [Doubly Robust Self-Training](https://arxiv.org/pdf/2306.00265.pdf), Jun. 1 2023.
- [When Does Optimizing a Proper Loss Yield Calibration?](https://arxiv.org/pdf/2305.18764.pdf), May 30 2023. `uncertainty`.
- [Soft Merging of Experts with Adaptive Routing](https://arxiv.org/pdf/2306.03745.pdf), Jun. 6 2023. `model merging`.
- [LibAUC: A Deep Learning Library for X-Risk Optimization](https://arxiv.org/pdf/2306.03065.pdf), Jun. 5 2023. `direct optimization`. [code](https://github.com/Optimization-AI/LibAUC).
- [Time Interpret: a Unified Model Interpretability Library for Time Series](https://arxiv.org/pdf/2306.02968.pdf), Jun. 6 2023. `interpretability`. [code](https://github.com/josephenguehard/time_interpret).

### Grow model to get specific properties

- [GrowNet](https://arxiv.org/pdf/2002.07971.pdf), 2020. `nips2020 rejected`.
- [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196), 2017. `citation: 6k+`.
- [Understanding Progressive Training Through the Framework of Randomized Coordinate Descent](https://arxiv.org/pdf/2306.03626.pdf), Jun. 6 2023.

### Generative model new frontiers

- [Conditionally Strongly Log-Concave Generative Models](https://arxiv.org/pdf/2306.00181.pdf), May 31 2023.

### RLHF theory

- [How to Query Human Feedback Efficiently in RL?](https://arxiv.org/pdf/2305.18505.pdf), May 29 2023.
~~- [Random Feedback Alignment Algorithms to train Neural Networks: Why do they Align?](https://arxiv.org/pdf/2306.02325.pdf), Jun. 4 2023.~~
  - This is not RLHF but a method to train NNs beyond backpropagation.

### Learning dynamics and generalization of neural networks

- [Benign Overfitting in Deep Neural Networks under Lazy Training](https://arxiv.org/pdf/2305.19377.pdf), May 30 2023.
- [Quantifying Overfitting: Evaluating Neural Network Performance through Analysis of Null Space](https://arxiv.org/pdf/2305.19424.pdf), May 30 2023.
- [On Emergence of Clean-Priority Learning in Early Stopped Neural Networks](https://arxiv.org/pdf/2306.02533.pdf), Jun. 5 2023.

### Data augmentation

- [Scaling up semi-supervised learning with unconstrained unlabeled data](https://arxiv.org/pdf/2306.01222.pdf), Jun. 2 2023. `UnMixMatch`.

### Data structure

- [EEL: Efficiently Encoding Lattices for Reranking](https://arxiv.org/pdf/2306.00947.pdf), Jun. 1 2023.

### Building blocks of neural architecture

- [Centered Self-Attention Layers](https://arxiv.org/pdf/2306.01610.pdf), Jun. 2 2023.

### Deep survival analysis

- [An Effective Meaningful Way to Evaluate Survival Models](https://arxiv.org/pdf/2306.01196.pdf), Jun. 1 2023.
- [Deep Learning for Survival Analysis](https://humboldt-wi.github.io/blog/research/information_systems_1920/group2_survivalanalysis/#deeplearning_sa), Feb. 6 2020.

### Continual/multitask learning

- [OMNI: Open-endedness via Models of human Notions of Interestingness](https://arxiv.org/pdf/2306.01711.pdf), Jun. 2 2023.
- [Resolving Interference When Merging Models](https://arxiv.org/pdf/2306.01708.pdf), Jun. 2 2023.
- [GateON: an unsupervised method for large scale continual learning](https://arxiv.org/pdf/2306.01690.pdf), Jun. 2 2023.
- [Improved Active Multi-Task Representation Learning via Lasso](https://arxiv.org/pdf/2306.02556.pdf), Jun. 5 2023.
- [Continual Learning in Linear Classification on Separable Data](https://arxiv.org/pdf/2306.03534.pdf), Jun. 6 2023. `continual learning theory`.

### Adversarial training

- [Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training](https://arxiv.org/pdf/2306.01271.pdf), Jun. 2 2023.

### Community detection

- [Semi-supervised community detection via structural similarity metrics](https://arxiv.org/pdf/2306.01089.pdf), Jun. 1 2023.

### Distribution shift

- [Measuring the Robustness of Natural Language Processing Models to Domain Shifts](https://arxiv.org/pdf/2306.00168.pdf), May 31 2023.
- [ELSA: Efficient Label Shift Adaptation through the Lens of Semiparametric Models](https://arxiv.org/pdf/2305.19123.pdf), May 30 2023.
- [Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms](https://arxiv.org/pdf/2305.19570.pdf), May 31 2023.
- [Deep into The Domain Shift: Transfer Learning through Dependence Regularization](https://arxiv.org/pdf/2305.19499.pdf), May 31 2023.
- [An Adaptive Method for Weak Supervision with Drifting Data](https://arxiv.org/pdf/2306.01658.pdf), Jun. 2 2023.
- [On Pitfalls of Test-Time Adaptation](https://arxiv.org/pdf/2306.03536.pdf), Jun. 6 2023.

### Old-school nlp

- [Back to patterns: efficient Japanese morphological analysis with feature-sequence trie](https://arxiv.org/pdf/2305.19045.pdf), May 30 2023.
- [Deriving Language Models from Masked Language Models](https://arxiv.org/pdf/2305.15501.pdf), May 24 2023.
- [PromptNER: Prompting For Named Entity Recognition](https://arxiv.org/pdf/2305.15444.pdf), May 24 2023.

### Representation learning

- [On the impact of activation and normalization in obtaining isometric embeddings at initialization](https://arxiv.org/pdf/2305.18399.pdf), May 28 2023.

### Transfer learning

- [Resolving Interference When Merging Models](https://arxiv.org/pdf/2306.01708.pdf), Jun. 2 2023.

### LLMs

- [Let’s Verify Step by Step](https://arxiv.org/pdf/2305.20050.pdf), May 31 2023.
- [Test-Time Training on Nearest Neighbors for Large Language Models](https://arxiv.org/pdf/2305.18466.pdf), May 29 2023.
  - [AdANNS: A Framework for Adaptive Semantic Search](https://arxiv.org/pdf/2305.19435.pdf), May 30 2023. `fast ann search`.
- [Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://arxiv.org/pdf/2305.04388.pdf), May 7 2023.
- [Did ChatGPT cheat on your test?](https://hitz-zentroa.github.io/lm-contamination/blog/).
- [Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions](https://arxiv.org/pdf/2306.02224.pdf), Jun. 4 2023. [code](https://github.com/Significant-Gravitas/Auto-GPT). 

#### LLMs for DBs

- [ChatDB: Augmenting LLMs with databases as their symbolic memory](https://arxiv.org/pdf/2306.03901.pdf), Jun. 7 2023.

#### Distillation

- [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/pdf/2306.02707.pdf), Ju. 5 2023.

#### Math ability

- [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/pdf/2306.01337.pdf), Jun. 2 2023.
- [Evaluating Language Models for Mathematics through Interactions](https://arxiv.org/pdf/2306.01694.pdf), Jun. 2 2023.

#### Programming ability and algorithm learning

- [Learning Transformer Programs](https://arxiv.org/pdf/2306.01128.pdf), Jun. 1 2023.
- [Beam Tree Recursive Cells](https://arxiv.org/pdf/2305.19999.pdf), Jun. 1 2023.
- [Natural Language Commanding via Program Synthesis](https://arxiv.org/pdf/2306.03460.pdf), Jun. 6 2023.

#### Data analysis via LLMs

- [Column Type Annotation using ChatGPT](https://arxiv.org/pdf/2306.00745.pdf), Jun. 1 2023.

#### Text generation via LLMs

- [An Invariant Learning Characterization of Controlled Text Generation](https://arxiv.org/pdf/2306.00198.pdf), May 31 2023.
- [Structured Voronoi Sampling](https://arxiv.org/pdf/2306.03061.pdf), Jun. 5 2023.

#### Benchmark

- [BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer](https://arxiv.org/pdf/2305.14857.pdf), May 24 2023.

#### Understanding LLMs

- [Large Language Models Are Not Abstract Reasoners](https://arxiv.org/pdf/2305.19555.pdf), May 31 2023.
- [What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization](https://arxiv.org/pdf/2305.19420.pdf), May 30 2023.
- [Examining the Emergence of Deductive Reasoning in Generative Language Models](https://arxiv.org/pdf/2306.01009.pdf), May 31 2023. `reasoning`.
- [On the Role of Attention in Prompt-tuning](https://arxiv.org/pdf/2306.03435.pdf), Jun. 6 2023.
  - _"softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model"_

##### Understanding transformers

- [The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles](https://arxiv.org/pdf/2306.01705.pdf), Jun. 2 2023. `understanding transformers`.
- [White-Box Transformers via Sparse Rate Reduction](https://arxiv.org/pdf/2306.01129.pdf), Jun. 1 2023.
- [Transformers learn to implement preconditioned gradient descent for in-context learning](https://arxiv.org/pdf/2306.00297.pdf), Jun. 1 2023.
- [Representational Strengths and Limitations of Transformers](https://arxiv.org/pdf/2306.02896.pdf), Jun. 5 2023.
- [Memorization Capacity of Multi-Head Attention in Transformers](https://arxiv.org/pdf/2306.02010.pdf), Jun. 3 2023.
- [Birth of a Transformer: A Memory Viewpoint](https://arxiv.org/pdf/2306.00802.pdf), Jun. 1 2023.

#### Mechanistic interpretability

- [Neuron to Graph: Interpreting Language Model Neurons at Scale](https://arxiv.org/pdf/2305.19911.pdf), May 31 2023.

#### Applications

- [Decision-Oriented Dialogue for Human–AI Collaboration](https://arxiv.org/pdf/2305.20076.pdf), Jun. 1 2023.
- [Knowledge Base Question Answering for Space Debris Queries](https://arxiv.org/pdf/2305.19734.pdf), May 31 2023.
