
- [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://openreview.net/attachment?id=bF1LVbP493&name=pdf), `icml2023`.
- [Effective resistance in metric spaces](https://arxiv.org/pdf/2306.15649.pdf), Jun. 27 2023.
- [ZipIt! Merging Models from Different Tasks without Training](https://arxiv.org/pdf/2305.03053.pdf), May 4 2023. `cvpr2023`.
- [Should you marginalize over possible tokenizations?](https://arxiv.org/pdf/2306.17757.pdf), Jun. 30 2023.
  - _"we analyze whether the practice of ignoring the marginalization is justified"_
  - _"results show that the gap in log-likelihood is no larger than 0.5% in most cases, but it becomes more pronounced for data with long complex words"_
- [Log-linear Guardedness and its Implications](https://arxiv.org/abs/2210.10012), Jun. 29 2023. `acl2023`.
- [SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees](https://arxiv.org/pdf/2307.04849.pdf), Jul. 10 2023. `automl`.
- [The Integer Linear Programming Inference Cookbook](https://arxiv.org/pdf/2307.00171.pdf), Jun. 30 2023. `tutorial`.
- [Bidirectional looking with a novel double exponential moving average to adaptive and non-adaptive monmentum optimizers](https://arxiv.org/pdf/2307.00631.pdf), Jul. 2023. `optimizer`.
- [Online nearest neighbor classification](https://arxiv.org/pdf/2307.01170.pdf), Jul. 3 2023.

### Time-series

- [EasyTPP: Towards Open Benchmarking the Temporal Point Processes](https://arxiv.org/pdf/2307.08097.pdf), Jul. 16 2023. [code](https://github.com/ant-research/EasyTemporalPointProcess).
- [BuildingsBench: a large-scale dataset of 900K buildings and benchmark for short-term load forecasting](https://arxiv.org/pdf/2307.00142.pdf), Jun. 30 2023.
- [Generalized time warping invariant dictionary learning for time series classification and clustering](https://arxiv.org/pdf/2306.17690.pdf), Jun. 30 2023.

### XXXformer

- [Memformer: A Memory-Augmented Transformer for Sequence Modeling](https://arxiv.org/pdf/2010.06891.pdf), Apr. 12 2022.
- [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/pdf/2307.08621.pdf), Jul. 17 2023.
- [FlashAttention-2: faster attention with better parallelism and work partitioning](https://arxiv.org/pdf/2307.08691.pdf), Jul. 17 2023.

### Active learning

- [Understanding Uncertainty Sampling](https://arxiv.org/pdf/2307.02719.pdf), Jul. 6 2023.

### Uncertainty estimation

- [Quantification of Uncertainty with Adversarial Models](https://arxiv.org/pdf/2307.03217.pdf), Jul. 6 2023.
- [A Novel Bayes’ Theorem for Upper Probabilities](https://arxiv.org/pdf/2307.06831.pdf), Jul. 13 2023.
- [Beyond Intuition, a Framework for Applying GPs to Real-World Data](https://arxiv.org/pdf/2307.03093.pdf), Jul. 6 2023.
- [Evaluating AI systems under uncertain ground truth: a case study in dermatology](https://arxiv.org/pdf/2307.02191.pdf), Jul. 5 2023.

### Distribution shift

- [Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective](https://arxiv.org/pdf/2307.06457.pdf), Jul. 12 2023.
- [On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets](https://arxiv.org/pdf/2307.05284.pdf), Jul. 11 2023.
  - code is already available.

### Learning dynamics

- [Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory](https://arxiv.org/pdf/2307.04204.pdf), Jul. 9 2023.
- [Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses](https://arxiv.org/pdf/2307.01827.pdf), Jul. 4 2023.

### Time series

- [Generalized Time Warping Invariant Dictionary Learning for Time Series Classification and Clustering](https://arxiv.org/pdf/2306.17690.pdf), Jun. 30 2023.
- [BuildingBench: a large-scale dataset of 900K buildings and benchmark for short-term load forecasting](https://arxiv.org/pdf/2307.00142.pdf), Jun. 30 2023.

### Inductive bias

- [The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks.](https://arxiv.org/pdf/2306.16922.pdf), Jun. 14 2023.
- [The Architecture of a Biologically Plausible Language Organ](https://arxiv.org/pdf/2306.15364.pdf), Jun. 27 2023.
- [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit](https://arxiv.org/pdf/2306.17759.pdf), Jun. 30 2023.
  - _"we believe that our theory sets the stage for future work on training and generalization in deep learning"_
- [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model](https://arxiv.org/pdf/2307.02129.pdf), Jul. 5 2023.
- [Transformers are Universal Predictors](https://arxiv.org/pdf/2307.07843.pdf), Jul. 15 2023.
- [Tangent Transformers for Composition, Privacy and Removal](https://arxiv.org/pdf/2307.08122.pdf), Jul. 16 2023.
- [Recursive Algorithmic Reasoning](https://arxiv.org/pdf/2307.00337.pdf), Jul. 1 2023. `algorithm learning`.

### Fintech

- [Realistic Synthetic Financial Transactions for Anti-Money Laundering Models](https://arxiv.org/pdf/2306.16424.pdf), Jun. 22 2023.

### Data imputation

- [Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach](https://arxiv.org/pdf/2306.16906.pdf), Jun. 29 2023.

### Evaluation

- [What Makes ImageNet Look Unlike LAION](https://arxiv.org/pdf/2306.15769.pdf), Jun. 27 2023.
  - _"Our explanation formalizes a long-held intuition in the community that ImageNet images are stereotypical, unnatural, and overly simple representations of the class category"_
 
### Hyperparameter optimization

- [PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning](https://arxiv.org/pdf/2306.12370.pdf), Jun. 21 2023.

### Machine unlearning

- [Ticketed Learning–Unlearning Schemes](https://arxiv.org/pdf/2306.15744.pdf), Jun. 27 2023.

### Interpretability

- [Delivering Inflated Explanations](https://arxiv.org/pdf/2306.15272.pdf), Jun. 27 2023. `feature attribution`.
- [Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms](https://arxiv.org/pdf/2306.08167.pdf), Jun. 13 2023. `systematic error analysis`.
- [Adversarial Attacks on the Interpretation of Neuron Activation Maximization](https://arxiv.org/pdf/2306.07397.pdf), Jun. 12 2023.
- [On Minimizing the Impact of Dataset Shifts on Actionable Explanations](https://arxiv.org/pdf/2306.06716.pdf), Jun. 11 2023. `distribution shift`.
- [GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps](https://arxiv.org/pdf/2306.16612.pdf), Jun. 29 2023. `explanation-guided learning`.
- [High Fidelity Image Counterfactuals with Probabilistic Causal Models](https://arxiv.org/pdf/2306.15764.pdf), Jun. 27 2023. `counterfactual`.
- [Geometric Autoencoders – What You See is What You Decode](https://arxiv.org/pdf/2306.17638.pdf), Jun. 30 2023. `dimension reduction`.
- [QI2 - an Interactive Tool for Data Quality Assurance](https://arxiv.org/pdf/2307.03419.pdf), Jul. 10 2023. `data-centric` `toolkit`.
  - _"It quantifies neighborhood input-output relationship behaviors over a set of data points. High dimensional anomalous structure"_
- [Visual analytics for machine learning](https://arxiv.org/pdf/2307.07712.pdf), Jul. 15 2023. `visualization` `data-centric`.

### Good old nlp

- [Linear Classifier: An Often-Forgotten Baseline for Text Classification](https://aclanthology.org/2023.acl-short.160.pdf), `acl2023`.

### Compositionality

- [Compositional Generalization from First Principles](https://arxiv.org/pdf/2307.05596.pdf), Jul. 10 2023.
- [Composition-contrastive Learning for Sentence Embeddings](https://arxiv.org/pdf/2307.07380.pdf), Jul. 14 2023.
- [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model](https://arxiv.org/pdf/2307.02129.pdf), Jul. 5 2023.

### Representation learning

- [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks](https://arxiv.org/pdf/2307.06887.pdf), Jul. 13 2023.
- [Mini-Batch Optimization of Contrastive Loss](https://arxiv.org/pdf/2307.05906.pdf), Jul. 12 2023.
  - _"we show that mini-batch optimization is equivalent to full-batch optimization if and only if all C(N, B) mini-batches are selected, while sub-optimality may arise when examining only a subset"_
  - _"utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches"_
- [Multi-Similarity Contrastive Learning](https://arxiv.org/pdf/2307.02712.pdf), Jul. 6 2023.

### `repos` to learn from

- [KeyBERT](https://github.com/MaartenGr/KeyBERT), 6mon ago.
- [OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models](https://github.com/thunlp/OpenDelta), 2mon ago.
- [OpenPrompt](https://github.com/thunlp/OpenPrompt), 3mon ago.
- [deeponto](https://krr-oxford.github.io/DeepOnto/), 

---

### LLMs

- [Next Steps for Human-Centered Generative AI: A Technical Perspective](https://arxiv.org/ftp/arxiv/papers/2306/2306.15774.pdf), Jun. 28 2023.
- [Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data](https://arxiv.org/pdf/2306.13840.pdf), Jun. 24 2023.
- [Provable Robust Watermarking for AI-Generated Text](https://arxiv.org/pdf/2306.17439.pdf), Jun. 30 2023.
- [Large Language Models](https://arxiv.org/pdf/2307.05782.pdf), Jul. 11 2023.
- [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](https://arxiv.org/pdf/2307.05532.pdf), Jul. 8 2023.
- [Certified Robustness for Large Language Models with Self-Denoising](https://arxiv.org/pdf/2307.07171.pdf), Jul. 14 2023.

#### Benchmark

- [CMMLU: Measuring massive multitask language understanding in Chinese](https://arxiv.org/pdf/2306.09212.pdf), Jun. 15 2023.
- [Disco-Bench: a discourse-aware evaluation benchmark for languag modelling](https://arxiv.org/pdf/2307.08074.pdf), Jul. 16 2023.

#### Data-centric

- [Improving Retrieval-Augmented Large Language Models via Data Importance Learning](https://arxiv.org/pdf/2307.03027.pdf), Jul. 6 2023.

#### Pre-training

- [Text Alignment Is An Efficient Unified Model for Massive NLP Tasks](https://arxiv.org/pdf/2307.02729.pdf), Jul. 6 2023. [code](https://github.com/yuh-zha/Align).
- [CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks](https://proceedings.mlr.press/v202/wang23t/wang23t.pdf), `icml2023`.
- [Copy is all you need](https://arxiv.org/pdf/2307.06962.pdf), Jul. 13 2023. [code](https://github.com/gmftbyGMFTBY/Copyisallyouneed).

#### Fine-tuning

- [Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain](https://arxiv.org/pdf/2307.03042.pdf), Jul. 6 2023.
- [Contrastive Error Attribution for Finetuned Language Models](https://arxiv.org/pdf/2212.10722.pdf), Jul. 11 2023.

#### Formal language

- [Why can neural language models solve next-word prediction? A mathematical perspective](https://arxiv.org/pdf/2306.17184.pdf), Jun. 20 2023.

#### Alignment

- [Is RLHF More Difficult than Standard RL?](https://arxiv.org/pdf/2306.14111.pdf), Jun. 25 2023.
- [Are aligned neural networks adversarially aligned?](https://arxiv.org/pdf/2306.15447.pdf), Jun. 26 2023.
- [Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision](https://arxiv.org/pdf/2306.16564.pdf), Jun. 28 2023.
- [On the Exploitability of Instruction Tuning](https://arxiv.org/pdf/2306.17194.pdf), Jun. 28 2023.

#### Longer context

- [Extending context window of large language models via position interpolation](https://arxiv.org/pdf/2306.15595.pdf), Jun. 28 2023.
- [Length Generalization in Arithmetic Transformers](https://arxiv.org/pdf/2306.15400.pdf), Jun. 27 2023.
- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172.pdf), Jul. 6 2023.

#### Scaling law

- [The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets](https://arxiv.org/pdf/2306.14975.pdf), Jun. 26 2023.
- [Scaling Laws Do Not Scale](https://arxiv.org/pdf/2307.03201.pdf), Jul. 5 2023.

#### In-context learning

- [Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression](https://arxiv.org/pdf/2306.15063.pdf), Jun.26 2023.
- [One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention](https://arxiv.org/pdf/2307.03576.pdf), Jul. 7 2023.
- [Scaling in-context demonstration with structured attention](https://arxiv.org/pdf/2307.02690.pdf), Jul. 5 2023.
  - _"to overcome length limit, order sensitivity of in-context demonstration"_ 
- [InstructEval: Systematic Evaluation of Instruction Selection Methods](https://arxiv.org/pdf/2307.00259.pdf), Jul. 1 2023.
- [Trainable transformer in transformer](https://arxiv.org/pdf/2307.01189.pdf), Jul. 3 2023.

#### Mechanistic interpretability

- [The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks](https://arxiv.org/pdf/2306.17844.pdf), Jun. 30 2023.

#### Algorthm learning

- [Teaching Arithmetic to Small Transformers](https://arxiv.org/pdf/2307.03381.pdf), Jul. 7 2023. [code](https://github.com/lee-ny/teaching_arithmetic).

#### Prompting

- [SCOTT: Self-Consistent Chain-of-Thought Distillation](https://arxiv.org/pdf/2305.01879.pdf), May 21 2023.
- [Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models](https://arxiv.org/pdf/2306.17820.pdf), Jun. 30 2023.

#### Application

- [What Should Data Science Education Do with Large Language Models?](https://arxiv.org/pdf/2307.02792.pdf), Jul. 7 2023. `education`.
- [Building Community Driven Libraries of Natural Programs](https://openreview.net/forum?id=iRea6QCxi1), `iclr2023`.
- [TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT](https://arxiv.org/pdf/2307.08674.pdf), Jul. 17 2023.

#### Safety

- [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/pdf/2307.02483.pdf), Jul. 5 2023.

#### Continual learning

- [Towards Robust and Efficient Continual Language Learning](https://arxiv.org/pdf/2307.05741.pdf), Jul. 11 2023.
- [Improving Language Plasticity via Pretraining with Active Forgetting](https://arxiv.org/pdf/2307.01163.pdf), Jul. 3 2023.




