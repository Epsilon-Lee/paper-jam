
- [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://openreview.net/attachment?id=bF1LVbP493&name=pdf), `icml2023`.
- [Effective resistance in metric spaces](https://arxiv.org/pdf/2306.15649.pdf), Jun. 27 2023.
- [ZipIt! Merging Models from Different Tasks without Training](https://arxiv.org/pdf/2305.03053.pdf), May 4 2023. `cvpr2023`.
- [Should you marginalize over possible tokenizations?](https://arxiv.org/pdf/2306.17757.pdf), Jun. 30 2023.
  - _"we analyze whether the practice of ignoring the marginalization is justified"_
  - _"results show that the gap in log-likelihood is no larger than 0.5% in most cases, but it becomes more pronounced for data with long complex words"_
- [Log-linear Guardedness and its Implications](https://arxiv.org/abs/2210.10012), Jun. 29 2023. `acl2023`.
- [SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees](https://arxiv.org/pdf/2307.04849.pdf), Jul. 10 2023. `automl`.
- [The Integer Linear Programming Inference Cookbook](https://arxiv.org/pdf/2307.00171.pdf), Jun. 30 2023. `tutorial`.
- [Bidirectional looking with a novel double exponential moving average to adaptive and non-adaptive monmentum optimizers](https://arxiv.org/pdf/2307.00631.pdf), Jul. 2023. `optimizer`.
- [Online nearest neighbor classification](https://arxiv.org/pdf/2307.01170.pdf), Jul. 3 2023.
- [SPRINT: A unified toolkit for evaluating and demystifying zero-shot neural sparse retrieval](https://arxiv.org/pdf/2307.10488.pdf), Jul. 19 2023. [code](https://github.com/thakur-nandan/sprint).
- [Efficient beam tree recursion](https://arxiv.org/pdf/2307.10779.pdf), Jul. 20 2023.
- [Efficient Estimation of Local Robustness of Machine Learning Models](https://arxiv.org/pdf/2307.13885.pdf), Jul. 26 2023.
  - _"analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and multivariate Normal CDF"_

### Spurious correlation

- [Mitigating spurious correlations in multi-modal models during fine-tuning](https://arxiv.org/abs/2304.03916), Apr. 8 2023. `spurious correlation`.
- [What do neural networks learn in image classification? A frequency shortcut perspective](https://arxiv.org/pdf/2307.09829.pdf), Jul. 19 2023. `spurious correlation`.
- [Contextual Reliability: When Different Features Matter in Different Contexts](https://arxiv.org/pdf/2307.10026.pdf), Jul. 19 2023. `spurious correlation`.
- [Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features](https://arxiv.org/pdf/2307.09933.pdf), Jul. 19 2023. [code](https://github.com/cianeastwood/sfb).
- [Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?](https://arxiv.org/pdf/2307.12344.pdf), Jul. 23 2023.

### Robustness

- [Do Perceptually Aligned Gradients Imply Robustness?](https://arxiv.org/pdf/2207.11378.pdf), Feb. 1 2023.

### Systems

- [UPREVE: An end-to-end causal discovery system](https://github.com/surajjunni/UPREVE).
- [Solving data quality problems with desbordante: a demo](https://arxiv.org/pdf/2307.14935.pdf), Jul. 27 2023.

### Feature selection

- [Finding optimal diverse feature sets with alternative feature selection](https://arxiv.org/pdf/2307.11607.pdf), Jul. 21 2023.

### Learning under noise

- [Learning from noisy labels with deep neural networks: a survey](https://arxiv.org/pdf/2007.08199.pdf), Mar. 10 2022.
- [A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond](https://arxiv.org/pdf/2307.08643.pdf), Jul. 17 2023.
- [Label Noise: Correcting a Correction](https://arxiv.org/pdf/2307.13100.pdf), Jul. 24 2023.
  - _"We observe that the presence of label noise implies a lower bound on the noisy generalised risk [...] we propose imposing a lower bound on the empirical risk during training to mitigate overfitting"_
  - _"We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually mo additional computational cost"_
- [Robust training under label noise by over-parameterization](https://arxiv.org/pdf/2202.14026.pdf), Aug. 2 2022. [code](https://github.com/shengliu66/SOP).
  - introduction of $u_i, v_i \in [-1, 1]^K$, to construct loss function: $l(f(x_i; \theta) + u_i \odot u_i - v_i \odot v_i, y_i)$, where $f(x_i; \theta), y_i \in \mathbb{R}^K$

### Auto data analysis

- [Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing](https://arxiv.org/pdf/2012.12627.pdf), Dec. 31 2020.
- [A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions](https://arxiv.org/pdf/2208.13629.pdf), Aug. 29 2022.

### Time-series

- [EasyTPP: Towards Open Benchmarking the Temporal Point Processes](https://arxiv.org/pdf/2307.08097.pdf), Jul. 16 2023. [code](https://github.com/ant-research/EasyTemporalPointProcess).
- [BuildingsBench: a large-scale dataset of 900K buildings and benchmark for short-term load forecasting](https://arxiv.org/pdf/2307.00142.pdf), Jun. 30 2023.
- [Generalized time warping invariant dictionary learning for time series classification and clustering](https://arxiv.org/pdf/2306.17690.pdf), Jun. 30 2023.

### XXXformer

- [Memformer: A Memory-Augmented Transformer for Sequence Modeling](https://arxiv.org/pdf/2010.06891.pdf), Apr. 12 2022.
- [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/pdf/2307.08621.pdf), Jul. 17 2023.
- [FlashAttention-2: faster attention with better parallelism and work partitioning](https://arxiv.org/pdf/2307.08691.pdf), Jul. 17 2023.

### Active learning

- [Understanding Uncertainty Sampling](https://arxiv.org/pdf/2307.02719.pdf), Jul. 6 2023.

### Uncertainty estimation

- [Quantification of Uncertainty with Adversarial Models](https://arxiv.org/pdf/2307.03217.pdf), Jul. 6 2023.
- [A Novel Bayes’ Theorem for Upper Probabilities](https://arxiv.org/pdf/2307.06831.pdf), Jul. 13 2023.
- [Beyond Intuition, a Framework for Applying GPs to Real-World Data](https://arxiv.org/pdf/2307.03093.pdf), Jul. 6 2023.
- [Evaluating AI systems under uncertain ground truth: a case study in dermatology](https://arxiv.org/pdf/2307.02191.pdf), Jul. 5 2023.
- [Confidence estimation using unlabeled data](https://arxiv.org/pdf/2307.10440.pdf), Jul. 19 2023.
  - _"we use training consistency as a surrogate function and propose a consistency ranking loss for uncertainty estimation"_
  - application areas: image classification and segmentation.
- [PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models](https://arxiv.org/pdf/2307.09254.pdf), Jul. 18 2023.

### Distribution shift

- [Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective](https://arxiv.org/pdf/2307.06457.pdf), Jul. 12 2023.
- [On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets](https://arxiv.org/pdf/2307.05284.pdf), Jul. 11 2023.
  - code is already available.

### Learning dynamics and generalization

- [Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory](https://arxiv.org/pdf/2307.04204.pdf), Jul. 9 2023.
- [Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses](https://arxiv.org/pdf/2307.01827.pdf), Jul. 4 2023.
- [Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization](https://arxiv.org/pdf/2307.12851.pdf), Jul. 24 2023.
- [The instabilities of large learning rate training: a loss landscape view](https://arxiv.org/pdf/2307.11948.pdf), Jul. 22 2023.
- [How to scale you EMA](https://arxiv.org/pdf/2307.13813.pdf), Jul. 27 2023.
- [Generalization on the unseen, logic reasoning and degree curriculum](https://arxiv.org/pdf/2301.13105.pdf), Jun. 28 2023.
- [Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum](https://arxiv.org/pdf/2307.14531.pdf), Jul. 26 2023.
- [The marginal value of momentum for small learning rate SGD](https://arxiv.org/pdf/2307.15196.pdf), Jul. 27 2023.
  - _"momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and finetuning language modelson downstream tasks"_
- [Noisy Interpolation Learning with Shallow Univariate ReLU Networks](https://arxiv.org/pdf/2307.15396.pdf), Jul. 28 2023. `overfitting dynamics`.

### Time series

- [Generalized Time Warping Invariant Dictionary Learning for Time Series Classification and Clustering](https://arxiv.org/pdf/2306.17690.pdf), Jun. 30 2023.
- [BuildingBench: a large-scale dataset of 900K buildings and benchmark for short-term load forecasting](https://arxiv.org/pdf/2307.00142.pdf), Jun. 30 2023.
- [Sequential Monte Carlo Learning for Time Series Structure Discovery](https://arxiv.org/pdf/2307.09607.pdf), Jul. 13 2023.

### Inductive bias

- [The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks.](https://arxiv.org/pdf/2306.16922.pdf), Jun. 14 2023.
- [The Architecture of a Biologically Plausible Language Organ](https://arxiv.org/pdf/2306.15364.pdf), Jun. 27 2023.
- [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit](https://arxiv.org/pdf/2306.17759.pdf), Jun. 30 2023.
  - _"we believe that our theory sets the stage for future work on training and generalization in deep learning"_
- [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model](https://arxiv.org/pdf/2307.02129.pdf), Jul. 5 2023.
- [Transformers are Universal Predictors](https://arxiv.org/pdf/2307.07843.pdf), Jul. 15 2023.
- [Tangent Transformers for Composition, Privacy and Removal](https://arxiv.org/pdf/2307.08122.pdf), Jul. 16 2023.
- [Recursive Algorithmic Reasoning](https://arxiv.org/pdf/2307.00337.pdf), Jul. 1 2023. `algorithm learning`.
- [Unsupervised Conditional Slot Attention for Object Centric Learning](https://arxiv.org/pdf/2307.09437.pdf), Jul. 18 2023.
- [Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation](https://arxiv.org/pdf/2307.02598.pdf), Jul. 5 2023.
- [What Can a Single Attention Layer Learn? A Study Through the Random Features Lens](https://arxiv.org/pdf/2307.11353.pdf), Jul. 21 2023. `theory of Transformer`.
  - _"permutation invariant to the key vectors"_
- [On the Interpretability of Attention Networks](https://arxiv.org/pdf/2212.14776.pdf), May 14 2023.
- [Fading memory as inductive bias in residual recurrent networks](https://arxiv.org/pdf/2307.14823.pdf), Jul. 27 2023.
- [Are Transformers with one layer self-attention using low-rank weight matrices universal approximators?](https://arxiv.org/pdf/2307.14023.pdf), Jul. 26 2023.

### Fintech

- [Realistic Synthetic Financial Transactions for Anti-Money Laundering Models](https://arxiv.org/pdf/2306.16424.pdf), Jun. 22 2023.

### Data imputation

- [Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach](https://arxiv.org/pdf/2306.16906.pdf), Jun. 29 2023.

### Evaluation

- [What Makes ImageNet Look Unlike LAION](https://arxiv.org/pdf/2306.15769.pdf), Jun. 27 2023.
  - _"Our explanation formalizes a long-held intuition in the community that ImageNet images are stereotypical, unnatural, and overly simple representations of the class category"_
- [Efficiency pentathlon: a standarized arena for efficiency evaluation](https://arxiv.org/pdf/2307.09701.pdf), Jul. 19 2023.

### Hyperparameter optimization

- [PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning](https://arxiv.org/pdf/2306.12370.pdf), Jun. 21 2023.

### Machine unlearning

- [Ticketed Learning–Unlearning Schemes](https://arxiv.org/pdf/2306.15744.pdf), Jun. 27 2023.

### Interpretability

- [Delivering Inflated Explanations](https://arxiv.org/pdf/2306.15272.pdf), Jun. 27 2023. `feature attribution`.
- [Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms](https://arxiv.org/pdf/2306.08167.pdf), Jun. 13 2023. `systematic error analysis`.
- [Adversarial Attacks on the Interpretation of Neuron Activation Maximization](https://arxiv.org/pdf/2306.07397.pdf), Jun. 12 2023.
- [On Minimizing the Impact of Dataset Shifts on Actionable Explanations](https://arxiv.org/pdf/2306.06716.pdf), Jun. 11 2023. `distribution shift`.
- [GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps](https://arxiv.org/pdf/2306.16612.pdf), Jun. 29 2023. `explanation-guided learning`.
- [High Fidelity Image Counterfactuals with Probabilistic Causal Models](https://arxiv.org/pdf/2306.15764.pdf), Jun. 27 2023. `counterfactual`.
- [Geometric Autoencoders – What You See is What You Decode](https://arxiv.org/pdf/2306.17638.pdf), Jun. 30 2023. `dimension reduction`.
- [QI2 - an Interactive Tool for Data Quality Assurance](https://arxiv.org/pdf/2307.03419.pdf), Jul. 10 2023. `data-centric` `toolkit`.
  - _"It quantifies neighborhood input-output relationship behaviors over a set of data points. High dimensional anomalous structure"_
- [Visual analytics for machine learning](https://arxiv.org/pdf/2307.07712.pdf), Jul. 15 2023. `visualization` `data-centric`.
- [Identifying Interpretable Subspaces in Image Representations](https://arxiv.org/pdf/2307.10504.pdf), Jul. 20 2023. `representation analysis` `dimension reduction` `probing`.
  - _"we show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts..."_
- [Feature Importance Measurement based on Decision Tree Sampling](https://arxiv.org/pdf/2307.13333.pdf), Jul. 25 2023.

### Good old nlp

- [Linear Classifier: An Often-Forgotten Baseline for Text Classification](https://aclanthology.org/2023.acl-short.160.pdf), `acl2023`.
- [Gzip versus bag-of-words for text classification with KNN](https://arxiv.org/pdf/2307.15002.pdf), Jul. 27 2023.

### Compositionality

- [Compositional Generalization from First Principles](https://arxiv.org/pdf/2307.05596.pdf), Jul. 10 2023.
- [Composition-contrastive Learning for Sentence Embeddings](https://arxiv.org/pdf/2307.07380.pdf), Jul. 14 2023.
- [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model](https://arxiv.org/pdf/2307.02129.pdf), Jul. 5 2023.

### Representation learning

- [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks](https://arxiv.org/pdf/2307.06887.pdf), Jul. 13 2023.
- [Mini-Batch Optimization of Contrastive Loss](https://arxiv.org/pdf/2307.05906.pdf), Jul. 12 2023.
  - _"we show that mini-batch optimization is equivalent to full-batch optimization if and only if all C(N, B) mini-batches are selected, while sub-optimality may arise when examining only a subset"_
  - _"utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches"_
- [Multi-Similarity Contrastive Learning](https://arxiv.org/pdf/2307.02712.pdf), Jul. 6 2023.

### `repos` to learn from

- [KeyBERT](https://github.com/MaartenGr/KeyBERT), 6mon ago.
- [OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models](https://github.com/thunlp/OpenDelta), 2mon ago.
- [OpenPrompt](https://github.com/thunlp/OpenPrompt), 3mon ago.
- [deeponto](https://krr-oxford.github.io/DeepOnto/), 

---

### LLMs

- [Next Steps for Human-Centered Generative AI: A Technical Perspective](https://arxiv.org/ftp/arxiv/papers/2306/2306.15774.pdf), Jun. 28 2023.
- [Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data](https://arxiv.org/pdf/2306.13840.pdf), Jun. 24 2023.
- [Large Language Models](https://arxiv.org/pdf/2307.05782.pdf), Jul. 11 2023.
- [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](https://arxiv.org/pdf/2307.05532.pdf), Jul. 8 2023.
- [Certified Robustness for Large Language Models with Self-Denoising](https://arxiv.org/pdf/2307.07171.pdf), Jul. 14 2023.
- [LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs](https://arxiv.org/pdf/2307.10168.pdf), Jul. 20 2023.

#### Watermarking

- [Provable Robust Watermarking for AI-Generated Text](https://arxiv.org/pdf/2306.17439.pdf), Jun. 30 2023.
- [Robust distortion-free watermarks for language models](https://arxiv.org/pdf/2307.15593.pdf), Jul. 28 2023.

#### Architecture

- [Scaling TransNormer to 175 Billion parameters](https://arxiv.org/pdf/2307.14995.pdf), Jul. 27 2023.

#### Data-centric

- [Prevalence of Neural Collapse during the terminal phase of deep learning training](https://arxiv.org/pdf/2307.14430.pdf), Jul. 26 2023.

#### Compression and efficiency

- [TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition](https://arxiv.org/pdf/2307.00526.pdf), Jul. 2 2023.
- [Incrementally-computable neural networks: efficient inference for dynamic graphs](https://arxiv.org/pdf/2307.14988.pdf), Jul. 27 2023.

#### Benchmark

- [CMMLU: Measuring massive multitask language understanding in Chinese](https://arxiv.org/pdf/2306.09212.pdf), Jun. 15 2023.
- [Disco-Bench: a discourse-aware evaluation benchmark for languag modelling](https://arxiv.org/pdf/2307.08074.pdf), Jul. 16 2023.
- [How Is ChatGPT’s Behavior Changing over Time?](https://arxiv.org/pdf/2307.09009.pdf), Jul. 18 2023.
- [FLASK: Fine-grained language model evaluation based on alignment skill sets](https://arxiv.org/pdf/2307.10928.pdf), Jul. 20 2023. [project home](https://kaistai.github.io/FLASK/).
- [L-Eval: Instituting standardized evaluation for long context language models](https://arxiv.org/pdf/2307.11088.pdf), Jul. 20 2023. [code](https://github.com/OpenLMLab/LEval).
- [ARB: Advanced Reasoning Benchmark for Large Language Models](https://arxiv.org/pdf/2307.13692.pdf), Jul. 25 2023.
- [SupperCLUE: A comprehensive Chinese large language model benchmark](https://arxiv.org/pdf/2307.15020.pdf), Jul. 27 2023.
- [Med-HALT: Medical domain hallucination test for large language models](https://arxiv.org/pdf/2307.15343.pdf), Jul. 28 2023. [benchmark url](https://medhalt.github.io/).

#### Data-centric

- [Improving Retrieval-Augmented Large Language Models via Data Importance Learning](https://arxiv.org/pdf/2307.03027.pdf), Jul. 6 2023.
- [Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks](https://arxiv.org/abs/2307.02477), Jul. 5 2023.

#### Pre-training

- [Text Alignment Is An Efficient Unified Model for Massive NLP Tasks](https://arxiv.org/pdf/2307.02729.pdf), Jul. 6 2023. [code](https://github.com/yuh-zha/Align).
- [CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks](https://proceedings.mlr.press/v202/wang23t/wang23t.pdf), `icml2023`.
- [Copy is all you need](https://arxiv.org/pdf/2307.06962.pdf), Jul. 13 2023. [code](https://github.com/gmftbyGMFTBY/Copyisallyouneed).

#### Fine-tuning

- [Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain](https://arxiv.org/pdf/2307.03042.pdf), Jul. 6 2023.
- [Contrastive Error Attribution for Finetuned Language Models](https://arxiv.org/pdf/2212.10722.pdf), Jul. 11 2023.
- [Analysis of Task Transferability in Large Pre-trained Classifiers](https://arxiv.org/pdf/2307.00823.pdf), Jul. 3 2023.

#### Formal language

- [Why can neural language models solve next-word prediction? A mathematical perspective](https://arxiv.org/pdf/2306.17184.pdf), Jun. 20 2023.

#### Alignment

- [Is RLHF More Difficult than Standard RL?](https://arxiv.org/pdf/2306.14111.pdf), Jun. 25 2023.
- [Are aligned neural networks adversarially aligned?](https://arxiv.org/pdf/2306.15447.pdf), Jun. 26 2023.
- [Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision](https://arxiv.org/pdf/2306.16564.pdf), Jun. 28 2023.
- [On the Exploitability of Instruction Tuning](https://arxiv.org/pdf/2306.17194.pdf), Jun. 28 2023.
- [Overthinking the Truth: Understanding how Language Models Process False Demonstrations](https://arxiv.org/pdf/2307.09476.pdf), Jul. 18 2023.
- [Aligning Large Language Models with Human: A Survey](https://arxiv.org/pdf/2307.12966.pdf), Jul. 24 2023.
- [Evaluating the ripple effects of knowledge editing in language models](https://arxiv.org/pdf/2307.12976.pdf), Jul. 24 2023. [code](https://github.com/edenbiran/RippleEdits/).
- [Evaluating the moral beliefs encoded in LLMs](https://arxiv.org/pdf/2307.14324.pdf), Jul. 26 2023. [code](https://github.com/ninodimontalcino/moralchoice).
- [Investigating the factual knowledge boundary of large language models with retrieval augmentation](https://arxiv.org/pdf/2307.11019.pdf), Jul. 23 2023.

#### Longer context

- [Extending context window of large language models via position interpolation](https://arxiv.org/pdf/2306.15595.pdf), Jun. 28 2023.
- [Length Generalization in Arithmetic Transformers](https://arxiv.org/pdf/2306.15400.pdf), Jun. 27 2023.
- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172.pdf), Jul. 6 2023.

#### Scaling law

- [The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets](https://arxiv.org/pdf/2306.14975.pdf), Jun. 26 2023.
- [Scaling Laws Do Not Scale](https://arxiv.org/pdf/2307.03201.pdf), Jul. 5 2023.
- [Scaling Laws for Imitation Learning in NetHack](https://arxiv.org/pdf/2307.09423.pdf), Jul. 18 2023.

#### In-context learning

- [Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression](https://arxiv.org/pdf/2306.15063.pdf), Jun.26 2023.
- [One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention](https://arxiv.org/pdf/2307.03576.pdf), Jul. 7 2023.
- [Scaling in-context demonstration with structured attention](https://arxiv.org/pdf/2307.02690.pdf), Jul. 5 2023.
  - _"to overcome length limit, order sensitivity of in-context demonstration"_ 
- [InstructEval: Systematic Evaluation of Instruction Selection Methods](https://arxiv.org/pdf/2307.00259.pdf), Jul. 1 2023.
- [Trainable transformer in transformer](https://arxiv.org/pdf/2307.01189.pdf), Jul. 3 2023.
- [In-context learning in large language models learns label relationships but is not conventional learning](https://arxiv.org/pdf/2307.12375.pdf), 

#### Mechanistic interpretability

- [The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks](https://arxiv.org/pdf/2306.17844.pdf), Jun. 30 2023.
- [Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla](https://arxiv.org/pdf/2307.09458.pdf), Jul. 19 2023.
- [Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations](https://arxiv.org/abs/2307.08678), Jul. 17 2023.

#### Algorthm learning

- [Teaching Arithmetic to Small Transformers](https://arxiv.org/pdf/2307.03381.pdf), Jul. 7 2023. [code](https://github.com/lee-ny/teaching_arithmetic).

#### Prompting

- [SCOTT: Self-Consistent Chain-of-Thought Distillation](https://arxiv.org/pdf/2305.01879.pdf), May 21 2023.
- [Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models](https://arxiv.org/pdf/2306.17820.pdf), Jun. 30 2023.

#### Application

- [What Should Data Science Education Do with Large Language Models?](https://arxiv.org/pdf/2307.02792.pdf), Jul. 7 2023. `education`.
- [Building Community Driven Libraries of Natural Programs](https://openreview.net/forum?id=iRea6QCxi1), `iclr2023`.
- [TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT](https://arxiv.org/pdf/2307.08674.pdf), Jul. 17 2023.
- [FinGPT: Democratizing Internet-scale Data for Financial Large Language Models](https://arxiv.org/pdf/2307.10485.pdf), Jul. 19 2023.
- [On the (In)Effectiveness of Large Language Models for Chinese Text Correction](https://arxiv.org/pdf/2307.09007.pdf), Jul. 18 2023.
- [Large Language Models can accomplish Business Process Management Tasks](https://arxiv.org/pdf/2307.09923.pdf), Jul. 19 2023.
- [Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences](https://arxiv.org/pdf/2307.14225.pdf), Jul. 26 2023. `recomendation` `cold-start`.
- [ArcGPT: A large language model tailored for real-world archival applications](https://arxiv.org/pdf/2307.14852.pdf), Jul. 27 2023.

#### Safety

- [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/pdf/2307.02483.pdf), Jul. 5 2023.

#### Continual learning

- [Towards Robust and Efficient Continual Language Learning](https://arxiv.org/pdf/2307.05741.pdf), Jul. 11 2023.
- [Improving Language Plasticity via Pretraining with Active Forgetting](https://arxiv.org/pdf/2307.01163.pdf), Jul. 3 2023.

#### Infra.

- [Optimized Network Architectures for Large Language Model Training with Billions of Parameters](https://arxiv.org/pdf/2307.12169.pdf), Jul. 22 2023.



