
- [Statistical exploration of the manifold hypothesis](https://arxiv.org/pdf/2208.11665.pdf), Dec. 4 2023.
- [Occam's razor](https://homepages.math.uic.edu/~lreyzin/papers/blumer86.pdf), 1987.
- [Simplifying Neural Network Training Under Class Imbalance](https://arxiv.org/pdf/2312.02517.pdf), Dec. 5 2023.
- [An Invitation to Deep Reinforcement Learning](https://arxiv.org/pdf/2312.08365.pdf), Dec. 13 2023. `survey`.

### Optimization of deep neural networks

- [How to guess a gradient](https://arxiv.org/pdf/2312.05134.pdf), Dec. 7 2023.

### Self-supervised learning, representation learning etc.

- [LiDAR: Sensing linear probing performance in joint embedding SSL architectures](https://arxiv.org/pdf/2312.04000.pdf), Dec. 7 2023.
  - _"LiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the quality of representations with JE architectures"_
- [Hard View Selection for Self-Supervised Learning](https://arxiv.org/abs/2310.03940), Dec. 1 2023.
  - sample efficiency
  - _"With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS."_

### Learning theory

- [The sample complexity of multi-distribution learning](https://arxiv.org/pdf/2312.04027.pdf), Dec. 7 2023.
- [Optimal multi-distribution learning](https://arxiv.org/pdf/2312.05134.pdf), Dec. 8 2023.
- [Generalization properties of retrieval-based models](https://arxiv.org/pdf/2210.02617.pdf), Oct. 6 2022.
  - _"We analyze a local learning framework that employs an explicit local empirical risk minimization based on retrieved examples for each input instance. [...] we show that breaking down the underlying learning task into local sub-tasks enables the model to employ a low complexity parametric component to ensure good overall accuracy."_
- [Understanding and Leveraging the Learning Phases of Neural Networks](https://arxiv.org/pdf/2312.06887.pdf), Dec. 14 2023.

### Interpretability and trustworthy ml

- [Error discovery by clustering influence embeddings](https://arxiv.org/pdf/2312.04712.pdf), Dec. 7 2023.
- [Deeper Understanding of Black-box Predictions via Generalized Influence Functions](https://arxiv.org/pdf/2312.05586.pdf), Dec. 9 2023.
- [Locally-minimal probabilistic explanations](https://arxiv.org/pdf/2312.11831.pdf), Dec. 19 2023.
  - One drawback of formal abductive explanations is explanation size
- [Understanding Text Classification Data and Models Using Aggregated Input Salience](https://r2hcai.github.io/AAAI-23/files/CameraReadys/23.pdf), `aaai2024`.
- [Anytime Approximate Formal Feature Attribution](https://arxiv.org/pdf/2312.06973.pdf), Dec. 12 2023.
- [Accelerating the Global Aggregation of Local Explanations](https://arxiv.org/pdf/2312.07991.pdf), Dec. 13 2023.

**Diffusion model interpretability**

- [The Journey, Not the Destination: How Data Guides Diffusion Models](https://arxiv.org/pdf/2312.06205.pdf), Dec. 11 2023.
  - _"provides a formal notion of data attribution in the context of diffusion models"_
- [FreeFlow: A comprehensive understanding on diffusion probabilistic models via optimal transport](https://arxiv.org/pdf/2312.05486.pdf), Dec. 9 2023.
  - _"While earlier DPMs relied upon the Markovian assumption, recent methods based on **differential equations** have been rapidly applied to enhance the efficency and capabilities of these models [...] a theoretical interpretation encapsulating these diverse alg orithms is insufficient yet pressingly required to guide further development of DPMs"_
  - _"a framework that provides a thorough explanation of the diffusion formula as time-dependent optimal transport "_

### Dataset distillation

- [On the diversity and realism of distilled dataset: an efficient dataset distillation paradigm](https://arxiv.org/pdf/2312.03526.pdf), Dec. 6 2023.

### Generative models

- [Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis](https://arxiv.org/pdf/2312.03491.pdf), Dec. 65 2023.
  - _"because the pre-defined data-to-noise diffusion process, their pior distribution is restricted to a noisy representation, which provides little information of the generation target"_

### Distribution shift

- [Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift](https://arxiv.org/pdf/2312.03318.pdf), Dec. 6 2023.
  - _"in domain adaptation settings, self-training and contrastive learning offer significant complementary gains, and in semi-supervised learning setting, suprisingly, the benefits are not synergistic"_
  - _"theoretically analyze these techniques in a simplified model of distribution shift, demonstrating scenarios under which the features produced by contrastive learning can yield a good initialization for self-training to further amplify gains and achieve optimal performance, even when either method alone would fail"_
- [Adversarial learning for feature shift detection and correcetion](https://arxiv.org/pdf/2312.04546.pdf), Dec. 7 2023.
  - _"the task of localizing and correcting the features originating such shifts has not been studied in depth."_ 
  - _"We show that mainstream supervised classifiers, such as random forest or gradient boosting trees, combined with simple iterative heuristics, can localize and correct feature shifts, outperforming current statistical and neural network-based techniques."_
- [How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation](https://arxiv.org/pdf/2312.07424.pdf), Dec. 13 2023.
- [Beyond Top-Class Agreement: Using Divergences to Forecast Performance under Distribution Shift](https://arxiv.org/pdf/2312.08033.pdf), Dec. 13 2023.
- [Benchmarking Distribution Shift in Tabular Data with TableShift](https://arxiv.org/pdf/2312.07577.pdf), Dec. 14 2023.
- [Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation](https://arxiv.org/pdf/2211.12703.pdf), Apr. 17 2023.

### Causal inference

- [Invariance & causal representation learning: Prospects and limitations](https://arxiv.org/pdf/2312.03580.pdf), Dec. 6 2023.
  - _"show that invariance alone is insufficient to identify latent causal variables"_

### LLMs and miracles beyond

- [Revisiting Topic-Guided Language Models](https://arxiv.org/pdf/2312.02331.pdf), Dec. 4 2023.
  - _"Suprisingly, we find that none of these methods outperform a standard LSTM language model baseline, and most fail to learn good topics"_
- [LLM360: Towards fully transparent open-source LLMs](https://arxiv.org/pdf/2312.06550.pdf), Dec. 11 2023.
  - _"We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community"_
  - _"We pretrain two new LLMs from scratch and release them under the LLM360 framework. AMBER is a 7B English LLM pretrained on 1.3T tokens. CRYSTALCODER is a 7B English and code LLM pretrained on 1.4T tokens [...] We release all all training code, pretraining data, model checkpoints and evaluation metrics collected during pretraining for both AMBER and CRYSTALCODER. AMBER is released with 360 model checkpoints saved during training, and CRYSTALCODER with 143."_
- [Large Language Models for Mathematicians](https://arxiv.org/pdf/2312.02517.pdf), Dec. 7 2023.
  - _"In this note, we discuss to what extent they can aid professional mathematicians"_
- [TinyGSM: achieving > 80% on GSM8k with small language models](https://arxiv.org/pdf/2312.09241.pdf), Dec. 14 2023.

#### Watermarking

- [Towards Optimal Statistical Watermarking](https://arxiv.org/pdf/2312.07930.pdf), Dec. 13 2023.

#### Alignment

- [A Study on the Calibration of In-context Learning](https://arxiv.org/pdf/2312.04021.pdf), Dec. 7 2023.
  - accuracy-calibration trade-off of ICL
  - _"show that such trade-offs may get worse as we increase model size, incorporate more ICL examples, and fine-tune models using instruction, dialog, or reinforcement learning from human feedback (RLHF) on carefully curated datasets"_
- [Self-evaluation improves selective generation in large language models](https://arxiv.org/pdf/2312.09300.pdf), Dec. 14 2023.
- [Helping or herding? Reward model ensembles mitigate but do not eliminate reward hacking](https://arxiv.org/pdf/2312.09244.pdf), Dec. 14 2023.
- [Alignment for Honesty](https://arxiv.org/pdf/2312.07000.pdf), Dec. 12 2023.
- [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/pdf/2312.06681.pdf), Dec. 9 2023.

#### The art of prompting

- [Prompt optimization via adversarial in-context learning](https://arxiv.org/pdf/2312.02614.pdf), Dec. 5 2023.

#### In-context learning

- [The mechanistic basis of data dependence and abrupt learning in an in-context classification task](https://arxiv.org/pdf/2312.03002.pdf), Dec. 3 2023.
  - _"Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning"_
  - _"ICL is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning"_
 
#### Mechanistic interpretability

- [On the Role of Attention in Prompt-tuning](https://arxiv.org/pdf/2306.03435.pdf), Ju.n 6 2023.
  - Use simplified setting (simple model and formal language) for nearly total control of guaranteed analysis: _"we explore prompt-tuning for **one-layer attention architectures** and study contextual mixture-models where each input token belongs to a context-relevant or irrelevant set"_
- [Generating interpretable networks using hypernetworks](https://arxiv.org/pdf/2312.03051.pdf), Dec. 5 2023.
  - _"An essential goal in MI is to decode a network, i.e., to convert a neural network's raw weights to an interpretable algorithm"_
  - _"We explore the possibility of using hypernetworks to generate interpretable networks whose underlying algorithms are not yet known"_
- [Incidental Polysemanticity](https://arxiv.org/pdf/2312.03096.pdf), Dec. 5 2023.
  - _"polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap."_
- [Interpretability illusions in the generalization of simplified models](https://arxiv.org/pdf/2312.03656.pdf), Dec. 6 2023.
  - _"even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model's behavior out of distribution - the understanding developed from simplified representations may be an illusion"_
  - _"Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations"_
- [Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars](https://arxiv.org/pdf/2312.01429.pdf), Dec. 3 2023.
- [Zoology: Measuring and improving recall in efficient language models](https://arxiv.org/pdf/2312.04927.pdf), Dec. 8 2023.
  - _"we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. Hakuna Matata means no worries Hakuna Matata it means no ??"_
  - associative recall
  - _"we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can clos 97.4% of the gap to attention, while maintaining sub-quadratic scaling"_
- [Grokking group multiplication with cosets](https://arxiv.org/pdf/2312.06581.pdf), Dec. 11 2023.
- [Challenges with unsupervised LLM knowledge discovery](https://arxiv.org/pdf/2312.10029.pdf), Dec. 15 2023.
- [Successor heads: Recurring, interpretable attention heads in the wild](https://arxiv.org/pdf/2312.09230.pdf), Dec. 14 2023.
- [A simulatability benchmark for language model explainability](https://arxiv.org/pdf/2312.12747.pdf), Dec. 20 2023.
- [Exploring the Residual Stream of Transformers](https://arxiv.org/pdf/2312.12141.pdf), Dec. 19 2023.

#### Benchmark

- [BARDA: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability](https://arxiv.org/pdf/2312.07527.pdf), Dec. 12 2023.

#### In-context learning explained

- [Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context](https://arxiv.org/pdf/2312.06528.pdf), Dec. 14 2023.
  - _"We provide theoretical and empirical evidence that non-linear Transformers can and in fact do, learn to implement learning algorithms to learn non-linear functions in context"_
- [Can a Transformer Represent a Kalman Filter?](https://arxiv.org/pdf/2312.06937.pdf), Dec. 14 2023.

#### For data science

- [Curated LLM: Synergy of LLMs and data curation for tabular augmentation in ultra low-data regimes](https://arxiv.org/pdf/2312.12112.pdf), Dec. 19 2023.

---

### Codebase

- [ml-explore](https://github.com/ml-explore), Apple's deep learning framework in `c++`.
- [trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback](https://aclanthology.org/2023.emnlp-main.530.pdf), `emnlp2023`. [github](https://github.com/CarperAI/trlx).




