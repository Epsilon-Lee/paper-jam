
- [Statistical exploration of the manifold hypothesis](https://arxiv.org/pdf/2208.11665.pdf), Dec. 4 2023.
- [Occam's razor](https://homepages.math.uic.edu/~lreyzin/papers/blumer86.pdf), 1987.

### Interpretability



### Dataset distillation

- [On the diversity and realism of distilled dataset: an efficient dataset distillation paradigm](https://arxiv.org/pdf/2312.03526.pdf), Dec. 6 2023.

### Generative models

- [Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis](https://arxiv.org/pdf/2312.03491.pdf), Dec. 65 2023.
  - _"because the pre-defined data-to-noise diffusion process, their pior distribution is restricted to a noisy representation, which provides little information of the generation target"_

### Distribution shift

- [Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift](https://arxiv.org/pdf/2312.03318.pdf), Dec. 6 2023.
  - _"in domain adaptation settings, self-training and contrastive learning offer significant complementary gains, and in semi-supervised learning setting, suprisingly, the benefits are not synergistic"_
  - _"theoretically analyze these techniques in a simplified model of distribution shift, demonstrating scenarios under which the features produced by contrastive learning can yield a good initialization for self-training to further amplify gains and achieve optimal performance, even when either method alone would fail"_
- [Adversarial learning for feature shift detection and correcetion](https://arxiv.org/pdf/2312.04546.pdf), Dec. 7 2023.
  - _"the task of localizing and correcting the features originating such shifts has not been studied in depth."_ 
  - _"We show that mainstream supervised classifiers, such as random forest or gradient boosting trees, combined with simple iterative heuristics, can localize and correct feature shifts, outperforming current statistical and neural network-based techniques."_

### Causal inference

- [Invariance & causal representation learning: Prospects and limitations](https://arxiv.org/pdf/2312.03580.pdf), Dec. 6 2023.
  - _"show that invariance alone is insufficient to identify latent causal variables"_

### LLMs and miracles beyond

- [Revisiting Topic-Guided Language Models](https://arxiv.org/pdf/2312.02331.pdf), Dec. 4 2023.
  - _"Suprisingly, we find that none of these methods outperform a standard LSTM language model baseline, and most fail to learn good topics"_

#### The art of prompting

- [Prompt optimization via adversarial in-context learning](https://arxiv.org/pdf/2312.02614.pdf), Dec. 5 2023.

#### In-context learning

- [The mechanistic basis of data dependence and abrupt learning in an in-context classification task](https://arxiv.org/pdf/2312.03002.pdf), Dec. 3 2023.
  - _"Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning"_
  - _"ICL is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning"_
 
#### Mechanistic interpretability

- [Generating interpretable networks using hypernetworks](https://arxiv.org/pdf/2312.03051.pdf), Dec. 5 2023.
  - _"An essential goal in MI is to decode a network, i.e., to convert a neural network's raw weights to an interpretable algorithm"_
  - _"We explore the possibility of using hypernetworks to generate interpretable networks whose underlying algorithms are not yet known"_
- [Incidental Polysemanticity](https://arxiv.org/pdf/2312.03096.pdf), Dec. 5 2023.
  - _"polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap."_
- [Interpretability illusions in the generalization of simplified models](https://arxiv.org/pdf/2312.03656.pdf), Dec. 6 2023.
  - _"even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model's behavior out of distribution - the understanding developed from simplified representations may be an illusion"_
  - _"Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations"_
 



