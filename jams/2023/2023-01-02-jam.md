
- [Bayesian Persuasion](https://web.stanford.edu/~gentzkow/research/BayesianPersuasion.pdf), 2011. `game theory`.
- [Information Design: A Unified Perspective](https://economics.mit.edu/sites/default/files/publications/paper_92_information%20design.pdf), 2019. `game theory`.
- [Differentiable User Models](https://arxiv.org/pdf/2211.16277.pdf), Nov. 29 2022.
- [We don’t talk enough about how game theory, as a means to understanding humans and human economies, was a total failure.](https://twitter.com/beenwrekt/status/1611738182533668870), discussion about `game theory`.
- [Rethinking Search: Making Domain Experts out of Dilettantes](https://arxiv.org/abs/2105.02274), 2021. [tweet](https://twitter.com/metzlerd/status/1614029603471003648). [Attributed QA](https://arxiv.org/abs/2212.08037).
  - Blueprint of information acquirement beyond information retrieval.
- [Is Federated Learning a Practical PET Yet?](https://arxiv.org/pdf/2301.04017.pdf), Jan. 9 2023. `federated learning` `privacy`.
- [A Survey on Distribution Testing: Your Data is Big. But is it Blue?](http://www.theoryofcomputing.org/articles/gs009/gs009.pdf), Clément L. Canonne, 2020.

### Data augmentation

- [Learning multimodal data augmentation in feature space](https://arxiv.org/pdf/2212.14453.pdf), Dec. 29 2022. `tabular data`.
  - data augmentation in feature space.
- [Careful Data Curation Stabilizes In-context Learning](https://arxiv.org/pdf/2212.10378.pdf), Dec. 20 2022. `data-centric`.

### Utilities of text rationale

- [Learning to Scaffold: Optimizing Model Explanations for Teaching](https://arxiv.org/pdf/2204.10810.pdf), Nov. 30 2022. `nips2022`.
- [Evaluating Explanations: How much do explanations from the teacher aid students?](https://www.cs.cmu.edu/~ddanish/papers/exp-as-comm.pdf), 2020. `tacl2022`.
- [Are shortest rationales the best explanations for human understanding?](https://aclanthology.org/2022.acl-short.2.pdf), `acl2022`.
- [ExSum: From local explanation to model understanding](https://aclanthology.org/2022.naacl-main.392.pdf), `naacl2022`.
- [Locally aggregated feature attribution on natural language model understanding](https://aclanthology.org/2022.naacl-main.159.pdf), `naacl2022`.
- [The irrationality of neural rationale models](https://aclanthology.org/2022.trustnlp-1.6.pdf), July 14 2022. `acl2022` `TrustNLP2022`.

### Mechanistic interpretability

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html), Sept. 14 2022. [arXiv](https://arxiv.org/ftp/arxiv/papers/2209/2209.10652.pdf).
- [Superposition, Memorization, and Double Descent](https://transformer-circuits.pub/2023/toy-double-descent/index.html), Jan. 5 2023.
- [Thinking Like Transformers](https://srush.github.io/raspy/), by Sasha Rush and Eran Yahav. [tweet](https://twitter.com/srush_nlp/status/1605213547264450560?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email).
- [Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models](https://arxiv.org/abs/2301.04213), Jan. 10 2023. [tweet](https://twitter.com/peterbhase/status/1613573825932697600).
- [Thinking Like Transformers](https://arxiv.org/pdf/2106.06981.pdf), Jul. 19 2021.
- [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/pdf/2301.05062.pdf), Jan. 14 2023. [tweet](https://twitter.com/davlindner/status/1613900577804525573).

### Beyond MLE

- [Quantile risk control: a flexible framework for bounding the probability of high-loss prediction](https://arxiv.org/pdf/2212.13629.pdf), Dec. 27 2022.
  - previous works: bounding the expected loss of a predictor --> problem: not sufficient in risk-sensitive applications where the _distribution of errors_ is important
  - propose a flexible framework to produce a family of bounds on quantiles of the loss distribution incurred by a predictor.

### Graph machine learning

- [Benchmarking Graph Neural Networks](https://arxiv.org/pdf/2003.00982.pdf), Dec. 28 2022.
- [Introduction to graph machine learning](https://huggingface.co/blog/intro-graphml), Jan. 3 2023. `blogpost`.
  - [Tweet: Want to learn about Machine Learning for Graphs?](https://twitter.com/osanseviero/status/1614718194941562880). Jan. 16.


### Decision-making

- [Assessing AI Fairness in Finance](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681673), Jan. 2022.
- [auton-survival: An Open-Source Package for Regression, Counterfactual Estimation, Evaluation and Phenotyping Censored Time-to-Event Data](https://blog.ml.cmu.edu/2022/08/05/auton-survival-an-open-source-package-for-regression-counterfactual-estimation-evaluation-and-phenotyping-censored-time-to-event-data/), Aug. 5 2022.

### Alignment

- [TruthfulQA: Measuring how models mimics human falsehoods](https://arxiv.org/pdf/2109.07958.pdf), May 8 202.2
- [Announcing the Inverse Scaling Prize](https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool), Jun. 27 2022. `blogpost`.
- [My AI Safety Lecture for UT Effective Altruism](https://scottaaronson.blog/?p=6823). Nov. 2022.
- [The alignment problem from a deep learning perspective](https://arxiv.org/pdf/2209.00626.pdf), Dec. 16 2022.

### Efficient NLP

- [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034), Dec. 28 2022.

### A = LLMs

- [A large language model for electronic health records](https://www.nature.com/articles/s41746-022-00742-2), Dec. 2022.
- [A fine-grained comparison of pragmatic language understanding in humans and language models](https://arxiv.org/pdf/2212.06801.pdf), Dec. 13 2022. `LLMs and pragmatics`.
- [Minerva: Solving Quantitative Reasoning Problems with Language Models](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html), Jun. 30 2022.
- [On emergence, scaling and inductive bias](https://www.yitay.net/blog/emergence-and-scaling), Nov. 16 2022. `blogpost` authored by Yi Tay.
- [Demonstrate-Search-Predict: composing retrieval and language models for knowledge-intensive NLP](https://arxiv.org/abs/2212.14024), Dec. 28 2022.
- [Rethinking with retrieval: faithful large language model inference](https://arxiv.org/abs/2301.00303), Dec. 31 2022.  [github](https://github.com/frankxu2004/knnlm-why).
  - How to use external knowledge to assist LLMs.
- [When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories](https://akariasai.github.io/files/llm_memorization.pdf), Dec. 21 2022. [tweet](https://twitter.com/AkariAsai/status/1605314211445280768?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email).
- [Memory Augmented Large Language Models are Computationally Universal](https://arxiv.org/pdf/2301.04589.pdf), Jan. 10 2023.
  - _"the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts"_
  - _"The result in this paper is distinct from previous studies that investigate the computational universality of neural sequence models such as recurrent neural networks and Transformers. The key distinction is that we consider a fixed language model with frozen weights, and show how external memory augmentation can elicit universal computational behavior"_
  - previous works _"do not apply to existing large language models without altering their weights (as far as currently known.)"_

#### In-context learning

- [General-purpose in-context learning by meta-learning transformers](https://arxiv.org/pdf/2212.04458.pdf), Dec. 8 2022.
- [A Survey for In-context Learning](https://arxiv.org/pdf/2301.00234.pdf), Dec. 31 2022. `survey` `pku`.
- [Careful Data Curation Stabilizes In-context Learning](https://arxiv.org/pdf/2212.10378.pdf), Dec. 20 2022. `data-centric`.

#### Product from chatGPT

- [GPTDuck](https://www.gptduck.com/), GPTDuck - question answering against any Github repository.
- [Bird SQL](https://twitter.com/perplexity_ai/status/1603441221753372673), [interface](https://www.perplexity.ai/sql). Twitter search interface that is powered by Perplexity’s structured search engine. It uses OpenAI Codex to translate natural language into SQL, giving everyone the ability to navigate large datasets like Twitter.
- [elicit](https://elicit.org/), The AI Research Assistant.
  - Elicit uses language models to help you automate research workflows, like parts of literature review.
  - Elicit can find relevant papers without perfect keyword match, summarize takeaways from the paper specific to your question, and extract key information from the papers.
  - While answering questions with research is the main focus of Elicit, there are also other research tasks that help with brainstorming, summarization, and text classification.
- [Create amazing blog posts art & images marketing copy sales emails SEO content Facebook ads web content love letters captions video scripts blog posts 10X faster with AI.](https://www.jasper.ai/), Jasper is the AI Content Generator that helps you and your team break through creative blocks to create amazing, original content 10X faster.
- [DUST: Design and Deploy Large Language Model Apps](https://dust.tt/).
- [Mental health with chatGPT](https://twitter.com/RobertRMorris/status/1611450197707464706).

#### Language reasoning

- [LAMBADA: Backward Chaining for Automated Reasoning in Natural Language](https://arxiv.org/abs/2212.13894), Dec. 20 2022. [tweet](https://twitter.com/martin_gorner/status/1608450724433907714).

### Causal inference

- [Identifiable deep generative models via sparse coding](https://openreview.net/forum?id=vd0onGWZbE), Jun. 15 2022. `tmlr2022`.
- [Learning latent structural causal models](https://arxiv.org/pdf/2210.13583.pdf), Oct. 24 2022.

### Unclassified blogs, tweets

- [A friendly introduction to principal component analysis](https://peterbloem.nl/blog/pca), Sep. 17 2020.
  - There are part 1 to part 5, 5 articles plus 1 notebook in this series.
- [How to Make the Most of Your Python Debugger in VSCode](https://towardsdatascience.com/how-to-make-most-of-your-python-debugger-in-vscode-9e05dfce533f), Feb. 2022.
- 💚 [Computers can be understood](https://blog.nelhage.com/post/computers-can-be-understood/), Feb. 2020. `computer system` `programming`.
- [What AI can tell us about intelligence](https://www.noemamag.com/what-ai-can-tell-us-about-intelligence/), Jun. 16 2022.
- [Injecting some numbers into the AGI debate](https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/), Jun. 27 2022.
- [My opening statement at the ICML 2022 Debate](https://kyunghyuncho.me/my-opening-statement-at-the-icml-2022-debate/), Jul. 23 2022. Kyunhyun Cho. `debate on science or engineering` that drives machine learning.
- [Retro is blazingly fast](http://mitchgordon.me/ml/2022/07/01/retro-is-blazing.html), Jul. 1 2022. `LLMs`
- [Long-term dynamics of fairness intervention in connection recommender systems](https://blog.ml.cmu.edu/2022/09/02/long-term-dynamics-of-fairness-intervention-in-connection-recommender-systems/), Sep. 2 2022.
- [What are the odds?](https://terrytao.wordpress.com/2022/10/03/what-are-the-odds/), Oct. 3 2022. `probability theory`
- 💚 [Too much efficiency makes everything worse: overfitting and strong version of Goodhart's law](https://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html), Nov. 7 2022. `generalization theory` `Goodhart's law`.
- [The Batch: issue 174](https://www.deeplearning.ai/the-batch/issue-174/), Dec. 7 2022.
- [Tweet: reverse engineering about Copilot](https://twitter.com/parth007_96/status/1604160949434421248), Dec. 18 2022.
- [Transferable skills and how to talk about them](https://hkotek.com/blog/altac-transferable_skills/), Dec. 31 2022.
- [Towards Deployable RL - What’s Broken with RL Research and a Potential Fix](https://avivtamar.substack.com/p/deployablerl?utm_source=twitter&sd=pf), Jan. 6 2023.
- [Carnegie Mellon University 10721: Philosophical Foundations of Machine Intelligence 2021](https://github.com/acmi-lab/cmu-10721-philosophy-machine-intelligence), 2023 coming soon.
- [Recursive games with ChatGPT](https://gist.github.com/liorfox/a5dc1d9a3fac894591666056971979ae), Jan. 2023.
  - _"TL;DR: I present examples of apparent "symbolic" capabilities of ChatGPT, and discuss some context and possible interpretations"_
- [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/), Jan. 10 2023.
- [Tweet: how to select GPUs?](https://twitter.com/Tim_Dettmers/status/1614992180472610816).

### Other interesting info.

- [Instant neural graphics primitives: lightning fast NeRF and more](https://github.com/NVlabs/instant-ngp), `cg`.
  - The [tech report](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) about 'multiresolution hash encoding'
- [A Concept Knowledge Graph for User Next Intent Prediction at Alipay](https://arxiv.org/abs/2301.00503), Jan. 2 2023. `e-commerce platform intent prediction`.



