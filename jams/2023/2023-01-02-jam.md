
- [Bayesian Persuasion](https://web.stanford.edu/~gentzkow/research/BayesianPersuasion.pdf), 2011. `game theory`.
- [Information Design: A Unified Perspective](https://economics.mit.edu/sites/default/files/publications/paper_92_information%20design.pdf), 2019. `game theory`.
- [Differentiable User Models](https://arxiv.org/pdf/2211.16277.pdf), Nov. 29 2022.
- [We donâ€™t talk enough about how game theory, as a means to understanding humans and human economies, was a total failure.](https://twitter.com/beenwrekt/status/1611738182533668870), discussion about `game theory`.
- [Rethinking Search: Making Domain Experts out of Dilettantes](https://arxiv.org/abs/2105.02274), 2021. [tweet](https://twitter.com/metzlerd/status/1614029603471003648). [Attributed QA](https://arxiv.org/abs/2212.08037).
  - Blueprint of information acquirement beyond information retrieval.
- [Is Federated Learning a Practical PET Yet?](https://arxiv.org/pdf/2301.04017.pdf), Jan. 9 2023. `federated learning` `privacy`.
- [A Survey on Distribution Testing: Your Data is Big. But is it Blue?](http://www.theoryofcomputing.org/articles/gs009/gs009.pdf), ClÃ©ment L. Canonne, 2020.
- [Optimization and sampling textbook by Yin Tat Lee and Santosh Vempala, far from done](https://github.com/YinTat/optimizationbook).
- [Human-Timescale Adaptation in an Open-Ended Task Space](https://arxiv.org/pdf/2301.07608.pdf), Jan. 18 2023.
  - _"We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains"_
- [MineDojo](https://github.com/MineDojo/MineDojo), Building Open-Ended Embodied Agents with Internet-Scale Knowledge. `nips2022` best paper.
- This paper is also related to the above two [Can Wikipedia Help Offline Reinforcement Learning?](https://arxiv.org/abs/2201.12122), Jul. 24 2022.
  - It is about how to do transfer learning with knowledge described in textual data.
- [OneFlow: Redesign the Distributed Deep Learning Framework from Scratch](https://arxiv.org/pdf/2110.15032.pdf), Apr. 19 2022.
- [RuleBERT: Teaching Soft Rules to Pre-Trained Language Models](https://arxiv.org/pdf/2109.13006.pdf), Sep. 24 2021. [github](https://github.com/MhmdSaiid/RuleBert).
  - This might used in data-scarcity scenarios via incorporating expert priors.
  - [Fixing Model Bugs with Natural Language Patches](https://homes.cs.washington.edu/~marcotcr/emnlp22_patches.pdf), `emnlp2022`.
    - Though this work is more about interpretability and editable model, it seems to match the incorporation of human prior knowledge here as _patches_.
- Similar to [Perspectives on Incorporating Expert Feedback into Model Updates](https://arxiv.org/pdf/2205.06905.pdf), Jul. 16 2022.
- [DEEPSTRUCT: Pretraining of Language Models for Structure Prediction](https://arxiv.org/pdf/2205.10475.pdf), May 21 2022. [github](https://github.com/cgraywang/deepstruct).
  - Wonder how this work compares with gpt3 or chatgpt.

### Generative models

- [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970), Aug. 25 2022.
- [Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve](https://arxiv.org/pdf/2212.03905.pdf), `tmlr2022`.

### Weak supervision

- [Training Complex Models with Multi-Task Weak Supervision](https://arxiv.org/pdf/1810.02840.pdf), Dec. 7 2018.
  - Weaker forms of supervision that provide noisier but cheaper labels are often used
  - Issues:
    - weak supervision sources have diverse and unknown accuracies
    - may output correlated labels
    - label different tasks
    - apply at different levels of granularity
  - They show that by solving a matrix completion-style problem, they can recover the accuracies of these multi-task sources given their dependency structure, but without any labeled data, leading to higher-quality supervision for training an end model
- [WRENCH: A Comprehensive Benchmark for Weak Supervision](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/1c9ac0159c94d8d0cbedc973445af2da-Paper-round2.pdf), `nips2021`.
  - Benchmark.
- [Label Propagation with Weak Supervision](https://arxiv.org/pdf/2210.03594.pdf), Oct. 7 2022.
- [Shoring up the foundations: fusing model embeddings and weak supervision](https://proceedings.mlr.press/v180/chen22e/chen22e.pdf), `uai2022`.

### Tabular representation learning

- [Why do tree-based models still outperform deep learning on tabular data?](https://arxiv.org/pdf/2207.08815.pdf), Jul. 18 2022. [github](https://github.com/LeoGrin/tabular-benchmark).
  - _"45 datasets from various domains with clear characteristics of tabular data"_
  - _"tree-based models remain state-of-the-art on medium-sized data (~10K samples) even without accounting for their superior speed."_
  - Desiderata for building tabular-specific NN models: _"be robust to uninformative features; preserve the orientation of the data; be able to easily learn irregular functions"_

### Data augmentation

- [Learning multimodal data augmentation in feature space](https://arxiv.org/pdf/2212.14453.pdf), Dec. 29 2022. `tabular data`.
  - data augmentation in feature space.
- [Careful Data Curation Stabilizes In-context Learning](https://arxiv.org/pdf/2212.10378.pdf), Dec. 20 2022. `data-centric`.

### Utilities of text rationale

- [Learning to Scaffold: Optimizing Model Explanations for Teaching](https://arxiv.org/pdf/2204.10810.pdf), Nov. 30 2022. `nips2022`.
- [Evaluating Explanations: How much do explanations from the teacher aid students?](https://www.cs.cmu.edu/~ddanish/papers/exp-as-comm.pdf), 2020. `tacl2022`.
- [Are shortest rationales the best explanations for human understanding?](https://aclanthology.org/2022.acl-short.2.pdf), `acl2022`.
- [ExSum: From local explanation to model understanding](https://aclanthology.org/2022.naacl-main.392.pdf), `naacl2022`.
- [Locally aggregated feature attribution on natural language model understanding](https://aclanthology.org/2022.naacl-main.159.pdf), `naacl2022`.
- [The irrationality of neural rationale models](https://aclanthology.org/2022.trustnlp-1.6.pdf), July 14 2022. `acl2022` `TrustNLP2022`.

### Mechanistic interpretability

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html), Sept. 14 2022. [arXiv](https://arxiv.org/ftp/arxiv/papers/2209/2209.10652.pdf).
- [Superposition, Memorization, and Double Descent](https://transformer-circuits.pub/2023/toy-double-descent/index.html), Jan. 5 2023.
- [Thinking Like Transformers](https://srush.github.io/raspy/), by Sasha Rush and Eran Yahav. [tweet](https://twitter.com/srush_nlp/status/1605213547264450560?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email).
- [Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models](https://arxiv.org/abs/2301.04213), Jan. 10 2023. [tweet](https://twitter.com/peterbhase/status/1613573825932697600).
- [Thinking Like Transformers](https://arxiv.org/pdf/2106.06981.pdf), Jul. 19 2021.
- [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/pdf/2301.05062.pdf), Jan. 14 2023. [tweet](https://twitter.com/davlindner/status/1613900577804525573).

### Beyond MLE

- [Quantile risk control: a flexible framework for bounding the probability of high-loss prediction](https://arxiv.org/pdf/2212.13629.pdf), Dec. 27 2022.
  - previous works: bounding the expected loss of a predictor --> problem: not sufficient in risk-sensitive applications where the _distribution of errors_ is important
  - propose a flexible framework to produce a family of bounds on quantiles of the loss distribution incurred by a predictor.

### Graph machine learning

- [Benchmarking Graph Neural Networks](https://arxiv.org/pdf/2003.00982.pdf), Dec. 28 2022.
- [Introduction to graph machine learning](https://huggingface.co/blog/intro-graphml), Jan. 3 2023. `blogpost`.
  - [Tweet: Want to learn about Machine Learning for Graphs?](https://twitter.com/osanseviero/status/1614718194941562880). Jan. 16.

### Decision-making

- [Assessing AI Fairness in Finance](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681673), Jan. 2022.
- [auton-survival: An Open-Source Package for Regression, Counterfactual Estimation, Evaluation and Phenotyping Censored Time-to-Event Data](https://blog.ml.cmu.edu/2022/08/05/auton-survival-an-open-source-package-for-regression-counterfactual-estimation-evaluation-and-phenotyping-censored-time-to-event-data/), Aug. 5 2022.

### Alignment

- [TruthfulQA: Measuring how models mimics human falsehoods](https://arxiv.org/pdf/2109.07958.pdf), May 8 202.2
- [Announcing the Inverse Scaling Prize](https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool), Jun. 27 2022. `blogpost`.
- [My AI Safety Lecture for UT Effective Altruism](https://scottaaronson.blog/?p=6823). Nov. 2022.
- [The alignment problem from a deep learning perspective](https://arxiv.org/pdf/2209.00626.pdf), Dec. 16 2022.

### Efficient NLP

- [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034), Dec. 28 2022.

### A = LLMs

- [A large language model for electronic health records](https://www.nature.com/articles/s41746-022-00742-2), Dec. 2022.
- [A fine-grained comparison of pragmatic language understanding in humans and language models](https://arxiv.org/pdf/2212.06801.pdf), Dec. 13 2022. `LLMs and pragmatics`.
- [Minerva: Solving Quantitative Reasoning Problems with Language Models](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html), Jun. 30 2022.
- [On emergence, scaling and inductive bias](https://www.yitay.net/blog/emergence-and-scaling), Nov. 16 2022. `blogpost` authored by Yi Tay.
- [Demonstrate-Search-Predict: composing retrieval and language models for knowledge-intensive NLP](https://arxiv.org/abs/2212.14024), Dec. 28 2022.
- [Rethinking with retrieval: faithful large language model inference](https://arxiv.org/abs/2301.00303), Dec. 31 2022.  [github](https://github.com/frankxu2004/knnlm-why).
  - How to use external knowledge to assist LLMs.
- [When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories](https://akariasai.github.io/files/llm_memorization.pdf), Dec. 21 2022. [tweet](https://twitter.com/AkariAsai/status/1605314211445280768?cn=ZmxleGlibGVfcmVjcw%3D%3D&refsrc=email).
- [Memory Augmented Large Language Models are Computationally Universal](https://arxiv.org/pdf/2301.04589.pdf), Jan. 10 2023.
  - _"the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts"_
  - _"The result in this paper is distinct from previous studies that investigate the computational universality of neural sequence models such as recurrent neural networks and Transformers. The key distinction is that we consider a fixed language model with frozen weights, and show how external memory augmentation can elicit universal computational behavior"_
  - previous works _"do not apply to existing large language models without altering their weights (as far as currently known.)"_
- [Dissociating language and thought in large language models: a cognitive perspective](https://arxiv.org/abs/2301.06627), Jan. 16 2023.
- [Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling](https://arxiv.org/abs/2301.06568), Jan. 16 2023. `protein language models`.
- [Tweet: dedup might not work that well](https://twitter.com/BlancheMinerva/status/1618634405333078017), Jan. 25 2023. See this [tweet](https://twitter.com/Tim_Dettmers/status/1618650683737255936) as well.

#### In-context learning

- [General-purpose in-context learning by meta-learning transformers](https://arxiv.org/pdf/2212.04458.pdf), Dec. 8 2022.
- [A Survey for In-context Learning](https://arxiv.org/pdf/2301.00234.pdf), Dec. 31 2022. `survey` `pku`.
- [Careful Data Curation Stabilizes In-context Learning](https://arxiv.org/pdf/2212.10378.pdf), Dec. 20 2022. `data-centric`.
- [Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning](https://arxiv.org/pdf/2301.07067.pdf), Jan. 17 2023.

#### Product from chatGPT

- [GPTDuck](https://www.gptduck.com/), GPTDuck - question answering against any Github repository.
- [Bird SQL](https://twitter.com/perplexity_ai/status/1603441221753372673), [interface](https://www.perplexity.ai/sql). Twitter search interface that is powered by Perplexityâ€™s structured search engine. It uses OpenAI Codex to translate natural language into SQL, giving everyone the ability to navigate large datasets like Twitter.
- [elicit](https://elicit.org/), The AI Research Assistant.
  - Elicit uses language models to help you automate research workflows, like parts of literature review.
  - Elicit can find relevant papers without perfect keyword match, summarize takeaways from the paper specific to your question, and extract key information from the papers.
  - While answering questions with research is the main focus of Elicit, there are also other research tasks that help with brainstorming, summarization, and text classification.
- [Create amazing blog posts art & images marketing copy sales emails SEO content Facebook ads web content love letters captions video scripts blog posts 10X faster with AI.](https://www.jasper.ai/), Jasper is the AI Content Generator that helps you and your team break through creative blocks to create amazing, original content 10X faster.
- [DUST: Design and Deploy Large Language Model Apps](https://dust.tt/).
- [Mental health with chatGPT](https://twitter.com/RobertRMorris/status/1611450197707464706).

#### Language reasoning

- [LAMBADA: Backward Chaining for Automated Reasoning in Natural Language](https://arxiv.org/abs/2212.13894), Dec. 20 2022. [tweet](https://twitter.com/martin_gorner/status/1608450724433907714).

#### Inverse-scaling

- [MemoTrap dataset](https://twitter.com/alisawuffles/status/1618347159807750144), Jan. 25 2023.
  - a dataset of 2.5K examples spanning text completion, translation & QA, where repeating memorized text & concepts is *not* the desired behavior. We find that LMs perform worseðŸ“‰ as they scale up, revealing severe failures in simple instruction-following.
- [Inverse scaling-law prizes](https://twitter.com/EthanJPerez/status/1617981045282082817), Jan. 25 2023.

### Causal inference

- [Estimating individual treatment effect: generalization bounds and algorithms](http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf), `icml2017`.
- [Identifiable deep generative models via sparse coding](https://openreview.net/forum?id=vd0onGWZbE), Jun. 15 2022. `tmlr2022`.
- [Learning latent structural causal models](https://arxiv.org/pdf/2210.13583.pdf), Oct. 24 2022. `representation learning` for `ite`.
  - _`the algorithms learn a balanced representation s.t. the induced treated and control distributions look similar`_

### Unclassified blogs, tweets

- [A friendly introduction to principal component analysis](https://peterbloem.nl/blog/pca), Sep. 17 2020.
  - There are part 1 to part 5, 5 articles plus 1 notebook in this series.
- [How to Make the Most of Your Python Debugger in VSCode](https://towardsdatascience.com/how-to-make-most-of-your-python-debugger-in-vscode-9e05dfce533f), Feb. 2022.
- ðŸ’š [Computers can be understood](https://blog.nelhage.com/post/computers-can-be-understood/), Feb. 2020. `computer system` `programming`.
- [What AI can tell us about intelligence](https://www.noemamag.com/what-ai-can-tell-us-about-intelligence/), Jun. 16 2022.
- [Injecting some numbers into the AGI debate](https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/), Jun. 27 2022.
- [My opening statement at the ICML 2022 Debate](https://kyunghyuncho.me/my-opening-statement-at-the-icml-2022-debate/), Jul. 23 2022. Kyunhyun Cho. `debate on science or engineering` that drives machine learning.
- [Retro is blazingly fast](http://mitchgordon.me/ml/2022/07/01/retro-is-blazing.html), Jul. 1 2022. `LLMs`
- [Long-term dynamics of fairness intervention in connection recommender systems](https://blog.ml.cmu.edu/2022/09/02/long-term-dynamics-of-fairness-intervention-in-connection-recommender-systems/), Sep. 2 2022.
- [What are the odds?](https://terrytao.wordpress.com/2022/10/03/what-are-the-odds/), Oct. 3 2022. `probability theory`
- ðŸ’š [Too much efficiency makes everything worse: overfitting and strong version of Goodhart's law](https://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html), Nov. 7 2022. `generalization theory` `Goodhart's law`.
- [The Batch: issue 174](https://www.deeplearning.ai/the-batch/issue-174/), Dec. 7 2022.
- [Tweet: reverse engineering about Copilot](https://twitter.com/parth007_96/status/1604160949434421248), Dec. 18 2022.
- [Transferable skills and how to talk about them](https://hkotek.com/blog/altac-transferable_skills/), Dec. 31 2022.
- [Towards Deployable RL - Whatâ€™s Broken with RL Research and a Potential Fix](https://avivtamar.substack.com/p/deployablerl?utm_source=twitter&sd=pf), Jan. 6 2023.
- [Carnegie Mellon University 10721: Philosophical Foundations of Machine Intelligence 2021](https://github.com/acmi-lab/cmu-10721-philosophy-machine-intelligence), 2023 coming soon.
- [Recursive games with ChatGPT](https://gist.github.com/liorfox/a5dc1d9a3fac894591666056971979ae), Jan. 2023.
  - _"TL;DR: I present examples of apparent "symbolic" capabilities of ChatGPT, and discuss some context and possible interpretations"_
- [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/), Jan. 10 2023.
- [Tweet: how to select GPUs?](https://twitter.com/Tim_Dettmers/status/1614992180472610816).
- [Tweet: FlashAttention in Adept](https://twitter.com/tri_dao/status/1615417167448780801).
- [Tweet: Markov inequality](https://twitter.com/shortstein/status/1615699209126285315).
- [Universal Image Segmentation with Mask2Former and OneFormer](https://huggingface.co/blog/mask2former), Mar. 17 2022.
  - This guide introduces Mask2Former and OneFormer, 2 state-of-the-art neural networks for image segmentation. The models are now available in ðŸ¤— transformers, an open-source library that offers easy-to-use implementations of state-of-the-art models. Along the way, you'll learn about the difference between the various forms of image segmentation.

### Other interesting info.

- [Instant neural graphics primitives: lightning fast NeRF and more](https://github.com/NVlabs/instant-ngp), `cg`.
  - The [tech report](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) about 'multiresolution hash encoding'
- [A Concept Knowledge Graph for User Next Intent Prediction at Alipay](https://arxiv.org/abs/2301.00503), Jan. 2 2023. `e-commerce platform intent prediction`.




