
- [Memory-Based Model Editing at Scale](https://arxiv.org/abs/2206.06520), Jun. 13 2022. `icml2022`.
- [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682), Jun. 15 2022.
- [Quantifying Adaptability in Pre-trained Language Models with 500 Tasks](https://arxiv.org/abs/2112.03204), `naacl2022`, [tweet](https://twitter.com/jacobandreas/status/1522557628005724160).
- [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615), Jun. 9 2022, [github repo](https://github.com/google/BIG-bench).

- [A Learning-Theoretic Framework for Certified Auditing of Machine Learning Models](https://arxiv.org/pdf/2206.04740.pdf), Jun. 9 2022. `certified safety`.
- [Prioritized Training on Points that are learnable, Worth Learning, and Not Yet Learnt](https://arxiv.org/pdf/2206.07137.pdf), Jun. 16 2022. `learning under noise`.

- [Reconstructing Training Data from Trained Neural Networks](https://arxiv.org/pdf/2206.07758.pdf), Jun. 15 2022,  `memorization` `privacy`.
- [The Privacy Onion Effect: Memorization is Relative](https://arxiv.org/abs/2206.10469), `privacy` `memorization`.
