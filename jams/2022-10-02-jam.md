**Outline**

- [MLOps](#mlops)
- [Self-Supervised Learning](#self-supervised-learning)
- [Diffusion Models](#diffusion-models)
- [Deep Generative Model](#deep-generative-model)
- [Causal Inference](#causal-inference)
- [Understanding Data Augmentation](#understanding-data-augmentation)
- [Zero-shot Dialogue Modelling and Dialogue Benchmark](#zero-shot-dialogue-modelling-and-dialogue-benchmark)
- [Fast Deep Learning](#fast-deep-learning)
- [Interpretability](#interpretability)
- [Conformal Prediction and Calibration](#conformal-prediction-and-calibration)

---

- [Exploring Low Rank Training of Deep Neural Networks](https://arxiv.org/abs/2209.13569), Sep. 27 2022. `efficient training of nn`
- [Entity Tagging: Extracting Entities in Text Without Mention Supervision](https://arxiv.org/abs/2209.06148), `ner`
- [Are Shortest Rationales the Best Explanations for Human Understanding?](https://aclanthology.org/2022.acl-short.2/), `acl2022` `text rationale`
- [No Free Lunch in ‚ÄúPrivacy for Free: How does Dataset Condensation Help Privacy‚Äù](https://arxiv.org/pdf/2209.14987.pdf), `critics` `privacy`
- [Compositional Semantic Parsing with Large Language Models](https://arxiv.org/pdf/2209.15003.pdf), Sep. 29 2022 `compositionality`
- [Learning the Transformer Kernel](https://arxiv.org/pdf/2110.08323.pdf), `tmlr202207`
- [Active Learning Through a Covering Lens](https://arxiv.org/pdf/2205.11320.pdf), May 23 2022. `active learning` `data-centric AI`
- [A Theory of Dynamic Benchmarks](https://arxiv.org/pdf/2210.03165.pdf), Oct. 6 2022. `adaptive benchmark`
- [Learnware: Small Models Do Big](https://arxiv.org/pdf/2210.03647.pdf), Oct. 7 2022. `MLOps` `Zhi-Hua Zhou`
- [Recitation-Augmented Language Models](https://arxiv.org/pdf/2210.01296.pdf)Ôºå Oct. 4 2022. `retrieval`
- [Benchmarking Compositionality with Formal Languages](https://arxiv.org/pdf/2208.08195.pdf), Sep. 20 2022. `compositionality`
- [Label Propagation with Weak Supervision](https://arxiv.org/pdf/2210.03594.pdf), Oct. 7 2022. submitted to `iclr2023`
- [Data Driven Semi-Supervised Learning](https://arxiv.org/pdf/2103.10547.pdf), Sep. 29 2021. `nips2021`
- [Neural Attentive Circuits](https://arxiv.org/pdf/2210.08031.pdf), Oct. 19 2022.
  - similar to neural architecture search?
- [Efficiently Controlling Multiple Risks with Pareto Testing](https://arxiv.org/pdf/2210.07913.pdf), Oct. 14 2022.
  - _"such that their predictions provably satisfy multiple explicit and simultaneous statistical guarantees (e.g. upper-bounded error rates)"_
- [Freeze then Train: Towards Provable Representation Learning under Spurious and Feature Noise](https://arxiv.org/pdf/2210.11075.pdf), Oct. 20 2022.


---

### MLOps

- [SEAL: Interactive Tool for Systematic Error Analysis and Labeling](https://arxiv.org/pdf/2210.05839.pdf), Oct. 11 2022.
- ü§ç [Efficiently Computing Local Lipschitz Constants of Neural Networks via Bound Propagation](https://arxiv.org/pdf/2210.07394.pdf), Oct. 13 2022. [github](https://github.com/shizhouxing/Local-Lipschitz-Constants).
  - Lipshitz constants are connected to many properties of neural networks, such as robustness, fairness, and generalization.
  - This paper develop an efficient framework for computing the $l_\infty$ local lipschitz constant of a neural network by **tightly** upper bounding the norm of Clarke Jacobian via linear bound propagation.
  - Applications:
    - On tiny models, the proposed method produces comparable bounds compared to exact methods that cannot scale to slightly larger models; and on larger models, the proposed method efficiently produces tighter bound results than exsiting methods; and the proposed method can scale to much larger practical models that previously cannot be handled.
    - An application on provable monotonicity analysis
      - A binary classification task about predicting income level. The task involves continuous features, we aim to check the monotonicity of income level w.r.t. age, education level, capital gain, capital loss, and hours per week.
   - *Very interesting and new paper to me. Zico has recently been focusing on formal verification, which might have the potential to make NN trustworthy*.


---

### Self-Supervised Learning

- [Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations](https://arxiv.org/pdf/2209.14905.pdf), Sep. 29 2022.
- [Joint Embedding Self-supervised Learning in The Kernel Regime](https://arxiv.org/pdf/2209.14884.pdf), Sep. 29 2022.

---

### Diffusion Models

- [Analyzing Diffusion as Serial Reproduction](https://arxiv.org/pdf/2209.14821.pdf), Sep. 29 2022.

### Deep Generative Model

- [The Union of Manifolds Hypothesis and its Implications for Deep Generative Modelling](https://arxiv.org/abs/2207.02862), Jul. 6 2022.

---

### Causal Inference

- [Falsification before Extrapolation in Causal Effect Estimation](https://arxiv.org/pdf/2209.13708.pdf), Sep. 29 2022.

---

### Understanding Data Augmentation

- [How Data Augmentation affects Optimization for Linear Regression](https://arxiv.org/abs/2010.11171), Oct. 2021.

---

### Zero-shot Dialogue Modelling and Dialogue Benchmark

- [BERT for Joint Intent Classification and Slot Filling](https://arxiv.org/pdf/1902.10909.pdf), Feb. 2019.
- [Zero-Shot Dialog Generation with Cross-Domain Latent Actions](https://aclanthology.org/W18-5001.pdf), `sigdial2018`.
- [AISFG: Abundant Information Slot Filling Generator](https://aclanthology.org/2022.naacl-main.308.pdf), `naacl2022`.
- [Robust Retrieval Augmented Generation for Zero-shot Slot Filling](https://arxiv.org/pdf/2108.13934.pdf), Sep. 2021.
- [mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling](https://arxiv.org/pdf/2203.12940.pdf), Jun. 28 2022.
- [Doc2Dial](https://doc2dial.github.io/), 2021.

---

### Fast Deep Learning

- [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433), Sep. 12 2022.

---

### Interpretability

- [AdaAX: Explaining Recurrent Neural Networks by Learning Automata with Adaptive States](https://dl.acm.org/doi/pdf/10.1145/3534678.3539356), `kdd2022`.
- [Approximate Conditional Coverage via Neural Model Approximations](https://arxiv.org/pdf/2205.14310.pdf), Sep. 30 2022. `knn` `conformal prediction`
- [Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks](https://arxiv.org/pdf/2207.13243.pdf), `transparency`
- [Problems with Shapley-value-based explanations as feature importance measures](https://arxiv.org/abs/2207.11208), Jun. 2020.
- [Verifiable and Provably Secure Machine Unlearning](https://arxiv.org/pdf/2210.09126.pdf), Oct. 17 2022.

---

### Conformal Prediction and Calibration

- [Predictive Inference with Feature Conformal Prediction](https://arxiv.org/pdf/2210.00173.pdf), Oct. 1 2022.
  - This paper proposes `feature conformal prediction` which is very different from applying conformal prediction to the prediction/output of the predictor (traditional way)
  - The combination of the idea of conformal with representation learning (a very Bengionian proposal!)
- [The Calibration Generalization Gap](https://arxiv.org/pdf/2210.01964.pdf), Oct. 6 2022.
- [Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning](https://arxiv.org/pdf/2002.06470.pdf), Jul. 18 2021 v4. `uncertainty estimation`
- [Bayesian Optimization with Conformal Coverage  Guarantees](https://arxiv.org/abs/2210.12496#), Oct. 22 2022. [tweet](https://twitter.com/samuel_stanton_/status/1584932428962885632). [github](https://github.com/samuelstanton/conformal-bayesopt). [motivation paper](https://arxiv.org/abs/2202.03613).


