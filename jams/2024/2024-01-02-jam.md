
- [Learning Rich Rankings](https://arxiv.org/pdf/2312.15081.pdf), Dec. 22 2023. `l2r`.
- [The thermodynamics of prediction](https://arxiv.org/pdf/1203.3271.pdf), Oct. 5 2023.
- [Trajeglish: Learning the langauge of driving senarios](https://arxiv.org/pdf/2312.04535.pdf), Dec. 7 2023. `autonomous driving`.
- [Pearl: A Production-Ready Reinforcement Learning Agent](https://arxiv.org/pdf/2312.03814.pdf), Dec. 6 2023. [github](https://pearlagent.github.io/).
- [What planning problems can a relational neural network solve?](https://arxiv.org/pdf/2312.03682.pdf), Dec. 6 2023. `inductive bias`.
- [Distribution-Dependent Rates for Multi-Distribution Learning](https://arxiv.org/pdf/2312.13130.pdf), Dec. 20 2023. `multi-distribution learning`.
- [A Novel Metric for Measuring Data Quality in Classification Applications](https://arxiv.org/pdf/2312.08066.pdf), Dec. 13 2023.
- [Discovering modular solutions that generalize compositionally](https://arxiv.org/pdf/2312.15001.pdf), Dec. 22 2023.
- [A Weighted K-Center Algorithm for Data Subset Selection](https://arxiv.org/pdf/2312.10602.pdf), Dec. 17 2023. `data selection`.
- [Anomaly component analysis](https://arxiv.org/pdf/2312.16139.pdf), Dec. 26 2023. `anomaly detection`.
- [Dataset Difficulty and the Role of Inductive Bias](https://arxiv.org/pdf/2401.01867.pdf), Jan. 3 2024. `inductive bias`.

### Interpretability

- [LETA: Learning Transferable Attribution for Generic Vision Explainer](https://arxiv.org/pdf/2312.15359.pdf), Dec. 23 2023.
- [Position Paper: Bridging the Gap Between Machine Learning and Sensitivity Analysis](https://arxiv.org/pdf/2312.13234.pdf), Dec. 20 2023.
- [Pyreal: A Framework for Interpretable ML Explanations](https://arxiv.org/pdf/2312.13084.pdf), Dec. 20 2023.

### Time series

- [Deep non-parametric time series forcaster](https://arxiv.org/pdf/2312.14657.pdf), Dec. 22 2023.
- [SutraNets: Sub-series autoregressive networks for long-sequence, probabilistic forecasting](https://arxiv.org/pdf/2312.14880.pdf), Dec. 22 2023.

### Generalization and optimization mysteries

- [Strong inductive biases provably prevent harmless interpolations](https://arxiv.org/pdf/2301.07605.pdf), Mar. 1 2023.
- [Understanding the Role of Optimization in Double Descent](https://arxiv.org/pdf/2312.03951.pdf), Dec. 6 2023.
- [On the Trajectories of SGD Without Replacement](https://arxiv.org/pdf/2312.16143.pdf), Dec. 26 2023.

### Distribution shift

- [Generative posterior networks for approximately Bayesian epistemic uncertainty estimation](https://arxiv.org/pdf/2312.17411.pdf), Dec. 29 2023.
- [Deep unsupervised domain adaptation for time series classification: A benchmark](https://arxiv.org/pdf/2312.09857.pdf), Dec. 18 2023.
- [When Model Meets New Normals: Test-time Adaptation for Unsupervised Time-series Anomaly Detection](https://arxiv.org/pdf/2312.11976.pdf), Dec. 19 2023.

### LLMs and beyond

- [Principled Gradient-based Markov Chain Monte Carlo for Text Generation](https://arxiv.org/pdf/2312.17710.pdf), Dec. 29 2023.
- [Large Language Models for Generative Information Extraction: A Survey](https://arxiv.org/pdf/2312.17617.pdf), Dec. 29 2023.
  - This might an entrance of useful information extraction techniques that can be applied to collection dialogues or same product identification task.
- [Using Large Language Models for Hyperparameter Optimizatio](https://arxiv.org/pdf/2312.04528.pdf), Dec. 7 2023.
- [AutoNumerics-Zero: Automated Discovery of State-of-the-Art Mathematical Functions](https://arxiv.org/pdf/2312.08472.pdf), Dec. 13 2023.

#### In-context learning

- [Can Transformers Learn Sequential Function Classes In Context?](https://arxiv.org/pdf/2312.12655.pdf), Dec. 21 2023.

#### Model merging

- [Merging by matching models in task subspaces](https://arxiv.org/pdf/2312.04339.pdf), Dec. 7 2023. [github](https://github.com/r-three/mats).
- [Concrete subspace learning based interference elimination for multi-task model fusion](https://arxiv.org/pdf/2312.06173.pdf), Dec. 11 2023.

#### Watermarks

- [On the learnability of watermarks for language models](https://arxiv.org/pdf/2312.04469.pdf), Dec. 7 2023. [github](https://github.com/chenchenygu/watermark-learnability).

#### Alignment

- [Teaching language models with canonical examples](https://openreview.net/pdf?id=SJwXWwc47T), `neurips2023`.
- [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/pdf/2312.12736.pdf), Dec. 20 2023.
- [What makes good data for alignment? A comprehensive study of automatic data selection in instruction tuning](https://arxiv.org/pdf/2312.15685.pdf), Dec. 25 2023.
- [Understanding data influence on context scaling: a close look at baseline solution](https://yaofu.notion.site/Understanding-data-influence-on-context-scaling-a-close-look-at-baseline-solution-eb17eab795dd4132b1a1ffe73f5e850a), Dec. 22 2023.
- [Theoretical guarantees on the best-of-n alignment policy](https://arxiv.org/pdf/2401.01879.pdf), Jan. 3 2024.

#### Scaling law

- [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/pdf/2210.10760.pdf), Oct. 19 2022.
- [SOLAR 10.7B: Scaling large language models with simple yet effective depth up-scaling](https://arxiv.org/pdf/2312.15166.pdf), Dec. 23 2023.

#### Finetuning

- [Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models](https://arxiv.org/pdf/2312.07887.pdf), Dec. 13 2023.

#### Reasoning

- [Neural algorithmic reasoning](https://thegradient.pub/neural-algorithmic-reasoning/), Oct. 14 2023.

#### Creativity

- [Can AI Be as Creative as Humans?](https://arxiv.org/pdf/2401.01623.pdf), Jan. 3 2024.

---

### Montly special topics

#### Deep learning optimization

> An growing list of papers.

- [Don't decay the learning rate, increase the batch size](https://arxiv.org/pdf/1711.00489.pdf), Feb. 24 2018.
  - _"one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam"_
  - _"reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallism and shorter training times"_
- [Super-Convergence: Very fast training of neural networks using large learning rates](https://arxiv.org/pdf/1708.07120.pdf), May 17 2018.
  - _"where neural networks can be trained an order of magnitude faster than with standard training methods"_
  - _"The existence of super-convergence is relevant to understanding why deep networks generalize well"_
- [A disciplined approach to neural network hyper-parameters: Part I - learning rate, batch size, momentum, and weight decay](https://arxiv.org/pdf/1803.09820.pdf), Apr. 24 2018.
- [Understanding batch normalization](https://arxiv.org/pdf/1806.02375.pdf), `neurips2018`.
  - _"show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization"_
- [A Bayesian perspective on generalization and stochastic gradient descent](https://arxiv.org/pdf/1710.06451.pdf), `iclr2018`.
  - _"We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy"_
- [Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks](https://proceedings.neurips.cc/paper/2019/file/bce9abf229ffd7e570818476ee5d7dde-Paper.pdf), `neurips2019`.
  - _"Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed"_
  - _"because the small learning rate model first memorizes easy-to-generalize, hard-to-fit patterns, it generalize worse on hard-to-generalize, easier-to-fit patterns than its large learning rate counterpart"_
- [A convergence theory for deep learning via over-parameterization](http://proceedings.mlr.press/v97/allen-zhu19a/allen-zhu19a.pdf), `icml2019`.
- [Why do larger models generalize better? A theoretical perspective via XOR problem](http://proceedings.mlr.press/v97/brutzkus19b/brutzkus19b.pdf), `icml2019`.
- [Cyclical annealing schedule: A simple approach to mitigating KL vanishing](https://arxiv.org/pdf/1903.10145.pdf), `naacl2019`.
- [On empirical comparisons of optimizers for deep learning](https://arxiv.org/abs/1910.05446), Oct. 11 2019.
  - _"demonstrate the sensitivity of optimizer comparisons to the hyperparameter tuning protocol [...] Our findings suggest that the hyperparameter search space may be the single most important factor explaining the rankings obtained by recent empirical comparisons in the literature"_
  - _"As tuning effort grows without bound, more general optimizers should never underperform the ones they can approximate (i.e. Adam should never perform worse than momentum)"_









