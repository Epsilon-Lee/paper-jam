
- [Recursively Summarizing Books with Human Feedback](https://arxiv.org/pdf/2109.10862.pdf), OpenAI.


- [Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach](https://arxiv.org/pdf/2010.07835.pdf), Tuo Zhao's group.

  - They study the setting of `finetuning with weak supervision` or without any labeled data
  - Issues: how to prevent overfitting to noise?
  - A contrastive self-learning framework: contrastive regularization and confidence-based weighting
