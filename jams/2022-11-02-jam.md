Outline.
- [PhD thesis](#phd-thesis)
- xxx

---

- [In Defense of the Unitary Scalarization for Deep Multi-Task Learning](https://arxiv.org/abs/2201.04122), `nips2022`.
- [TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation](https://hal.archives-ouvertes.fr/hal-03515501/document), 2022.
- [The Curious Case of Benign Memorization](https://arxiv.org/pdf/2210.14019.pdf), Oct. 25 2022.
- [Combining Machine Learning and Lifetime-based Resource Management for Memory Allocation and Beyond](https://colinraffel.com/publications/cacm2022combining.pdf),  Proceedings of the TwentyFifth International Conference on Architectural Support for Programming Languages and Operating Systems 2022.
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805#), June. 15 2021.
- [Compute Trends across Three Era of Machine Learning](https://arxiv.org/pdf/2202.05924.pdf), Mar. 9 2022.
- [K-SAM: Sharpness-Aware Minimization at the Speed of SGD](https://arxiv.org/pdf/2210.12864.pdf), Oct. 23 2022.
- [The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing](https://aclanthology.org/2021.acl-long.84.pdf), `acl2021`. `human-centered`
- [CNT (Conditioning on Noisy Targets): A new Algorithm for Leveraging Top-Down Feedback](https://arxiv.org/pdf/2210.09505.pdf), Oct. 27 2022.
- [Understanding Domain Learning in Language Models Through Subpopulation Analysis](https://arxiv.org/pdf/2210.12553.pdf), Oct. 22 2022. `visualization` `svcca`
- [On Mutual Information Maximization for Representation Learning](https://arxiv.org/abs/1907.13625), Jan. 23 2020. `iclr2020` [tweet discussion](https://twitter.com/skornblith/status/1156928383013576705)
  - _"Why do ML researchers keep applying information theory to deterministic settings?"_
- [Productivity and Reuse in Language](https://web.stanford.edu/~ngoodman/papers/odonnell-cogsci11.pdf), `cognitive science`

---

### PhD thesis

- [Optimisation & Generalisation in Networks of Neurons](https://arxiv.org/pdf/2210.10101.pdf), Oct. 18 2022.

### Finetuning

- [Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://arxiv.org/pdf/2210.14199.pdf), Oct. 25 2022.

### Data augmentation

- [Provably Learning Diverse Features in Multiview Data with Midpoint Mixup](https://arxiv.org/pdf/2210.13512.pdf), arXiv Oct. 24 20022.

### Text generation

- [Contrastive decoding: open-ended text generation as optimization](https://arxiv.org/abs/2210.15097), Oct. 27 2022.

### Foundation models

- [What Language Model to Train if You Have One Million GPU Hours?](https://arxiv.org/pdf/2210.15424.pdf), Oct. 27 2022.

### Data-centric, learning dynamics

- [Characterizing Datapoints via Second-Split Forgetting](https://arxiv.org/pdf/2210.15424.pdf), Oct. 26 2022.
