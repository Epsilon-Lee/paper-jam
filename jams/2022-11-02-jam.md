Outline.
- [PhD thesis](#phd-thesis)
- [Dataset](#dataset)
- [Finetuning](#finetuning)
- [Data augmentation](#data-augmentation)

---

- [In Defense of the Unitary Scalarization for Deep Multi-Task Learning](https://arxiv.org/abs/2201.04122), `nips2022`.
- [TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation](https://hal.archives-ouvertes.fr/hal-03515501/document), 2022.
- [The Curious Case of Benign Memorization](https://arxiv.org/pdf/2210.14019.pdf), Oct. 25 2022.
- [Combining Machine Learning and Lifetime-based Resource Management for Memory Allocation and Beyond](https://colinraffel.com/publications/cacm2022combining.pdf),  Proceedings of the TwentyFifth International Conference on Architectural Support for Programming Languages and Operating Systems 2022.
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805#), June. 15 2021.
- [Compute Trends across Three Era of Machine Learning](https://arxiv.org/pdf/2202.05924.pdf), Mar. 9 2022.
- [K-SAM: Sharpness-Aware Minimization at the Speed of SGD](https://arxiv.org/pdf/2210.12864.pdf), Oct. 23 2022.
- [The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing](https://aclanthology.org/2021.acl-long.84.pdf), `acl2021`. `human-centered`
- [CNT (Conditioning on Noisy Targets): A new Algorithm for Leveraging Top-Down Feedback](https://arxiv.org/pdf/2210.09505.pdf), Oct. 27 2022.
- [Understanding Domain Learning in Language Models Through Subpopulation Analysis](https://arxiv.org/pdf/2210.12553.pdf), Oct. 22 2022. `visualization` `svcca`
- [On Mutual Information Maximization for Representation Learning](https://arxiv.org/abs/1907.13625), Jan. 23 2020. `iclr2020` [tweet discussion](https://twitter.com/skornblith/status/1156928383013576705)
  - _"Why do ML researchers keep applying information theory to deterministic settings?"_
- [Productivity and Reuse in Language](https://web.stanford.edu/~ngoodman/papers/odonnell-cogsci11.pdf), `cognitive science`
- [Interpretability in the wild: A circuit for indirect object identification in GPT-2 small](https://arxiv.org/pdf/2211.00593.pdf), arXiv  Nov. 1 2022. `mechanistic interpretability`
- [Characterizing intrinsic compositionality in transformers with tree projections](https://arxiv.org/pdf/2211.01288.pdf), Nov. 2 2022. `compositionality` `interpretability`
- [An Information-Theoretic Framework for Supervised Learning](https://arxiv.org/pdf/2203.00246.pdf), Jun. 7 2022.
- [Reinforcement Learning, Bit by Bit](https://arxiv.org/abs/2103.04047), arXiv.v2 2022.
- [Human Language Understanding & Reasoning](https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf), Chris Manning 2022.
- [On Writing a Textbook on Natural Language Processing](https://aclanthology.org/2021.teachingnlp-1.22.pdf), Jacob Eisenstein 2021.
- [Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation](https://arxiv.org/pdf/2211.01292.pdf), Nov. 2 2022. `nmt`
- [Stewardship of global collective behavior](https://www.pnas.org/doi/10.1073/pnas.2025764118), Jun. 21 2021. `pnas` `computational social science`
- [How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?](https://openreview.net/forum?id=c0l2YolqD2T), `nips2022` `dataset&benchmark track` `unsupervised learning`
- [If Beam Search is the Answer, What was the Question?](https://aclanthology.org/2020.emnlp-main.170.pdf), `emnlp2020`.
- [Revisiting the Uniform Information Density Hypothesis](https://arxiv.org/pdf/2109.11635.pdf), `emnlp2021`.
- [Jury Learning: Integrating Dissenting Voices into Machine Learning Models](https://dl.acm.org/doi/pdf/10.1145/3491102.3502004), `chi2022`.
- [A Survey on Causal Inference](https://arxiv.org/pdf/2002.02770.pdf), arXiv 2020. [因果推断漫谈](https://dango.rocks/blog/2019/01/08/Causal-Inference-Introduction1/). [加权、分层、匹配](https://joyspace.jd.com/pages/xUPr4RMV5jlTiEnaviSs).
- [Scaling up Trustless DNN Inference with Zero-Knowledge Proofs](https://arxiv.org/pdf/2210.08674.pdf), arXiv Oct. 17 2022. `trustworthy ml`.
- [A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models](https://arxiv.org/pdf/2210.12023.pdf), Oct. 24 2022.
- [Algorithms with Prediction Portfolios](https://arxiv.org/pdf/2210.12438.pdf), Oct. 22 2022.
- [Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data](https://arxiv.org/pdf/2210.13043.pdf), Oct. 24 2022.
- [Generating synthetic data in finance: opportunities, challenges and pitfalls](https://www.jpmorgan.com/content/dam/jpm/cib/complex/content/technology/ai-research-publications/pdf-8.pdf),  Workshop on AI in Financial Services: Data, Fairness, Explainability, Trustworthiness, and Privacy, 2019.
- [End-to-End Learning to Index and Search in Large Output Spaces](https://arxiv.org/abs/2210.08410), `extreme classification`.
- [Machine learning for streaming data: state of the art, challenges, and opportunities](https://www.kdd.org/exploration_files/3._CR_7._Machine_learning_for_streaming_data_state_of_the_art-Final.pdf), `kdd2019`. [Online ML Package](https://github.com/online-ml/river).
- [ProtoVAE: A Trustworthy Self-Explainable Prototypical Variational Model](https://arxiv.org/pdf/2210.08151.pdf), Oct. 15 2022. `nips2022`. `interpretability`
- [Bayesian subset selection and variable importance for interpretable prediction and classification](https://arxiv.org/pdf/2104.10150.pdf), arXiv Feb. 16 2022. `nips2022`. `interpretability`
- [Neural Basis Models for Interpretability](https://arxiv.org/pdf/2205.14120.pdf), Oct. 18 2022. `nips2022`. `transparent models`
- [Task-aware Retrieval with Instructions](https://arxiv.org/pdf/2211.09260.pdf), Nov. 16 2022.
- [Introduction to Online Nonstochastic Control](https://arxiv.org/abs/2211.09619), Nov. 17 2022. `book`
- [On Measures of Biases and Harms in NLP](https://arxiv.org/pdf/2108.03362.pdf), Oct. 13 2022. `trustworthy nlp` `bias`
- [Weakly Supervised Representation Learning with Sparse Perturbations](https://arxiv.org/pdf/2206.01101.pdf), Jun. 2 2022. `representation learning`
- [DeepMed: Semiparametric Causal Mediation Analysis with Debiased Deep Learning](https://arxiv.org/pdf/2210.04389.pdf), Oct. 10 2022. `interpretability` `mediation analysis`
- [Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs](https://arxiv.org/pdf/2004.11440.pdf), May 30 2020. `interpretability`
- [Semantic Probabilistic Layers for Neuro-Symbolic Learning](https://openreview.net/pdf?id=o-mxIWAY1T8), `nips2022` `reasoning` `neuro-symbolic`
- [Seminar of Category Theory for AI](https://cats.for.ai/program/).
- [A Neural Corpus Indexer for Document Retrieval](https://openreview.net/pdf?id=fSfcEYQP_qc), `nips2022 outstanding paper`.
- [Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation](https://arxiv.org/pdf/1710.06169.pdf), `model auditing` `transparancy`.
- [Nevis'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research](https://arxiv.org/pdf/2211.11747.pdf), Nov. 15 2022. `dynamic benchmarking`.
- [Schrödinger’s Bat: Diffusion Models Sometimes Generate Polysemous Words in Superposition](https://arxiv.org/pdf/2211.13095.pdf), Nov. 23 2022. issue of `polysemy` with `stable diffusion`
- [Hierarchical Phrase-based Sequence-to-Sequence Learning](https://arxiv.org/pdf/2211.07906.pdf), `emnlp2022`. [code](https://github.com/berlino/btg-seq2seq).

---

### Representation learning

- [Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods](https://arxiv.org/abs/2205.11508), May 23 2022. `nips2022`
- [Self-Supervised Learning based on Heat Equation](https://arxiv.org/pdf/2211.13228.pdf), Nov. 23 2022.

### PhD thesis

- [Optimisation & Generalisation in Networks of Neurons](https://arxiv.org/pdf/2210.10101.pdf), Oct. 18 2022.
- [Objective Criteria for Explainable Machine Learning](https://kilthub.cmu.edu/articles/thesis/Objective_Criteria_for_Explainable_Machine_Learning/21569385), 2022.
- [Dissection of Deep Neural Networks](https://dissection.csail.mit.edu/Bau-davidbau-PhD-EECS-2021-thesis.pdf), Sep. 2021. `mechanistic interpretability`.

### Dataset

- [The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset](https://openreview.net/forum?id=UoEw6KigkUn), `nips2022`.
  - This paper documents the data creation and curation efforts undertaken by BigScience to assemble the **Responsible Open-Science Open-Collaboration Text Sources (ROOTS)**, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion parameter _BigScience Large Open-science Open-access Multilingual_ (BLOOM) language model.
- [Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation](https://openreview.net/forum?id=UrAYT2QwOX8&noteId=7OMGhdDG25), `nips2022`.
  - This paper presents Bank Account Fraud (BAF), the first publicly available, privacy-preserving, large-scale, realistic suite of tabular datasets.
  - This suite was generated by applying state-of-the-art tabular data generation techniques on an anonymized, real-world bank account opening fraud detection dataset.
- [DC-BENCH: Dataset Condensation Benchmark](https://arxiv.org/pdf/2207.09639.pdf), Oct. 17 2022, `nips2022`.
  - Dataset condensation is a newly emerging techinique aiming at learning a tiny dataset that captures the rich information encoded in the original dataset.
  - DC-Bench is the first comprehensive benchmark for dataset synthesis methods.

### Finetuning

- [Downstream Datasets Make Suprisingly Good Pretraining Corpora](https://arxiv.org/pdf/2209.14389.pdf), arXiv Sep. 28 2022.
- [Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://arxiv.org/pdf/2210.14199.pdf), Oct. 25 2022.
- [How to Fine-Tune Vision Models with SGD](https://arxiv.org/pdf/2211.09359.pdf), Nov. 17 2022. `ood generalization`
  - Under ood setting, finetuning with AdamW is significantly better than momentum SGD.
  - The authors find that large gaps occur when the finetuning gradients in the first embedding layer are much larger than in the rest of the model.
  - Simple fix: freezing the embedding layer.
- [Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping](https://arxiv.org/abs/2210.10325), Oct. 19 2022.

### Data augmentation

- [Provably Learning Diverse Features in Multiview Data with Midpoint Mixup](https://arxiv.org/pdf/2210.13512.pdf), arXiv Oct. 24 20022.
- [A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments](https://arxiv.org/abs/2202.08325), arXiv Feb. 16 2022. `nips2022`
- [The Effects of Regularization and Data Augmentation are Class Dependent](https://arxiv.org/abs/2204.03632), Apr. 7 2022. `nips2022`

### Text generation

- [Contrastive decoding: open-ended text generation as optimization](https://arxiv.org/abs/2210.15097), Oct. 27 2022.

### Foundation models

- [What Language Model to Train if You Have One Million GPU Hours?](https://arxiv.org/pdf/2210.15424.pdf), Oct. 27 2022.

### Data-centric, learning dynamics

- [Characterizing Datapoints via Second-Split Forgetting](https://arxiv.org/pdf/2210.15424.pdf), Oct. 26 2022.
- [Optimizing Data Collection for Machine Learning](https://arxiv.org/pdf/2210.01234.pdf), `nips2022` arXiv Oct. 3 2022.

### Machine teaching

- [Iterative Teaching by Data Hallucination](https://arxiv.org/pdf/2210.17467.pdf), OCt. 31 2022.
- [Measuring Progress on Scalable Oversight for Large Language Models](https://arxiv.org/abs/2211.03540), Nov. 4 2022.

### Scaling-up

- [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf), Mar. 29 2022.
- [Broken Neural Scaling Laws](https://arxiv.org/pdf/2210.14891.pdf), Nov. 10 2022.

### MLOps

- [A Human-Centric Perspective on Model Monitoring](https://arxiv.org/pdf/2206.02868.pdf), arXiv Sep. 20 2022.
- [ModelDiff: A Framework for Comparing Learning Algorithms](https://arxiv.org/pdf/2211.12491.pdf), Nov. 22 2022.
- [Manifold: A Model-Agnostic Visual Debugging Tool for Machine Learning at Uber](https://www.uber.com/blog/manifold/), Jan. 14 2019.
- [Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims](https://arxiv.org/pdf/2004.07213.pdf), Apr. 20 2020.

### Trustworthy ML

- [Measuring Forgetting of Memorized Training Examples](https://arxiv.org/pdf/2207.00099.pdf), Jun. 30 2022. `memorization and privacy`
- [Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://arxiv.org/pdf/2210.17546.pdf), Oct. 31 2022. `memorization and privacy`


