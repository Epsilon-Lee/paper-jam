Outline.
- [PhD thesis](#phd-thesis)
- xxx

---

- [In Defense of the Unitary Scalarization for Deep Multi-Task Learning](https://arxiv.org/abs/2201.04122), `nips2022`.
- [TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation](https://hal.archives-ouvertes.fr/hal-03515501/document), 2022.
- [The Curious Case of Benign Memorization](https://arxiv.org/pdf/2210.14019.pdf), Oct. 25 2022.
- [Combining Machine Learning and Lifetime-based Resource Management for Memory Allocation and Beyond](https://colinraffel.com/publications/cacm2022combining.pdf),  Proceedings of the TwentyFifth International Conference on Architectural Support for Programming Languages and Operating Systems 2022.
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805#), June. 15 2021.
- [Compute Trends across Three Era of Machine Learning](https://arxiv.org/pdf/2202.05924.pdf), Mar. 9 2022.
- [K-SAM: Sharpness-Aware Minimization at the Speed of SGD](https://arxiv.org/pdf/2210.12864.pdf), Oct. 23 2022.
- [The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing](https://aclanthology.org/2021.acl-long.84.pdf), `acl2021`. `human-centered`
- [CNT (Conditioning on Noisy Targets): A new Algorithm for Leveraging Top-Down Feedback](https://arxiv.org/pdf/2210.09505.pdf), Oct. 27 2022.
- [Understanding Domain Learning in Language Models Through Subpopulation Analysis](https://arxiv.org/pdf/2210.12553.pdf), Oct. 22 2022. `visualization` `svcca`
- [On Mutual Information Maximization for Representation Learning](https://arxiv.org/abs/1907.13625), Jan. 23 2020. `iclr2020` [tweet discussion](https://twitter.com/skornblith/status/1156928383013576705)
  - _"Why do ML researchers keep applying information theory to deterministic settings?"_
- [Productivity and Reuse in Language](https://web.stanford.edu/~ngoodman/papers/odonnell-cogsci11.pdf), `cognitive science`
- [Iterative Teaching by Data Hallucination](https://arxiv.org/pdf/2210.17467.pdf), OCt. 31 2022. `machine teaching`
- [Interpretability in the wild: A circuit for indirect object identification in GPT-2 small](https://arxiv.org/pdf/2211.00593.pdf), arXiv  Nov. 1 2022. `mechanistic interpretability`
- [Characterizing intrinsic compositionality in transformers with tree projections](https://arxiv.org/pdf/2211.01288.pdf), Nov. 2 2022. `compositionality` `interpretability`
- [An Information-Theoretic Framework for Supervised Learning](https://arxiv.org/pdf/2203.00246.pdf), Jun. 7 2022.
- [Reinforcement Learning, Bit by Bit](https://arxiv.org/abs/2103.04047), arXiv.v2 2022.
- [Human Language Understanding & Reasoning](https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf), Chris Manning 2022.
- [On Writing a Textbook on Natural Language Processing](https://aclanthology.org/2021.teachingnlp-1.22.pdf), Jacob Eisenstein 2021.
- [Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation](https://arxiv.org/pdf/2211.01292.pdf), Nov. 2 2022. `nmt`
- [Stewardship of global collective behavior](https://www.pnas.org/doi/10.1073/pnas.2025764118), Jun. 21 2021. `pnas` `computational social science`
- [How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?](https://openreview.net/forum?id=c0l2YolqD2T), `nips2022` `dataset&benchmark track` `unsupervised learning`
- [If Beam Search is the Answer, What was the Question?](https://aclanthology.org/2020.emnlp-main.170.pdf), `emnlp2020`.
- [Revisiting the Uniform Information Density Hypothesis](https://arxiv.org/pdf/2109.11635.pdf), `emnlp2021`.

---

### PhD thesis

- [Optimisation & Generalisation in Networks of Neurons](https://arxiv.org/pdf/2210.10101.pdf), Oct. 18 2022.

### Finetuning

- [Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://arxiv.org/pdf/2210.14199.pdf), Oct. 25 2022.

### Data augmentation

- [Provably Learning Diverse Features in Multiview Data with Midpoint Mixup](https://arxiv.org/pdf/2210.13512.pdf), arXiv Oct. 24 20022.

### Text generation

- [Contrastive decoding: open-ended text generation as optimization](https://arxiv.org/abs/2210.15097), Oct. 27 2022.

### Foundation models

- [What Language Model to Train if You Have One Million GPU Hours?](https://arxiv.org/pdf/2210.15424.pdf), Oct. 27 2022.

### Data-centric, learning dynamics

- [Characterizing Datapoints via Second-Split Forgetting](https://arxiv.org/pdf/2210.15424.pdf), Oct. 26 2022.
