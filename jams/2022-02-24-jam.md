
- [Quantifying the Effects of Data Augmentation](https://arxiv.org/pdf/2202.09134.pdf), Feb. 18 2022. `da theory`
- [Masked prediction tasks: a parameter identifiability view](https://arxiv.org/pdf/2202.09305.pdf), Feb. 18 2022. `self-supervised learning theory`
- [DataMUX: Data Multiplexing for Neural Networks](https://arxiv.org/pdf/2202.09318.pdf), Feb. 18 2022. `efficient computation`
- [A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments](https://arxiv.org/pdf/2202.08325.pdf), `da theory`
- [Energy-based Constrained Text Generation with Langevin Dynamics](https://arxiv.org/pdf/2202.11705.pdf), `decoding` `constrained text generation`
- [Red Teaming Language Models with Language Models](https://www.deepmind.com/research/publications/2022/Red-Teaming-Language-Models-with-Language-Models), Feb. 7 2022. `trustworthy lm`
- [A Survey on Vision Transformer](https://arxiv.org/pdf/2012.12556.pdf), Feb. 23 2022.
- [A History of Meta-gradient: Gradient Methods for Meta-learning](https://arxiv.org/pdf/2202.09701.pdf), Feb. 20 2022.
- [DATALAB: A Platform for Data Analysis and Intervention](https://arxiv.org/pdf/2202.12875.pdf), Feb. 25 2022. `data-centric`
- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837.pdf), Feb. 25 2022. `in-context learning`
- [Overcoming a Theoretical Limitation of Self-Attention](https://arxiv.org/pdf/2202.12172.pdf), Feb. 24 2022. `self-attention`
