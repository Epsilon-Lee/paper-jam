
# Fine-tuning methods for foundation models

- [Domain Adversarial Fine-Tuning as an Effective Regularizer](https://arxiv.org/abs/2009.13366), [github](https://github.com/GeorgeVern/AFTERV1.0), `emnlp2020`.
- [Recall and learn: Fine-tuning deep pretrained language models with less forgetting](https://arxiv.org/abs/2004.12651), `emnlp2018`.
- [Smart: Robust and efficient fine-tuning for pre trained natural language models through principled regularized optimization](https://arxiv.org/abs/1911.03437), `acl2020`.
- [Mixout: Effective regularization to finetune large-scale pretrained language models](https://arxiv.org/abs/1909.11299), `iclr2020`.
- [Better fine-tuning by reducing representational collapse](https://arxiv.org/abs/2008.03156), `iclr2021`.
- [NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better](https://arxiv.org/pdf/2202.12024.pdf), `acl2022`.
- [Raise a child in large language model: Towards effective and generalizable fine-tuning](https://arxiv.org/abs/2109.05687), `emnlp2022`.
- [AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning](https://arxiv.org/pdf/2210.05883.pdf), `nips2022`.
- [ROSE: Robust Selective Fine-tuning for Pre-trained Language Models](https://arxiv.org/pdf/2210.09658.pdf), `emnlp2022`.
