
### Bert distillation

- [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351), Sep. 23 2019.
- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf), Mar. 1 2020.
- [FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/pdf/2004.02178.pdf), Apr. 29 2020.
- [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/pdf/2004.02984.pdf), Apr. 14 2020.

### Efficiently using foundation models

- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339.pdf), Aug. 15 2022.
  > *"This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs."*
