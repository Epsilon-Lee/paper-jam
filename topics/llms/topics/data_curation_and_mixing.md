
- [Awesome-LLM-Synthetic-Data](https://github.com/wasiahmad/Awesome-LLM-Synthetic-Data?tab=readme-ov-file).
- [Large language models for data annotation and synthesis: A survey](https://aclanthology.org/2024.emnlp-main.54.pdf), EMNLP 2024.
- [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/pdf/2305.10429), Nov. 21 2023.
- [TinyGSM: achieving > 80% on GSM8k with small language models](https://arxiv.org/pdf/2312.09241), Dec. 14 2023.
- [What makes good data for alignment? A comphrehensive study of automatic data selection in instruction tuning](https://arxiv.org/pdf/2312.15685), Apr. 16 2024.
- [MetaMath: Bootstrap your own methematical questions for large language models](https://arxiv.org/pdf/2309.12284), May 3 2024.
- [Instruction Mining: When Data Mining Meets Large Language Model Finetuning](https://arxiv.org/abs/2307.06290), COLM 2024.
- [Data, Data Everywhere: A Guide for Pretraining Dataset Construction](https://www.arxiv.org/pdf/2407.06380), Jul. 8 2024. `data curation`.
  - _"we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain [...] we show how such attribute information can be used to further refine and improve the quality of a pretraining set"_
- [On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](https://arxiv.org/pdf/2406.15126), Jun. 14 2024. `data synthesis`.
- [Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG](https://arxiv.org/abs/2406.13069), Jun. 24 2024.
- [The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale](https://arxiv.org/pdf/2406.17557v1), Jun. 25 2024.
- [Entropy Law: The Story Behind Data Compression and LLM Performance](https://arxiv.org/pdf/2407.06645), Jul. 11 2024. `data selection` `learning dynamics`.
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://arxiv.org/pdf/2407.03502), Jul. 3 2024.
- [How NuminaMath Won the 1st AIMO Progress Prize](https://huggingface.co/blog/winning-aimo-progress-prize), Jul. 11 2024. `blogpost`.
- [TAGCOS: Task-agnostic gradient clustered coreset selection for instruction tuning data](https://arxiv.org/pdf/2407.15235), Jul. 21 2024. [code](https://github.com/2003pro/TAGCOS).
- [Consent in Crisis: The Rapid Decline of the AI Data Commons](https://arxiv.org/pdf/2407.14933), Jul. 20 2024.
- [Open Artificial Knowledge](https://oakdataset.org/), Jul. 19 2024
- [Programming every example: Lifting pretraining data quality like experts at scale](https://arxiv.org/pdf/2409.17115), Sep. 25 2024.
- [CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation](https://arxiv.org/pdf/2409.02098), Sep. 3 2024.
- [Data-Prep-Kit: getting your data ready for LLM application development](https://arxiv.org/pdf/2409.18164), 2024.
- [A Little Human Data Goes A Long Way](https://arxiv.org/pdf/2410.13098), Oct. 17 2024.
- [DEM: Distribution Edited Model for Training with Mixed Data Distributions](https://arxiv.org/pdf/2406.15570), Nov. 5 2024.
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://www.microsoft.com/en-us/research/uploads/prod/2024/07/AgentInstruct.pdf), [blogpost](https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/), Jul. 2024.
- [Not all tokens are what you need for pretraining](https://openreview.net/pdf?id=0NMzBwqaAJ), NeurIPS 2024 best paper runner up. [code](https://github.com/microsoft/rho).
- [GREATS: Online selection of high-quality data for llm training in every iteration](https://openreview.net/pdf/ac1fcb465f6684c753ea5fbbef2742c636cf326a.pdf), NeurIPS 2024.
- [RedStone: Curating general, code, math, and QA data for large language models](https://arxiv.org/pdf/2412.03398), Dec. 4 2024. [code](https://github.com/microsoft/redstone).
- [OpenCSG Chinese corpus: A series of high-quality Chinese datasets for llm training](https://arxiv.org/pdf/2501.08197), Jan. 14 2025.
- [Towards best practices for open datasets for LLM training](https://arxiv.org/pdf/2501.08365), Jan. 2025.
- [Diversity-driven data selection for language model tuning through sparse autoencoder](https://arxiv.org/pdf/2502.14050), Feb. 19 2025.
  - [Position: Measure dataset diversity, don't just claim it](https://arxiv.org/pdf/2407.08188v1), Jul. 11 2024.
- [Organize the web: Constructing domains enhances pre-training data curation](https://arxiv.org/pdf/2502.10341), Feb. 14 2025. [code](https://github.com/CodeCreator/WebOrganizer).
- [Craw4LLM: Efficient web crawling for LLM pretraining](https://arxiv.org/pdf/2502.13347), Feb. 19 2025. [code](https://github.com/cxcscmu/Crawl4LLM).
- [Shall your data strategy work? Perform a swift study](https://arxiv.org/pdf/2502.13514), Feb. 19 2025.
- [Data-efficient pretraining with group-level data influence modeling](https://arxiv.org/pdf/2502.14709), Feb. 20 2025.
- [Large-scale data selection for instruction tuning](https://arxiv.org/pdf/2503.01807), Mar. 3 2025. [code](https://github.com/hamishivi/automated-instruction-selection).
- [Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models](https://arxiv.org/pdf/2502.15950), Feb. 21 2025.
- [Data caricatures: On the representation of African American language in pretraining corpora](https://arxiv.org/pdf/2503.10789), Mar. 13 2025.
- [Navigating rifts in human-LLM grounding: Study and benchmark](https://arxiv.org/pdf/2503.13975), Mar. 18 2025. `dataset analysis`.
- [Optimizing ml training with metagradient descent](https://arxiv.org/pdf/2503.13751), Mar. 17 2025. `data selection`.
  - compared with LESS by Mengzhou. 
- [Predictive dadta selection: The data that predicts is the data that teaches](https://arxiv.org/pdf/2503.00808), Apr. 4 2025. [code](https://github.com/hkust-nlp/preselect).
- [DataDecide: How to predict best pretraining data with small experiments](https://arxiv.org/pdf/2504.11393), Apr. 15 2025. [hf](https://huggingface.co/collections/allenai/datadecide-67edb1d2bacba40b5d3ed633).
- [Position: The most expensive part of an LLM should be its training data](https://arxiv.org/pdf/2504.12427), Apr. 16 2025.
- [On LLMs-driven synthetic data generation, curation and evaluation: A survey](https://arxiv.org/pdf/2406.15126), Jun. 14 2024.
- [Best practices and lessons learned on synthetic data](https://arxiv.org/pdf/2404.07503), Aug. 10 2024.
- [DataRater: Meta-learned dataset curation](https://arxiv.org/abs/2505.17895), May 23 2025.
- [How to get your LLM to generate challenging problems for evaluation](https://arxiv.org/pdf/2502.14678), Feb. 20 2025. [code](https://github.com/McGill-NLP/CHASE).
- [UltraIF: Advancing instruction following from the wild](https://arxiv.org/pdf/2502.04153), Feb. 6 2025. [code](https://github.com/kkk-an/UltraIF).
- [Ultra-FineWeb: Efficient data filtering and verification for high-quality LLM training data](https://arxiv.org/pdf/2505.05427), May 8 2025.
- [Domain2Vec: Vectorizing datasets to find the optimal data mixture without training](https://arxiv.org/pdf/2506.10952), Jun. 12 2025.
- [NICE Data selection for instruction tuning in LLMs with non-differentiable evaluation metric](https://openreview.net/pdf?id=2wt8m5HUBs), ICML 2025.
- [FineWeb2: One pipeline to scale them all - Adapting pre-training data processing to every language](https://arxiv.org/pdf/2506.20920), Jun. 26 2025.
- [RefineX: Learning to refine pre-training data at scale from expert-guided programs](https://arxiv.org/pdf/2507.03253), Jul. 4 2025.
- [Scaling laws for optimal data mixtures](https://arxiv.org/abs/2507.09404), Jul. 12 2025.
- [BeyondWeb Lessions from scaling synthetic data fro trillion-scale pretraining](https://arxiv.org/pdf/2508.10975), Aug. 14 2025.
- [Generative data refinement: Just ask for better data](https://arxiv.org/pdf/2509.08653v1), Sep. 10 2025.


