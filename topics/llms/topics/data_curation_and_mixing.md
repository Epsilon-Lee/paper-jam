
- [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/pdf/2305.10429), Nov. 21 2023.
- [TinyGSM: achieving > 80% on GSM8k with small language models](https://arxiv.org/pdf/2312.09241), Dec. 14 2023.
- [What makes good data for alignment? A comphrehensive study of automatic data selection in instruction tuning](https://arxiv.org/pdf/2312.15685), Apr. 16 2024.
- [MetaMath: Bootstrap your own methematical questions for large language models](https://arxiv.org/pdf/2309.12284), May 3 2024.
- [Instruction Mining: When Data Mining Meets Large Language Model Finetuning](https://arxiv.org/abs/2307.06290), COLM 2024.
- [Data, Data Everywhere: A Guide for Pretraining Dataset Construction](https://www.arxiv.org/pdf/2407.06380), Jul. 8 2024. `data curation`.
  - _"we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain [...] we show how such attribute information can be used to further refine and improve the quality of a pretraining set"_
- [On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](https://arxiv.org/pdf/2406.15126), Jun. 14 2024. `data synthesis`.
- [Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG](https://arxiv.org/abs/2406.13069), Jun. 24 2024.
- [The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale](https://arxiv.org/pdf/2406.17557v1), Jun. 25 2024.
- [Entropy Law: The Story Behind Data Compression and LLM Performance](https://arxiv.org/pdf/2407.06645), Jul. 11 2024. `data selection` `learning dynamics`.
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://arxiv.org/pdf/2407.03502), Jul. 3 2024.
- [How NuminaMath Won the 1st AIMO Progress Prize](https://huggingface.co/blog/winning-aimo-progress-prize), Jul. 11 2024. `blogpost`.
- [TAGCOS: Task-agnostic gradient clustered coreset selection for instruction tuning data](https://arxiv.org/pdf/2407.15235), Jul. 21 2024. [code](https://github.com/2003pro/TAGCOS).
- [Consent in Crisis: The Rapid Decline of the AI Data Commons](https://arxiv.org/pdf/2407.14933), Jul. 20 2024.
- [Open Artificial Knowledge](https://oakdataset.org/), Jul. 19 2024
- [Programming every example: Lifting pretraining data quality like experts at scale](https://arxiv.org/pdf/2409.17115), Sep. 25 2024.
- [CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation](https://arxiv.org/pdf/2409.02098), Sep. 3 2024.
- [Data-Prep-Kit: getting your data ready for LLM application development](https://arxiv.org/pdf/2409.18164), 2024.
- [A Little Human Data Goes A Long Way](https://arxiv.org/pdf/2410.13098), Oct. 17 2024.
- [DEM: Distribution Edited Model for Training with Mixed Data Distributions](https://arxiv.org/pdf/2406.15570), Nov. 5 2024.
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://www.microsoft.com/en-us/research/uploads/prod/2024/07/AgentInstruct.pdf), [blogpost](https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/), Jul. 2024.
- [Not all tokens are what you need for pretraining](https://openreview.net/pdf?id=0NMzBwqaAJ), NeurIPS 2024 best paper runner up. [code](https://github.com/microsoft/rho).
- [GREATS: Online selection of high-quality data for llm training in every iteration](https://openreview.net/pdf/ac1fcb465f6684c753ea5fbbef2742c636cf326a.pdf), NeurIPS 2024.
- [RedStone: Curating general, code, math, and QA data for large language models](https://arxiv.org/pdf/2412.03398), Dec. 4 2024. [code](https://github.com/microsoft/redstone).
- [OpenCSG Chinese corpus: A series of high-quality Chinese datasets for llm training](https://arxiv.org/pdf/2501.08197), Jan. 14 2025.
- [Towards best practices for open datasets for LLM training](https://arxiv.org/pdf/2501.08365), Jan. 2025.
- [Diversity-driven data selection for language model tuning through sparse autoencoder](https://arxiv.org/pdf/2502.14050), Feb. 19 2025.
  - [Position: Measure dataset diversity, don't just claim it](https://arxiv.org/pdf/2407.08188v1), Jul. 11 2024.
- [Organize the web: Constructing domains enhances pre-training data curation](https://arxiv.org/pdf/2502.10341), Feb. 14 2025. [code](https://github.com/CodeCreator/WebOrganizer).
- [Craw4LLM: Efficient web crawling for LLM pretraining](https://arxiv.org/pdf/2502.13347), Feb. 19 2025. [code](https://github.com/cxcscmu/Crawl4LLM).
- [Shall your data strategy work? Perform a swift study](https://arxiv.org/pdf/2502.13514), Feb. 19 2025.
- [Data-efficient pretraining with group-level data influence modeling](https://arxiv.org/pdf/2502.14709), Feb. 20 2025.
- [Large-scale data selection for instruction tuning](https://arxiv.org/pdf/2503.01807), Mar. 3 2025. [code](https://github.com/hamishivi/automated-instruction-selection).
- [Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models](https://arxiv.org/pdf/2502.15950), Feb. 21 2025.
- [Data caricatures: On the representation of African American language in pretraining corpora](https://arxiv.org/pdf/2503.10789), Mar. 13 2025.
- [Navigating rifts in human-LLM grounding: Study and benchmark](https://arxiv.org/pdf/2503.13975), Mar. 18 2025. `dataset analysis`.
- [Optimizing ml training with metagradient descent](https://arxiv.org/pdf/2503.13751), Mar. 17 2025. `data selection`.
  - compared with LESS by Mengzhou. 
- [Predictive dadta selection: The data that predicts is the data that teaches](https://arxiv.org/pdf/2503.00808), Apr. 4 2025. [code](https://github.com/hkust-nlp/preselect).


