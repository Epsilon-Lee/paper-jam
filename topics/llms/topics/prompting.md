
### Prompting techniques

- [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608), Jun. 6 2024. `prompting`.
- [A survey of prompt engineering methods in large language models for different nlp tasks](https://arxiv.org/pdf/2407.12994), Jul. 17 2024.
- [In-context impersonation reveals large language models' strengths and biases](https://arxiv.org/pdf/2305.14930), Nov. 26 2023. [code](https://github.com/ExplainableML/in-context-impersonation).
- [Promptbreeder: Self-referential self-improvement via prompt evolution](https://arxiv.org/pdf/2309.16797), Sep. 28 2023. [code](https://github.com/vaughanlove/PromptBreeder).
- [Rational metareasoning for large language models](https://arxiv.org/pdf/2410.05563), Dec. 21 2024.
- [How susceptible are LLMs to influence in prompts](https://arxiv.org/pdf/2408.11865), Aug. 17 2024.
- [What Did I Do Wrong? Quantifying LLMsâ€™ Sensitivity and Consistency to Prompt Engineering](https://arxiv.org/pdf/2406.12334), Jun. 18 2024. `prompt engineering`.
- [Understanding the Relationship between Prompts and Response Uncertainty in Large Language Models](https://arxiv.org/pdf/2407.14845), Jul. 20 2024. related to `knowledge extraction`.
- [FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers](https://arxiv.org/pdf/2408.04816), Aug. 9 2024. `COLM2024`. [code](https://github.com/jnwilliams/FUSE_prompt_inversion).
- [Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates](https://arxiv.org/pdf/2402.18540), Feb. 28 2024.
- [Does Prompt Formatting Have Any Impact on LLM Performance?](https://arxiv.org/pdf/2411.10541), Nov. 15 2024.
- [Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts](https://arxiv.org/pdf/2409.13728), Oct. 24 2024. [code](https://github.com/meszarosanna/rule_extrapolation).
- [Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery](https://proceedings.neurips.cc/paper_files/paper/2023/file/a00548031e4647b13042c97c922fadf1-Paper-Conference.pdf), NeurIPS 2023.
- [Automatic prompt optimization with "gradient descent" and beam search](https://arxiv.org/pdf/2305.03495), Oct. 19 2023.
- [Prompt optimization with logged bandit data](https://arxiv.org/pdf/2504.02646), Apr. 3 2025.
- [PromptBreeder: Self-referential self-improvement via prompt evolution](https://arxiv.org/pdf/2309.16797), Sep. 28 2025.
- [Accelerating language model workflows with prompt choreography](https://blog.tjbai.com/prompt_choreography.pdf), May 2025. by `Json Eisner`. [code](https://github.com/tjbai/choreo).
- [What makes a good natural language prompt?](https://arxiv.org/pdf/2506.06950), Jun. 7 2025. [code](https://github.com/dxlong2000/NLPromptEval).
- [Evolving prompts in-context: An open-ended, self-replicating perspective](https://arxiv.org/pdf/2506.17930), Jun. 22 2025.
- [Self-supervised prompt optimization](https://arxiv.org/pdf/2502.06855), Feb. 15 2025. [code](https://github.com/FoundationAgents/MetaGPT).
- [GEPA: Reflective prompt evaluation can outperform reinforcement learning](https://arxiv.org/abs/2507.19457), Jul. 2025.
- [Prompt adaptation as a dynamic complement in generative ai systems](https://arxiv.org/pdf/2407.14333v5), Apr. 22 2025.
- [iPrOp: Interactive prompt optimization for large language models with a human in the loop](https://aclanthology.org/2025.acl-srw.18.pdf), ACL 2025.
- [System prompt optimization with meta-learning](https://arxiv.org/pdf/2505.09666), May 14 2025. [code](https://github.com/Dozi01/MetaSPO).
- [DRO-InstructZero: Distributionally robust prompt optimization for large language models](https://arxiv.org/pdf/2510.15260), Oct. 17 2025.
- [No loss, no gain: Gated refinement and adaptive compression for prompt optimization](https://www.arxiv.org/pdf/2509.23387), Sep. 27 2025. [code](https://github.com/Eric8932/GRACE).
- [Prompt-MII: Meta-learning instruction induction for LLMs](https://arxiv.org/pdf/2510.16932), Oct. 19 2025. [code](https://github.com/millix19/promptmii).

#### Theory

- [On the power of context-enhanced learning in LLMs](https://arxiv.org/pdf/2503.01821), Mar. 3 2025.
- [A theoretical framework for prompt engineering: Approximating smooth functions with transformer prompts](https://arxiv.org/pdf/2503.20561), Mar. 26 2025.


