
### Length generalization

- [Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers](https://arxiv.org/pdf/2408.05506), Aug. 10 2024.
- [A formal framework for understanding length generalization in transformers](https://arxiv.org/pdf/2410.02140), Oct. 3 2024.
- [Self-improving transformers overcome easy-to-hard and length generalization challengs](https://arxiv.org/pdf/2502.01612), Feb. 3 2025.
- [NoLiMa: Long-context evaluation beyond literal matching](https://arxiv.org/pdf/2502.05167), Feb. 7 2025.
- [LongPO: Long context self-evolution of large language models through short-to-long preference optimization](https://www.arxiv.org/pdf/2502.13922), ICLR 2025. [tweet](https://x.com/gzchen3/status/1892870929966883236). [code](https://github.com/DAMO-NLP-SG/LongPO).
- [How to train long-context language models (effectively)](https://arxiv.org/pdf/2410.02660), Oct. 3 2024.
- [LIFT: Improving long context understanding of large language models through long input fine-tuning](https://arxiv.org/pdf/2502.14644), Feb. 2025.
- [The Role of Sparsity for Length Generalization in Transformers](https://arxiv.org/pdf/2502.16792), Feb. 24 2025.
- [A survey on transformer context extension: Approaches and evaluation](https://arxiv.org/pdf/2503.13299), Mar. 17 2025.
- [Scaling context, not parameters: Training a compact 7B language model for efficient long-context processing](https://arxiv.org/pdf/2505.08651), Mar. 13 2025. [hf](https://arxiv.org/pdf/2505.08651).
- [A comphrehensive survey on long context language modeling](https://arxiv.org/pdf/2503.17407), Mar. 20 2025.
- [How to train long-context language models effectively](https://arxiv.org/pdf/2410.02660), Apr. 3 2025. [code](https://github.com/princeton-nlp/ProLong).
- [LongProc: Benchmarking long-context language models on long procedural generation](https://arxiv.org/pdf/2501.05414), Apr. 21 2025. [code](https://princeton-pli.github.io/LongProc/).
- [HELMET: How to evaluate long-context language models effectively and thoroughly](https://arxiv.org/pdf/2410.02694?), Mar. 6 2025.
- [Extrapolation by association: Length generalization transfer in transformers](https://arxiv.org/pdf/2506.09251), Jun. 10 2025.
- [Michelangelo: Long context evaluations beyond haystacks via latent structure queries](https://arxiv.org/pdf/2409.12640), Sep. 20 2024.
- [The role of sparsity for length generalization in LLMs](https://openreview.net/pdf?id=S9LkB0UBKb), ICML 2025. `theory`.
- [LLMs now accept longer inputs, and the best models can use them more effectively](https://epoch.ai/data-insights/context-windows), Jun. 25 2025.
- [Lost in the maze: Overcoming context limitations in long-horizon agentic search](https://arxiv.org/pdf/2510.18939), Oct. 21 2025.


