
- [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373), May 31 2023.
- [Sailor: Open Language Models for South-East Asia](https://arxiv.org/pdf/2404.03608), Apr. 4 2024.
- [LokiLM: Technical Report](https://arxiv.org/abs/2407.07370), Jul. 10 2024.
- [H2O-Danube3 Technical Report](https://arxiv.org/pdf/2407.09276), Jul. 12 2024. `made for smartphone`.
- [Falcon2-11b technical report](https://arxiv.org/pdf/2407.14885), Jul. 20 2024.
- [Aquila2 Technical Report](https://arxiv.org/pdf/2408.07410), Aug. 14 2024.
- [Apriel-Nemotron-15B-Thinker](https://arxiv.org/pdf/2508.10948), Aug. 13 2025.
 - To learn about details of mid-training, e.g. how cpt degrades or inreases scores on various benchmarks

**Intern**
- [InternLM2 Technical Report](https://arxiv.org/pdf/2403.17297), Mar. 26 2024.
- [InternVL3.5: Advancing open-source multimodal models in versality, reasoning, and efficiency](https://arxiv.org/pdf/2508.18265), Aug. 2025.

**Cohere**
- [Command A: An enterprise-ready large language model](https://cohere.com/research/papers/command-a-technical-report.pdf), Mar. 27 2025.

**Nemotron**
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990), Feb. 4 2022.
- [Nemotron-4 340B Technical Report](https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf), Jun. 2024.
- [NVIDIA Nemotron Nano 2: An accurate and efficient hybrid Mamba-Transformer reasoning model](https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf), Aug. 18 2025.
  - The partial pre-training data is available, e.g. Nemotron-CC-v2/Math-v1/Code-v1/SFT-v1, as well as the post-training dataset.

**Llama series**
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971), Feb. 27 2023.
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288), Jul. 18 2023.
- [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783), Jul. 2024.

**Claude**
- [Introducing Claude](https://www.anthropic.com/news/introducing-claude), Mar. 14 2023.
- [Model Card and Evaluations for Claude Models](https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf), Jul. 8 2023.
- [The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf), [blogpost](https://www.anthropic.com/news/claude-3-family), Mar. 4 2024.
- [System Card: Claude Opus 4 & Claude Sonnet 4](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf), May 2025.

**GPT series**
- [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774), Mar. 15 2023.
- [GPT-4o System Card](https://arxiv.org/pdf/2410.21276), Aug. 8 2024.
- [OpenAI o3 and o4-mini system card](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf), Apr. 16 2025.
- [gpt-oss-120b & gpt-oss-20b model card](https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf), Aug. 5 2025.

**Phi series**
- [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644), Oct. 2 2023.
- [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463), Sep. 11 2023.
- [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219), Apr. 22 2024.
- [Phi-4 technical report](https://arxiv.org/pdf/2412.08905), Dec. 12 2024.
- [Phi-4-Mini technical report: Compact yet powerful multimodal language models via mixture-of-loras](https://arxiv.org/pdf/2503.01743), Mar. 7 2025.

**Gemini and Gemma**
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446), Jan. 21 2022.
- [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805), Jun. 17 2024.
- [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530), Jun. 14 2024.
- [Gemma 2: Improving Open Language Models at a Practical Size](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf), Jun. 27 2024.
- [Introducing Gemini 2.0: our new AI model for the agentic era](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message).
- [Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context and next generation agentic capabilities](https://arxiv.org/pdf/2507.06261), Jul. 11 2025.

**Deepseek series**
- [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954), Jan. 2024.
- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434), May 7 2024.
- [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/abs/2406.11931), Jun. 17 2024.
- [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/pdf/2408.08152), Aug. 2024.
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437), Dec. 26 2024.

**AI2**
- [Olmoe: Open mixture-of-experts language models](https://arxiv.org/pdf/2409.02060), Sep. 3 2024. [data](https://huggingface.co/datasets/allenai/OLMoE-mix-0924).
- [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838), Feb. 1 2024.
- [OLMo 2: The best fully open language model to date](https://allenai.org/blog/olmo2), Nov. 26 2024.
- [TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/pdf/2411.15124), Dec. 6 2024.
- [2 OLMo 2 Furious](https://arxiv.org/pdf/2501.00656), Dec. 31 2024.

**Qwen series**
- [Qwen Technincal Report](https://arxiv.org/pdf/2309.16609), Sep. 28 2023.
- [Qwen2 technical report](https://arxiv.org/pdf/2407.10671), Jul. 15 2024.
- [Qwen2.5 Technical Report](https://arxiv.org/pdf/2412.15115), Dec. 19 2024.
- [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388), May 15 2025.

**MiniCPM**
- [MiniCPM4: Ultra-efficient LLMs on end devices](https://arxiv.org/pdf/2506.07900), Jun. 9 2025.

**Mistral AI**
- [Magistral](https://arxiv.org/pdf/2506.10910), Jun. 12 2025.

**MiniMax**
- [MiniMax-01: Scaling foundation models with lightning attention](https://arxiv.org/pdf/2501.08313), Jan. 14 2025. [code](https://github.com/MiniMax-AI/MiniMax-01).
- [MiniMax-M1: Scaling test-time compute efficiently with lightning attention](https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf), Jun. 16 2025.

**GLM**
- [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/pdf/2406.12793), Jun. 18 2024.
- [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/pdf/2507.01006), Jul. 2 2025.
- [GLM-4.5: Agentic, reasoning, and coding (ARC) foundation models](https://arxiv.org/pdf/2508.06471), Aug. 8 2025.

**MiMo**
- [MiMo: Unlocking the reasoning potential of language model - From pretraining to posttraining](https://arxiv.org/pdf/2505.07608), Jun. 5 2025.

### Domain LLMs

- [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085), Nov. 16 2022. `science`.
- [HuatuoGPT, towards taming language models to be a doctor](https://arxiv.org/pdf/2305.15075), May 24 2023.
- [InternLM-Law: An Open Source Chinese Legal Large Language Model](https://arxiv.org/pdf/2406.14887), Jun. 21 2024. `domain llms` `post-training`.
- [LiLiuM: eBay's Large Language Models for E-Commerce](https://arxiv.org/pdf/2406.12023), Jun. 17 2024.
- [SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain](https://arxiv.org/pdf/2407.19584), Jul. 29 2024. `legal`.
- [SeaLLMs - Large Language Models for Southeast Asia](https://arxiv.org/pdf/2312.00738), Jul. 1 2024. `multilinguality`.
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](https://arxiv.org/pdf/2407.19672), Jul. 29 2024. `multilinguality`.
- [Med42-v2: A suite of clinical LLMs](https://arxiv.org/pdf/2408.06142), Aug. 12 2024.
- [MedGemma technical report](https://arxiv.org/pdf/2507.05201), Jul. 7 2025.


