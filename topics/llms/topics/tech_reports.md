
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990), Feb. 4 2022.
- [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373), May 31 2023.
- [InternLM2 Technical Report](https://arxiv.org/pdf/2403.17297), Mar. 26 2024.
- [Sailor: Open Language Models for South-East Asia](https://arxiv.org/pdf/2404.03608), Apr. 4 2024.
- [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/pdf/2406.11931), Jun. 17 2024.
- [Nemotron-4 340B Technical Report](https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf), Jun. 2024.
- [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/pdf/2406.12793), Jun. 18 2024.
- [LokiLM: Technical Report](https://arxiv.org/abs/2407.07370), Jul. 10 2024.
- [H2O-Danube3 Technical Report](https://arxiv.org/pdf/2407.09276), Jul. 12 2024. `made for smartphone`.
- [Falcon2-11b technical report](https://arxiv.org/pdf/2407.14885), Jul. 20 2024.
- [Aquila2 Technical Report](https://arxiv.org/pdf/2408.07410), Aug. 14 2024.
- [MiniMax-01: Scaling foundation models with lightning attention](https://arxiv.org/pdf/2501.08313), Jan. 14 2025. [code](https://github.com/MiniMax-AI/MiniMax-01).

**Llama series**

- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971), Feb. 27 2023.
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288), Jul. 18 2023.
- [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783), Jul. 2024.

**Claude**

- [Introducing Claude](https://www.anthropic.com/news/introducing-claude), Mar. 14 2023.
- [Model Card and Evaluations for Claude Models](https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf), Jul. 8 2023.
- [The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf), [blogpost](https://www.anthropic.com/news/claude-3-family), Mar. 4 2024.

**GPT series**

- [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774), Mar. 15 2023.
- [GPT-4o System Card](https://arxiv.org/pdf/2410.21276), Aug. 8 2024.

**Phi series**

- [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644), Oct. 2 2023.
- [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463), Sep. 11 2023.
- [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219), Apr. 22 2024.
- [Phi-4 technical report](https://arxiv.org/pdf/2412.08905), Dec. 12 2024.

**Gemini and Gemma**

- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446), Jan. 21 2022.
- [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805), Jun. 17 2024.
- [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530), Jun. 14 2024.
- [Gemma 2: Improving Open Language Models at a Practical Size](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf), Jun. 27 2024.
- [Introducing Gemini 2.0: our new AI model for the agentic era](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message).

**Deepseek series**

- [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954), Jan. 2024.
- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434), May 7 2024.
- [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/abs/2406.11931), Jun. 17 2024.
- [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/pdf/2408.08152), Aug. 2024.
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437), Dec. 26 2024.

**AI2**

- [Olmoe: Open mixture-of-experts language models](https://arxiv.org/pdf/2409.02060), Sep. 3 2024. [data](https://huggingface.co/datasets/allenai/OLMoE-mix-0924).
- [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838), Feb. 1 2024.
- [OLMo 2: The best fully open language model to date](https://allenai.org/blog/olmo2), Nov. 26 2024.
- [TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/pdf/2411.15124), Dec. 6 2024.
- [2 OLMo 2 Furious](https://arxiv.org/pdf/2501.00656), Dec. 31 2024.

**Qwen series**

- [Qwen Technincal Report](https://arxiv.org/pdf/2309.16609), Sep. 28 2023.
- [Qwen2 technical report](https://arxiv.org/pdf/2407.10671), Jul. 15 2024.
- [Qwen2.5 Technical Report](https://arxiv.org/pdf/2412.15115), Dec. 19 2024.

### Domain LLMs

- [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085), Nov. 16 2022. `science`.
- [HuatuoGPT, towards taming language models to be a doctor](https://arxiv.org/pdf/2305.15075), May 24 2023.
- [InternLM-Law: An Open Source Chinese Legal Large Language Model](https://arxiv.org/pdf/2406.14887), Jun. 21 2024. `domain llms` `post-training`.
- [LiLiuM: eBay's Large Language Models for E-Commerce](https://arxiv.org/pdf/2406.12023), Jun. 17 2024.
- [SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain](https://arxiv.org/pdf/2407.19584), Jul. 29 2024. `legal`.
- [SeaLLMs - Large Language Models for Southeast Asia](https://arxiv.org/pdf/2312.00738), Jul. 1 2024. `multilinguality`.
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](https://arxiv.org/pdf/2407.19672), Jul. 29 2024. `multilinguality`.
- [Med42-v2: A suite of clinical LLMs](https://arxiv.org/pdf/2408.06142), Aug. 12 2024.


