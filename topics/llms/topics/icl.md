
### In-context learning

- [Out-of-distribution generalization via composition: A lens through induction heads in Transformers](https://arxiv.org/abs/2408.09503), Dec. 28 2024.
- [ICLR: In-context learning of representations](https://arxiv.org/pdf/2501.00070), May 2 2025.
- [In-context parameteric inference: Point or distribution estimators?](https://arxiv.org/pdf/2502.11617), Feb. 17 2025.
- [Beyond examples: High-level automated reasoning paradigm in in-context learning via MCTS](https://arxiv.org/pdf/2411.18478), Jun. 2 2025. [code](https://github.com/jinyangwu/HiARICL).
- [ParallelPrompt: Extracting parallelism from large language model queries](https://arxiv.org/pdf/2506.18728), Jun. 26 2025.
- [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/pdf/2505.12075), May 21 2025.
- [Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences](https://arxiv.org/pdf/2509.16189), Sep. 19 2025.
- [Is in-context learning learning?](https://arxiv.org/pdf/2509.10414), Sep. 15 2025.
- [Is in-context learning sufficient for instruction following in LLMs?](https://arxiv.org/pdf/2405.19874), Apr. 18 2025. [code](https://github.com/tml-epfl/icl-alignment).
- [Towards provable emergence of in-context reinforcement learning](https://arxiv.org/pdf/2509.18389), Sep. 22 2025.
- [On the theoretical interpretations of concept-based in-context learning](https://arxiv.org/pdf/2509.20882), Sep. 25 2025.
- [Mechanism of task-oriented information removal in in-context learning](https://arxiv.org/pdf/2509.21012), Sep. 25 2025.
- [Understanding prompt tuning and in-context learning via meta-learning](https://arxiv.org/pdf/2505.17010), May 22 2025.

#### Empirical observation of icl

- [Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning](https://aclanthology.org/2024.findings-emnlp.239.pdf), EMNLP 2024. [code](https://github.com/MikaStars39/ICLvsFinetune).
- [A survey on in-context learning](https://arxiv.org/pdf/2301.00234), Oct. 5 2024.
- [Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation](https://arxiv.org/pdf/2305.16938), May 30 2023. [code](https://github.com/uds-lsv/llmft).
- [Is Mamba Capable of In-Context Learning?](https://arxiv.org/pdf/2402.03170v2), Apr. 24 2024.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `in-context learning`.
- [Many-Shot In-Context Learning](https://arxiv.org/pdf/2404.11018), Oct. 17 2024.
- [Multi-Attribute Constraint Satisfaction via Language Model Rewriting](https://arxiv.org/abs/2412.19198), Dec. 26 2024.
- [Distilled pretraining: A modern lens of data, in-context learning and test-time scaling](https://arxiv.org/pdf/2509.01649), Sep. 1 2025.

#### Theory of icl

- [In-context Learning and Induction Heads](https://arxiv.org/pdf/2209.11895), Mar. 8 2022. `in-context-learning`.
- [The learnability of in-context learning](https://arxiv.org/pdf/2303.07895), Mar. 14 2023.
- [A mechanism for sample-efficient in-context learning for sparse retrieval tasks](https://arxiv.org/pdf/2305.17040), May 26 2023.
- [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://arxiv.org/pdf/2110.10090), Jun. 24 2024. `variable mechanism`.
- [Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs](https://arxiv.org/pdf/2405.11880), May 20 2024. `icl`.
- [MLPs Learn In-Context](https://arxiv.org/pdf/2405.15618), May 24 2024. `icl`.
- [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://arxiv.org/pdf/2405.19874), May 30 2024. `icl`.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `icl`.
- [Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning](https://arxiv.org/pdf/2406.11890), Jun. 14 2024. `icl`.
- [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/pdf/2406.11233), Jun. 17 2024. `icl`.
- [Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning](https://arxiv.org/pdf/2406.14022), Jun. 20 2024. `in-context learning`.
- [Transformers are universal on-context learners](https://arxiv.org/pdf/2408.01367), Aug. 2 2024. `in-context learning`.
- [Memorization in in-context learning](https://arxiv.org/pdf/2408.11546), Aug. 21 2024.
- [Divide, reweight, and conquer: A logit arithmetic approach for in-context learning](https://arxiv.org/pdf/2410.10074), Oct. 14 2024. [code](https://github.com/Chengsong-Huang/LARA).
- [Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models](https://arxiv.org/pdf/2410.09701), Oct. 13 2024.
- [Re-examing learning linear functions in context](https://arxiv.org/pdf/2411.11465), Nov. 18 2024.
- [Task vectors in in-context learning: Emergence, formation, and benefits](https://arxiv.org/pdf/2501.09240), Jan. 16 2025.
- [The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities](https://arxiv.org/pdf/2501.08716), Jan. 15 2025.
- [Rapid word learning through meta in-context learning](https://arxiv.org/pdf/2502.14791), Feb. 20 2024.
- [In-Context Learning with Long-Context Models: An In-Depth Exploration](https://arxiv.org/abs/2405.00200), Apr. 30 2024.
- [Strategy coopetition explains the emergence and transience of in-context learning](https://arxiv.org/pdf/2503.05631), Mar. 7 2025. [code](https://github.com/aadityasingh/icl-dynamics).
- [In-context linear regression demystified: Training dynamics and mechanistic interpretability of multi-head softmax attention](https://arxiv.org/pdf/2503.12734), Mar. 17 2025.
- [Test-time training provably improves transformers as in-context learners](https://arxiv.org/pdf/2503.11842), Mar. 14 2025.
- [Understanding the generalization of in-context learning in transformers: An empirical study](https://arxiv.org/pdf/2503.15579), Mar. 19 2025.
- [Distinct computations emerge from compositional curricula in in-context learning](https://openreview.net/pdf?id=oo5TNikeJl), ICLR 2025.
  - [Curriculum effects and compositionality emerge with in-context learning in neural networks](https://arxiv.org/abs/2402.08674), Feb. 13 2024.
- [RNNs are not Transformers (Yet): The key bottleneck on in-context retrieval](https://arxiv.org/pdf/2402.18510), Dec. 6 2024. [code](https://arxiv.org/pdf/2402.18510).
- [Why in-context learning models are good few-shot learners?](https://openreview.net/forum?id=iLUcsecZJp), Jan. 23 2025. `ICLR 2025`.
- [In-context parametric inference: Point or distribution estimators?](https://arxiv.org/pdf/2502.11617), Feb. 17 2025.
- [Pearl: Towards permutation-resilient LLMs](https://arxiv.org/pdf/2502.14628), Feb. 20 2025.
- [A mechanism for sample-efficient in-context learning for sparse retrieval tasks](https://proceedings.mlr.press/v237/abernethy24a/abernethy24a.pdf), ALT 2024.
- [Brewing knowledge in context: Distillation perspectives on in-context learning](https://arxiv.org/pdf/2506.11516), Jun. 13 2025.
- [In-context learning strategies emerge rationally](https://arxiv.org/abs/2506.17859), Jun. 21 2025. [tweet](https://x.com/EkdeepL/status/1938777753064776060).
- [A theory of emergent in-context learning as implicit structure induction](https://arxiv.org/pdf/2303.07971), Mar. 14 2023.
- [Are emergent abilities in large language models just in-context learning](https://arxiv.org/pdf/2309.01809), Jul. 15 2024.
- [When does divide and conquer work for long context LLM? A noise decomposition framework](https://arxiv.org/pdf/2506.16411), Jun. 19 2025. [code](https://github.com/NehzUx/DivideConquerAgents).
- [In-context Occam's Razor: How transformers prefer simpler hypothesis on the fly](https://arxiv.org/pdf/2506.19351), Jun. 24 2025. [code](https://github.com/puneesh00/ICL-Bayes-Occam).
- [Learning without training: The implicit dynamics of in-context learning](https://arxiv.org/pdf/2507.16003), Jul. 21 2025.
- [What one cannot, two can: Two-layer transformers provably represent induction heads on any-order Markov chains](https://arxiv.org/pdf/2508.07208), Aug. 10 2025. [code](https://anonymous.4open.science/r/markov-llm-depth-icl-63F0/README.md).
- [Task generalization with AutoRegressive compositional structure: Can learning from D tasks generalize to D^T tasks?](https://arxiv.org/pdf/2502.08991),  Feb. 2025.
- [In-context algorithm emulation in fixed-weight transformers](https://arxiv.org/pdf/2508.17550), Aug. 24 2025. [code](https://github.com/MAGICS-LAB/algo_emu).
- [One-layer transformers fail to solve the induction heads task](https://arxiv.org/pdf/2408.14332), Aug. 26 2024.
- [Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods](https://arxiv.org/pdf/2408.14511), Aug. 25 2024.
- [In-Context Learning with Representations: Contextual Generalization of Trained Transformers](https://arxiv.org/pdf/2408.10147), Aug. 19 2024.
- [A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks](https://proceedings.mlr.press/v237/abernethy24a/abernethy24a.pdf), alt 2024.
- [Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers](https://arxiv.org/pdf/2409.17357), Sep. 25 2024.
- [Transformers as Algorithms: Generalization and Stability in In-context Learning](https://proceedings.mlr.press/v202/li23l/li23l.pdf), ICML 2023.
- [ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models](https://arxiv.org/pdf/2405.09220). May 27 2024.
- [Can in-context learning really generalize to out-of-distribution tasks?](https://arxiv.org/pdf/2410.09695), Oct. 13 2024.
- [Towards the effect of examples on in-context learning: A theoretical case study](https://arxiv.org/pdf/2410.09411), Oct. 12 2024.
- [Inference and Verbalization Functions During In-Context Learning](https://arxiv.org/pdf/2410.09349), Oct. 12 2024. [code](https://github.com/JunyiTao/infer-then-verbalize-during-icl).
- [How Transformers Implement Induction Heads: Approximation and Optimization Analysis](https://arxiv.org/pdf/2410.11474), Oct. 16 2024.
- [Context-scaling versus task-scaling in in-context learning](https://arxiv.org/pdf/2410.12783), Oct. 16 2024.
- [A theory of emergent in-context learning as implicit structure induction](https://arxiv.org/pdf/2303.07971), Mar. 14 2024.
- [Can generative ai solve your in-context learning problem? A martingale perspective](https://arxiv.org/pdf/2412.06033), Dec. 8 2024.
- [The dual-route model of induction](https://arxiv.org/pdf/2504.03022), Apr. 3 2025. [code](https://dualroute.baulab.info/).
- [Multi-head transformeres provably learn symbolic multi-step reasoning via gradient descent](https://arxiv.org/pdf/2508.08222), Aug. 11 2025.
- [Induction heads as an essential mechanism for pattern matching in in-context learning](https://arxiv.org/pdf/2407.07011), Apr. 2025. [code](https://arxiv.org/pdf/2407.07011).
- [The emergence of sparse attention: Impact of data distribution and benefits of repetition](https://arxiv.org/pdf/2505.17863), May 23 2025.
- [Theoretical bounds for stable in-context learning](https://arxiv.org/pdf/2509.20677), Sep. 22 2025.





