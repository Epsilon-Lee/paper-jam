
### In-context learning

- [A survey on in-context learning](https://arxiv.org/pdf/2301.00234), Oct. 5 2024.
- [In-context Learning and Induction Heads](https://arxiv.org/pdf/2209.11895), Mar. 8 2022. `in-context-learning`.
- [Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation](https://arxiv.org/pdf/2305.16938), May 30 2023. [code](https://github.com/uds-lsv/llmft).
- [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://arxiv.org/pdf/2110.10090), Jun. 24 2024. `variable mechanism`.
- [Is Mamba Capable of In-Context Learning?](https://arxiv.org/pdf/2402.03170v2), Apr. 24 2024.
- [Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs](https://arxiv.org/pdf/2405.11880), May 20 2024. `icl`.
- [MLPs Learn In-Context](https://arxiv.org/pdf/2405.15618), May 24 2024. `icl`.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `in-context learning`.
- [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://arxiv.org/pdf/2405.19874), May 30 2024. `icl`.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `icl`.
- [Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning](https://arxiv.org/pdf/2406.11890), Jun. 14 2024. `icl`.
- [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/pdf/2406.11233), Jun. 17 2024. `icl`.
- [Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning](https://arxiv.org/pdf/2406.14022), Jun. 20 2024. `in-context learning`.
- [Transformers are universal on-context learners](https://arxiv.org/pdf/2408.01367), Aug. 2 2024. `in-context learning`.
- [Memorization in in-context learning](https://arxiv.org/pdf/2408.11546), Aug. 21 2024.
- [Divide, reweight, and conquer: A logit arithmetic approach for in-context learning](https://arxiv.org/pdf/2410.10074), Oct. 14 2024. [code](https://github.com/Chengsong-Huang/LARA).
- [Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models](https://arxiv.org/pdf/2410.09701), Oct. 13 2024.
- [Re-examing learning linear functions in context](https://arxiv.org/pdf/2411.11465), Nov. 18 2024.
- [Many-Shot In-Context Learning](https://arxiv.org/pdf/2404.11018), Oct. 17 2024.
- [Multi-Attribute Constraint Satisfaction via Language Model Rewriting](https://arxiv.org/abs/2412.19198), Dec. 26 2024.
- [Task vectors in in-context learning: Emergence, formation, and benefits](https://arxiv.org/pdf/2501.09240), Jan. 16 2025.
- [The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities](https://arxiv.org/pdf/2501.08716), Jan. 15 2025.
- [Rapid word learning through meta in-context learning](https://arxiv.org/pdf/2502.14791), Feb. 20 2024.
- [In-Context Learning with Long-Context Models: An In-Depth Exploration](https://arxiv.org/abs/2405.00200), Apr. 30 2024.
- [Strategy coopetition explains the emergence and transience of in-context learning](https://arxiv.org/pdf/2503.05631), Mar. 7 2025. [code](https://github.com/aadityasingh/icl-dynamics).
- [In-context linear regression demystified: Training dynamics and mechanistic interpretability of multi-head softmax attention](https://arxiv.org/pdf/2503.12734), Mar. 17 2025.
- [Test-time training provably improves transformers as in-context learners](https://arxiv.org/pdf/2503.11842), Mar. 14 2025.
- [Understanding the generalization of in-context learning in transformers: An empirical study](https://arxiv.org/pdf/2503.15579), Mar. 19 2025.
- [Distinct computations emerge from compositional curricula in in-context learning](https://openreview.net/pdf?id=oo5TNikeJl), ICLR 2025.
  - [Curriculum effects and compositionality emerge with in-context learning in neural networks](https://arxiv.org/abs/2402.08674), Feb. 13 2024.
- [RNNs are not Transformers (Yet): The key bottleneck on in-context retrieval](https://arxiv.org/pdf/2402.18510), Dec. 6 2024. [code](https://arxiv.org/pdf/2402.18510).
- [Why in-context learning models are good few-shot learners?](https://openreview.net/forum?id=iLUcsecZJp), Jan. 23 2025. `ICLR 2025`.
- [In-context parametric inference: Point or distribution estimators?](https://arxiv.org/pdf/2502.11617), Feb. 17 2025.
- [Pearl: Towards permutation-resilient LLMs](https://arxiv.org/pdf/2502.14628), Feb. 20 2025.
- [A mechanism for sample-efficient in-context learning for sparse retrieval tasks](https://proceedings.mlr.press/v237/abernethy24a/abernethy24a.pdf), ALT 2024.
- [Brewing knowledge in context: Distillation perspectives on in-context learning](https://arxiv.org/pdf/2506.11516), Jun. 13 2025.
- [In-context learning strategies emerge rationally](https://arxiv.org/abs/2506.17859), Jun. 21 2025. [tweet](https://x.com/EkdeepL/status/1938777753064776060).
- [A theory of emergent in-context learning as implicit structure induction](https://arxiv.org/pdf/2303.07971), Mar. 14 2023.
- [Are emergent abilities in large language models just in-context learning](https://arxiv.org/pdf/2309.01809), Jul. 15 2024.
- [When does divide and conquer work for long context LLM? A noise decomposition framework](https://arxiv.org/pdf/2506.16411), Jun. 19 2025. [code](https://github.com/NehzUx/DivideConquerAgents).


