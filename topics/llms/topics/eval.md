
### Evaluation

- [#InsTag: Instruction tagging for analyzing supervised fine-tuning of large language models](https://arxiv.org/pdf/2308.07074), Aug. 15 2023. [code](https://github.com/OFA-Sys/InsTag).
- [Lessons from the Trenches on reproducible evaluation of language models](https://arxiv.org/abs/2405.14782), May 23 2024. `evaluation`.
- [Evaluating language models as risk scores](https://arxiv.org/pdf/2407.14614), Jul. 19 2024.
- [Training on the test task confounds evaluation and emergence](https://arxiv.org/pdf/2407.07890?), Jul. 10 2024. [code](https://github.com/socialfoundations/training-on-the-test-task).
- [Language model developers should report train-test overlap](https://arxiv.org/abs/2410.08385), Oct. 10 2024. [code](https://github.com/stanford-crfm/data-overlap).
- [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/pdf/2411.00640), Nov. 1 2024.
- [Evaluating Generative AI Systems is a Social Science Measurement Challenge](https://arxiv.org/pdf/2411.10939), Nov. 7 2024.
- [Causality can systematically address the monsters under the bench(marks)](https://arxiv.org/pdf/2502.05085), Feb. 7 2025.
- [How contaminated is your benchmark? Quantifying dataset leakage in large language models with kernel divergence](https://arxiv.org/pdf/2502.00678), Feb. 2 2025.
- [EvalTree: Profiling language model weakness via hierarchical capability trees](https://arxiv.org/pdf/2503.08893), Mar. 11 2025. [code](https://github.com/Zhiyuan-Zeng/EvalTree).
- [Reliable and efficient amortized model-based evaluation](https://arxiv.org/pdf/2503.13335), Mar. 17 2025.
- [In-house evaluation is not enough: Towards robust third-party flaw disclosure for general-purpose AI](https://arxiv.org/pdf/2503.16861), Mar. 25 2025.
- [Evaluation framework for AI systems in the wild](https://arxiv.org/pdf/2504.16778), Apr. 2025. `white paper`.
- [The leaderboard illusion](https://arxiv.org/pdf/2504.20879), Apr. 29 2025.
- [Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text](https://www.jair.org/index.php/jair/article/view/13715/26927), [arxiv version](https://arxiv.org/pdf/2202.06935), Feb. 14 2022.
- [Training on the test task confounds evaluation and emergence](https://arxiv.org/pdf/2407.07890), Apr. 21 2025. [code](https://github.com/socialfoundations/training-on-the-test-task).
- [Cost-of-pass: An economic framework for evaluating language models](https://arxiv.org/pdf/2504.13359), Apr. 17 2025. [code](https://github.com/mhamzaerol/Cost-of-Pass).
- [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/pdf/2505.03814), May 2 2025.
- [From rankings to insights: Evaluation should shift focus from leaderboard to feedback](https://arxiv.org/pdf/2505.06698), May 16 2025. [code](https://github.com/liudan193/Feedbacker).
- [Adaptively evaluating models with task elicitation](https://davisrbrown.com/assets/task_elicitation_initial.pdf), Mar. 3 2025.
- [Answer matching outperforms multiple choice for language model evaluation](https://arxiv.org/pdf/2507.02856), Jul. 3 2025. [code](https://github.com/nikhilchandak/answer-matching).
- [Train-before-Test harmonizez language model rankings](https://arxiv.org/pdf/2507.05195), Jul. 7 2025. [code](https://github.com/socialfoundations/lm-harmony).
- [An empirical analysis of uncertainty in large language model evaluations](https://arxiv.org/pdf/2502.10709), Mar. 2 2025. [code](https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty).
- [Contextualized evaluations: Judging language model responses to underspecified queries](https://arxiv.org/abs/2411.07237), Nov. 11 2024. [code](https://github.com/allenai/ContextEval).
- [Signal and noise: A framework for reducing uncertainty in language model evaluation](https://arxiv.org/pdf/2508.13144), Aug. 18 2025. [code](https://github.com/allenai/signal-and-noise).
- [What makes a good reasoning chain? Uncovering structural patterns in long chain-of-thought reasoning](https://arxiv.org/pdf/2505.22148), May 28 2025.
  - The visualization is impressive.
- [Evaluating step-by-step reasoning traces: A survey](https://arxiv.org/pdf/2502.12289), May 24 2025.
- [Scaling up active testing to large language models](https://arxiv.org/pdf/2508.09093), Aug. 12 2025.
- [AutoEval done right: Using synthetic data for model evaluation](https://openreview.net/pdf?id=S8kbmk12Oo), ICML 2025.
- [Improving model evaluation using SMART filtering of benchmark datasets](https://arxiv.org/pdf/2410.20245), Feb. 10 2025. [code](https://github.com/facebookresearch/ResponsibleNLP/tree/main/SMART-Filtering).
- [Chaning answer order can decrease MMLU accuracy](https://arxiv.org/pdf/2406.19470), Nov. 11 2024.
- [SpecEval: Evaluating model adherence to behavior specifications](https://arxiv.org/pdf/2509.02464), Sep. 2 2025. [code](https://github.com/ahmeda14960/specevaldataset).
- [Fluid language model benchmarking](https://openreview.net/pdf?id=mxcCg9YRqj), COLM 2025. [code](https://github.com/allenai/fluid-benchmarking).
- [Rethinking human preference evaluation of LLM rationales](https://arxiv.org/abs/2509.11026), Sep. 14 2025.
  - How to evaluate LLM-generated explanations of their own reasoning traces?
- [Evalet: Evaluating large language models by fragmenting outputs into functions](https://arxiv.org/abs/2509.11206), Sep. 14 2025.
  - _"a novel LLM-based evaluation method that dissects each output into key fragments and interprets the functions of each fragment, where each fragment may serve multiple functions. With functions, we refer to the rhetorical roles or purposes that text fragments serve that are relevant to a given evaluation criterion."_
- [When three experiments are bettern than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias-variance trade-off](https://arxiv.org/pdf/2509.04363), Sep. 4 2025.
- [Look before you leap: Estimating LLM benchmark scores from descriptions](https://arxiv.org/pdf/2509.20645), Sep. 25 2025. [code](https://github.com/JJumSSu/PRECOG).
  - _"estimating a model's score from a redacted task description and intended configuration, with no access to dataset instances"_
- [Readability \neq learnability: Rethinking the role of simplicity in training small language models](https://openreview.net/pdf?id=AFMGbq39bQ), COLM 2025. [data](https://huggingface.co/collections/ivnle/llamatales-6716dad1a3113c4c3ea1038e). [openreview](https://openreview.net/forum?id=AFMGbq39bQ#discussion).
- [Predicting language models' success at zero-shot probabilistic prediction](https://arxiv.org/abs/2509.15356), Sep. 18 2025. [code](https://arxiv.org/abs/2509.15356).
- [Let's measure information step-by-step: LLM-based evaluation beyond vibes](https://arxiv.org/pdf/2508.05469), Aug. 21 2025. [code](https://github.com/zrobertson466920/llm-peer-prediction/tree/main).
- [Holistic Agent Leaderboard: The missing infrastructure for AI agent evaluation](https://arxiv.org/pdf/2510.11977), Oct. 13 2025.
- [LLMs judge themselves: A game-theoretic framework for human-aligned evaluation](https://arxiv.org/pdf/2510.15746), Oct. 17 2025.
- [Stress-testing model specs reveals character differences among language models](https://arxiv.org/pdf/2510.07686?), Oct. 23 2025.
- [Benchmark as microscopes: A call for model metrology](https://arxiv.org/pdf/2407.16711), Jul. 30 2024.
- [Zero-shot benchmarking: A framework for flexible and scalable automatic evaluation of language models](https://openreview.net/pdf?id=WARZwyDf17), COLM 2025. [code](https://github.com/deep-spin/zsb).

#### Data contamination

- [Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation](https://arxiv.org/pdf/2406.14644), Jun. 20 2024.
- [Accuracy is not all you need](https://arxiv.org/pdf/2407.09141), Jul. 12 2024.
  - The paper dubs synthetic data from LLMs as generative teaching.
- [The emperor's new clothes in benchmarking? A rigorous examination of mitigation strategies for LLM benchmark data contamination](https://arxiv.org/pdf/2503.16402), Mar. 20 2025. [code](https://github.com/ASTRAL-Group/BDC_mitigation_assessment).
- [Beyond memorization: Reasoning-driven synthesis as a mitigation strategy against benchmark contamination](https://arxiv.org/pdf/2509.00072), Aug. 26 2025. [code](https://github.com/TerryJCZhang/BeyondMemorization).
- [Beyond the leaderboard: Understanding performance disparities in large language models via model diffing](https://arxiv.org/pdf/2509.18792), Sep. 23 2025.

#### Benchmark

- [From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline](https://arxiv.org/pdf/2406.11939), Jun. 17 2024. `benchmark`.
- [Training on the Test Task Confounds Evaluation and Emergence](https://arxiv.org/pdf/2407.07890), Jul. 10 2024.
  - _"We argue that the seeming superriority of one model family over another may be explained by a different degree of training on the test task."_
- [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/pdf/2403.07008), May 28 2024.
- [Benchmarking Complex Instruction-Following with Multiple Constraints Composition](https://arxiv.org/pdf/2406.14491), Jul. 4 2024. [github](https://github.com/thu-coai/ComplexBench).
- [GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2402.19255), Jul. 2 2024.
- [metabench A Sparse Benchmark to Measure General Ability in Large Language Models](https://arxiv.org/pdf/2407.12844), Jul. 4 2024. [code](https://github.com/adkipnis/metabench).
- [AutoBencher: Creating Salient, Novel, Difficult Datasets for Language Models](https://arxiv.org/abs/2407.08351), Jul. 11 2024.
- [Benchmark agreement testing done right: A guide to llm benchmark evaluation](https://arxiv.org/pdf/2407.13696), Jul. 18 2024.
- [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/pdf/2408.08926), Aug. 15 2024.
- [Benchmarking Large Language Models for Math Reasoning Tasks](https://arxiv.org/pdf/2408.10839), Aug. 20 2024.
- [Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization](https://arxiv.org/pdf/2409.18433), Sep. 27 2024.
- [HARDMath: A benchmark dataset for challenging problems in applied mathematics](https://arxiv.org/pdf/2410.09988), Oct. 13 2024.
- [SuperGPQA: Scaling LLM evaluation across 285 graduate disciplines](https://arxiv.org/pdf/2502.14739), Feb. 20 2025.
- [xBench: Tracking agents productivity scaling with profession-aligned real-world evaluations](https://xbench.org/files/xbench_profession_v2.4.pdf), May 2025.
- [AutoBencher: Towards declarative benchmark construction](https://openreview.net/pdf?id=ymt4crbbXh), ICLR 2025. [code](https://github.com/XiangLi1999/AutoBencher).
- [UQ: Assessing language models on unsolved questions](https://arxiv.org/pdf/2508.17580), Aug. 25 2025.
- [GDPval: Evaluating AI model performance on real-world economically valuable tasks](https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf), Sep. 2025.
- [Can the capability of large language models be described by human ability? A meta study](https://arxiv.org/pdf/2504.12332), Apr. 13 2025.
- [VitaBench: Benchmarking LLM agents with versatile interactive tasks in real-world applications](https://arxiv.org/pdf/2509.26490), Oct. 17 2025. [code](https://vitabench.github.io/).
- [CTIArena: Benchmarking LLM knowledge and reasoning across heterogeneous cyber threat intelligence](https://arxiv.org/pdf/2510.11974), Oct. 13 2025. [code](https://arxiv.org/pdf/2510.11974).
- [Micro Evals: Individual evaluations of agent-generated code. Vulnerabilities are found by the community](https://www.designarena.ai/evals).

#### Evaluation for agents

- [Why do multi-agent LLM systems fail?](https://arxiv.org/pdf/2503.13657), Apr. 22 2025. [code](https://github.com/multi-agent-systems-failure-taxonomy/MAST).

#### Eval toolkit

- [Foundation Model Evaluations Library](https://github.com/aws/fmeval), [paper](https://arxiv.org/pdf/2407.12872), Jul. 15 2024.
- [UltraEval](https://github.com/OpenBMB/UltraEval).
- [simple-evals](https://github.com/openai/simple-evals), OpenAI simple-evals.


