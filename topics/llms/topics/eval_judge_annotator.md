
### Evaluation

- [Lessons from the Trenches on reproducible evaluation of language models](https://arxiv.org/abs/2405.14782), May 23 2024. `evaluation`.
- [Evaluating language models as risk scores](https://arxiv.org/pdf/2407.14614), Jul. 19 2024.
- [Training on the test task confounds evaluation and emergence](https://arxiv.org/pdf/2407.07890?), Jul. 10 2024. [code](https://github.com/socialfoundations/training-on-the-test-task).
- [Language model developers should report train-test overlap](https://arxiv.org/abs/2410.08385), Oct. 10 2024. [code](https://github.com/stanford-crfm/data-overlap).
- [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/pdf/2411.00640), Nov. 1 2024.
- [Evaluating Generative AI Systems is a Social Science Measurement Challenge](https://arxiv.org/pdf/2411.10939), Nov. 7 2024.
- [Causality can systematically address the monsters under the bench(marks)](https://arxiv.org/pdf/2502.05085), Feb. 7 2025.
- [How contaminated is your benchmark? Quantifying dataset leakage in large language models with kernel divergence](https://arxiv.org/pdf/2502.00678), Feb. 2 2025.
- [Reliable and efficient amortized model-based evaluation](https://arxiv.org/pdf/2503.13335), Mar. 17 2025.
- [In-house evaluation is not enough: Towards robust third-party flaw disclosure for general-purpose AI](https://arxiv.org/pdf/2503.16861), Mar. 25 2025.
- [Evaluation framework for AI systems in the wild](https://arxiv.org/pdf/2504.16778), Apr. 2025. `white paper`.
- [The leaderboard illusion](https://arxiv.org/pdf/2504.20879), Apr. 29 2025.
- [Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text](https://www.jair.org/index.php/jair/article/view/13715/26927), [arxiv version](https://arxiv.org/pdf/2202.06935), Feb. 14 2022.
- [Training on the test task confounds evaluation and emergence](https://arxiv.org/pdf/2407.07890), Apr. 21 2025. [code](https://github.com/socialfoundations/training-on-the-test-task).
- [Cost-of-pass: An economic framework for evaluating language models](https://arxiv.org/pdf/2504.13359), Apr. 17 2025. [code](https://github.com/mhamzaerol/Cost-of-Pass).
- [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/pdf/2505.03814), May 2 2025.
- [From rankings to insights: Evaluation should shift focus from leaderboard to feedback](https://arxiv.org/pdf/2505.06698), May 16 2025. [code](https://github.com/liudan193/Feedbacker).
- [Adaptively evaluating models with task elicitation](https://davisrbrown.com/assets/task_elicitation_initial.pdf), Mar. 3 2025.
- [Answer matching outperforms multiple choice for language model evaluation](https://arxiv.org/pdf/2507.02856), Jul. 3 2025. [code](https://github.com/nikhilchandak/answer-matching).
- [Train-before-Test harmonizez language model rankings](https://arxiv.org/pdf/2507.05195), Jul. 7 2025. [code](https://github.com/socialfoundations/lm-harmony).
- [An empirical analysis of uncertainty in large language model evaluations](https://arxiv.org/pdf/2502.10709), Mar. 2 2025. [code](https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty).
- [Contextualized evaluations: Judging language model responses to underspecified queries](https://arxiv.org/abs/2411.07237), Nov. 11 2024. [code](https://github.com/allenai/ContextEval).
- [Signal and noise: A framework for reducing uncertainty in language model evaluation](https://arxiv.org/pdf/2508.13144), Aug. 18 2025. [code](https://github.com/allenai/signal-and-noise).
- [What makes a good reasoning chain? Uncovering structural patterns in long chain-of-thought reasoning](https://arxiv.org/pdf/2505.22148), May 28 2025.
  - The visualization is impressive.
- [Evaluating step-by-step reasoning traces: A survey](https://arxiv.org/pdf/2502.12289), May 24 2025.
- [Scaling up active testing to large language models](https://arxiv.org/pdf/2508.09093), Aug. 12 2025.
- [AutoEval done right: Using synthetic data for model evaluation](https://openreview.net/pdf?id=S8kbmk12Oo), ICML 2025.
- [Improving model evaluation using SMART filtering of benchmark datasets](https://arxiv.org/pdf/2410.20245), Feb. 10 2025. [code](https://github.com/facebookresearch/ResponsibleNLP/tree/main/SMART-Filtering).
- [Chaning answer order can decrease MMLU accuracy](https://arxiv.org/pdf/2406.19470), Nov. 11 2024.
- [SpecEval: Evaluating model adherence to behavior specifications](https://arxiv.org/pdf/2509.02464), Sep. 2 2025. [code](https://github.com/ahmeda14960/specevaldataset).
- [Fluid language model benchmarking](https://openreview.net/pdf?id=mxcCg9YRqj), COLM 2025. [code](https://github.com/allenai/fluid-benchmarking).
- [Rethinking human preference evaluation of LLM rationales](https://arxiv.org/abs/2509.11026), Sep. 14 2025.
  - How to evaluate LLM-generated explanations of their own reasoning traces?
- [Evalet: Evaluating large language models by fragmenting outputs into functions](https://arxiv.org/abs/2509.11206), Sep. 14 2025.
  - _"a novel LLM-based evaluation method that dissects each output into key fragments and interprets the functions of each fragment, where each fragment may serve multiple functions. With functions, we refer to the rhetorical roles or purposes that text fragments serve that are relevant to a given evaluation criterion."_
- [When three experiments are bettern than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias-variance trade-off](https://arxiv.org/pdf/2509.04363), Sep. 4 2025.
- [Look before you leap: Estimating LLM benchmark scores from descriptions](https://arxiv.org/pdf/2509.20645), Sep. 25 2025. [code](https://github.com/JJumSSu/PRECOG).
  - _"estimating a model's score from a redacted task description and intended configuration, with no access to dataset instances"_
- [Readability \neq learnability: Rethinking the role of simplicity in training small language models](https://openreview.net/pdf?id=AFMGbq39bQ), COLM 2025. [data](https://huggingface.co/collections/ivnle/llamatales-6716dad1a3113c4c3ea1038e). [openreview](https://openreview.net/forum?id=AFMGbq39bQ#discussion).
- [Predicting language models' success at zero-shot probabilistic prediction](https://arxiv.org/abs/2509.15356), Sep. 18 2025. [code](https://arxiv.org/abs/2509.15356).
- [Let's measure information step-by-step: LLM-based evaluation beyond vibes](https://arxiv.org/pdf/2508.05469), Aug. 21 2025. [code](https://github.com/zrobertson466920/llm-peer-prediction/tree/main).
- [Holistic Agent Leaderboard: The missing infrastructure for AI agent evaluation](https://arxiv.org/pdf/2510.11977), Oct. 13 2025.
- [LLMs judge themselves: A game-theoretic framework for human-aligned evaluation](https://arxiv.org/pdf/2510.15746), Oct. 17 2025.
- [Stress-testing model specs reveals character differences among language models](https://arxiv.org/pdf/2510.07686?), Oct. 23 2025.
- [Benchmark as microscopes: A call for model metrology](https://arxiv.org/pdf/2407.16711), Jul. 30 2024.
- [Zero-shot benchmarking: A framework for flexible and scalable automatic evaluation of language models](https://openreview.net/pdf?id=WARZwyDf17), COLM 2025. [code](https://github.com/deep-spin/zsb).
- [Structured prompting enables more robust, holistic evaluation of language models](https://arxiv.org/pdf/2511.20836), Nov. 25 2025.
- [How to currectly report LLM-as-a-judge evaluations](https://arxiv.org/pdf/2511.21140), Nov. 26 2025.
- [Fantastic bugs and where to find them in AI benchmarks](https://arxiv.org/pdf/2511.16842), Nov. 20 2025.
- [UK AISI alignment evaluation case-study](https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/6927663ba1576c0a2acb3cba_UK_AISI_Alignment_Testing_Case_Study%20(1)%20(1).pdf), Nov. 2025.
- [Measuring what matters: Construct validity in large language model benchmarks](https://arxiv.org/pdf/2511.04703), Nov. 3 2025.
- [Product evals in three simple steps](https://eugeneyan.com/writing/product-evals/), Nov. 2025.

#### Eval-guided dev

- [#InsTag: Instruction tagging for analyzing supervised fine-tuning of large language models](https://arxiv.org/pdf/2308.07074), Aug. 15 2023. [code](https://github.com/OFA-Sys/InsTag).
- [EvalTree: Profiling language model weakness via hierarchical capability trees](https://arxiv.org/pdf/2503.08893), Mar. 11 2025. [code](https://github.com/Zhiyuan-Zeng/EvalTree).
- [Antislop: A comprehensive framework for identifying and eliminating repetitive patterns in language models](https://arxiv.org/pdf/2510.15061), Oct. 16 2025. [code](https://github.com/sam-paech/auto-antislop).

#### Data contamination

- [Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation](https://arxiv.org/pdf/2406.14644), Jun. 20 2024.
- [Accuracy is not all you need](https://arxiv.org/pdf/2407.09141), Jul. 12 2024.
  - The paper dubs synthetic data from LLMs as generative teaching.
- [The emperor's new clothes in benchmarking? A rigorous examination of mitigation strategies for LLM benchmark data contamination](https://arxiv.org/pdf/2503.16402), Mar. 20 2025. [code](https://github.com/ASTRAL-Group/BDC_mitigation_assessment).
- [Beyond memorization: Reasoning-driven synthesis as a mitigation strategy against benchmark contamination](https://arxiv.org/pdf/2509.00072), Aug. 26 2025. [code](https://github.com/TerryJCZhang/BeyondMemorization).
- [Beyond the leaderboard: Understanding performance disparities in large language models via model diffing](https://arxiv.org/pdf/2509.18792), Sep. 23 2025.

#### Benchmark

- [From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline](https://arxiv.org/pdf/2406.11939), Jun. 17 2024. `benchmark`.
- [Training on the Test Task Confounds Evaluation and Emergence](https://arxiv.org/pdf/2407.07890), Jul. 10 2024.
  - _"We argue that the seeming superriority of one model family over another may be explained by a different degree of training on the test task."_
- [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/pdf/2403.07008), May 28 2024.
- [Benchmarking Complex Instruction-Following with Multiple Constraints Composition](https://arxiv.org/pdf/2406.14491), Jul. 4 2024. [github](https://github.com/thu-coai/ComplexBench).
- [GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2402.19255), Jul. 2 2024.
- [metabench A Sparse Benchmark to Measure General Ability in Large Language Models](https://arxiv.org/pdf/2407.12844), Jul. 4 2024. [code](https://github.com/adkipnis/metabench).
- [AutoBencher: Creating Salient, Novel, Difficult Datasets for Language Models](https://arxiv.org/abs/2407.08351), Jul. 11 2024.
- [Benchmark agreement testing done right: A guide to llm benchmark evaluation](https://arxiv.org/pdf/2407.13696), Jul. 18 2024.
- [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/pdf/2408.08926), Aug. 15 2024.
- [Benchmarking Large Language Models for Math Reasoning Tasks](https://arxiv.org/pdf/2408.10839), Aug. 20 2024.
- [Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization](https://arxiv.org/pdf/2409.18433), Sep. 27 2024.
- [HARDMath: A benchmark dataset for challenging problems in applied mathematics](https://arxiv.org/pdf/2410.09988), Oct. 13 2024.
- [SuperGPQA: Scaling LLM evaluation across 285 graduate disciplines](https://arxiv.org/pdf/2502.14739), Feb. 20 2025.
- [xBench: Tracking agents productivity scaling with profession-aligned real-world evaluations](https://xbench.org/files/xbench_profession_v2.4.pdf), May 2025.
- [AutoBencher: Towards declarative benchmark construction](https://openreview.net/pdf?id=ymt4crbbXh), ICLR 2025. [code](https://github.com/XiangLi1999/AutoBencher).
- [UQ: Assessing language models on unsolved questions](https://arxiv.org/pdf/2508.17580), Aug. 25 2025.
- [GDPval: Evaluating AI model performance on real-world economically valuable tasks](https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf), Sep. 2025.
- [Can the capability of large language models be described by human ability? A meta study](https://arxiv.org/pdf/2504.12332), Apr. 13 2025.
- [VitaBench: Benchmarking LLM agents with versatile interactive tasks in real-world applications](https://arxiv.org/pdf/2509.26490), Oct. 17 2025. [code](https://vitabench.github.io/).
- [CTIArena: Benchmarking LLM knowledge and reasoning across heterogeneous cyber threat intelligence](https://arxiv.org/pdf/2510.11974), Oct. 13 2025. [code](https://arxiv.org/pdf/2510.11974).
- [Micro Evals: Individual evaluations of agent-generated code. Vulnerabilities are found by the community](https://www.designarena.ai/evals).
- [Global PIQA: Evaluating physical commonsense reasoning across 100+ languages and cultures](https://arxiv.org/abs/2510.24081), Oct. 28 2025.
- [OKBench: Democratizing LLM evaluation with fully automated, on-demand, open knowledge benchmarking](https://arxiv.org/pdf/2511.08598), Oct. 31 2025.

#### Evaluation for agents

- [Why do multi-agent LLM systems fail?](https://arxiv.org/pdf/2503.13657), Apr. 22 2025. [code](https://github.com/multi-agent-systems-failure-taxonomy/MAST).

#### Eval toolkit

- [Foundation Model Evaluations Library](https://github.com/aws/fmeval), [paper](https://arxiv.org/pdf/2407.12872), Jul. 15 2024.
- [UltraEval](https://github.com/OpenBMB/UltraEval).
- [simple-evals](https://github.com/openai/simple-evals), OpenAI simple-evals.

---

### LLM as a judge

- [Judging llm-as-a-judge with mt-bench and chatbot](https://arxiv.org/abs/2306.05685), Jun. 9 2023.
- [Benchmarking cognitive biases in large language models as evaluators](https://arxiv.org/pdf/2309.17012), Sep. 25 2024. [code](https://github.com/minnesotanlp/cobbler).
- [Preference leakage: A contamination problem in LLM-as-a-judge](https://arxiv.org/pdf/2502.01534), Feb. 3 2025.
- [Investigating non-transitivity in LLM-as-a-judge](https://arxiv.org/pdf/2502.14074), Feb. 19 2025.
- [Crowd comparative reasoning: Unlocking comprehensive evaluation for LLM-as-a-judge](https://arxiv.org/pdf/2502.12501), Feb. 18 2025.
- [LLM Juries for Evaluation](https://www.comet.com/site/blog/llm-juries-for-evaluation/), `blogpost`.
- [Does context matter? ContextJudgeBench for evaluating LLM-based judges in contextual settings](), Mar. 19 2025. [code](https://github.com/SalesforceAIResearch/ContextualJudgeBench).
- [Fantastic LLMs for preference data annotation and how to (not) find them](https://arxiv.org/pdf/2411.02481v1), Nov. 4 2024.
- [Replacing judges with juries: Evaluting LLM generations with a penel of diverse models](https://arxiv.org/abs/2404.18796), Apr. 29 2024.
- [Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data](https://arxiv.org/pdf/2410.13341), Feb. 11 2025.
  - _"Our main results shows that when the judge is no more accurate than the evaluated model, no debiasing method can decrease the required amount of ground truth labels by more than half."_
- [On scalable oversight with weak LLMs judging strong LLMs](https://proceedings.neurips.cc/paper_files/paper/2024/file/899511e37a8e01e1bd6f6f1d377cc250-Paper-Conference.pdf), NeurIPS 2024.
- [Evaluating language model agency through negotiations](https://arxiv.org/abs/2401.04536), Jan. 9 2024.
- [Replacing judges with juries: Evaluating LLM generations with a panel of diverse models](https://arxiv.org/pdf/2404.18796), May 1 2024.
- [Trust or escalate: LLM judges with provable guarantees for human agreement](https://arxiv.org/pdf/2407.18370), Jul. 25 2024. [code](https://github.com/jaehunjung1/cascaded-selective-evaluation).
- [TRACT: Regression-aware fine-tuning meets chain-of-thought reasoning for LLM-as-a-judge](https://arxiv.org/pdf/2503.04381), Mar. 6 2025. [code](https://github.com/d223302/TRACT).
- [Learning to plan & reason for evaluation with thinking-LLM-as-a-judge](https://arxiv.org/pdf/2501.18099), Jan. 30 2025.
- [An LLM-as-judge won't save the product - fixing your process will](https://eugeneyan.com/writing/eval-process/), 2025.
- [Judging LLMs on a simplex](https://arxiv.org/pdf/2505.21972), May 28 2025.
  - _"These results underscore the importance of taking a more holistic approach to uncertainty quantification when using LLMs as judges."_
- [Feedback friction: LLMs struggle to fully incorporate external feedback](https://arxiv.org/pdf/2506.11930), Jun. 13 2025.
- [Bridging human and LLM judgements: Understanding and narrowing the gap](https://arxiv.org/pdf/2508.12792), Aug. 18 2025.
- [Justice or prejudice? Quantifying biases in LLM-as-a-Judge](https://arxiv.org/pdf/2410.02736), Oct. 4 2024. [code](https://github.com/llm-judge-bias/llm-judge-bias.github.io/).
- [Reverse engineering human preferences with reinforcement learning](https://arxiv.org/abs/2505.15795), May 21 2025. `LLM-as-a-judge`.
- [No free labels: Limitations of LLM-as-a-judge without human grounding](https://arxiv.org/pdf/2503.05061), Mar. 7 2025. [data](https://huggingface.co/collections/kensho/no-free-labels-67ca139c3943728b3be887a6).
- [Large language models are inconsistent and biased evaluators](https://arxiv.org/pdf/2405.01724), May 2 2024.
- [JudgeLM: Fine-tuning large language models are scalable judges](https://arxiv.org/pdf/2310.17631v2), Mar. 1 2025. [code](https://github.com/baaivision/JudgeLM).
- [Analyzing uncertainty of LLM-as-a-judge: Interval evaluations with conformal prediction](https://arxiv.org/abs/2509.18658), Sep. 23 2025. [code](https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge).
- [Towards scalable oversight with collaborative multi-agent debate in error detection](https://arxiv.org/pdf/2510.20963), Oct. 23 2025.
- [AutoRubric-R1V: Rubric-based generative rewards for faithful multimodal reasoning](https://arxiv.org/pdf/2510.14738), Oct. 16 2025.
- [IF-Critic: Towards a fine-grained LLM critic for instruction-following evaluation](https://arxiv.org/pdf/2511.01014), Nov. 2 2025. [code](https://github.com/thu-coai/IF-CRITIC).
- [Foundational automatic evaluatiors: Scaling multi-task generative evaluator training for reasoning-centric domains](https://arxiv.org/pdf/2510.17793), Nov. 19 2025. [code](https://huggingface.co/collections/Salesforce/fare).
- [On evaluating LLM alignment by evaluating LLM as judges](https://arxiv.org/pdf/2511.20604), Nov. 25 2025.
- [J1: Incentivizing thinking in LLM-as-a-judge via reinforcement learning](https://arxiv.org/pdf/2505.10320), Oct. 13 2025.

#### Self-critique, self-verification

- [Uncertainty estimation for language reward models](https://arxiv.org/pdf/2203.07472), Mar. 14 2022.
- [Self-critiquing models for assisting human evaluators](https://arxiv.org/pdf/2206.05802), Jun. 14 2022.
- [Teaching language models to support answers with verified quotes](https://arxiv.org/pdf/2203.11147), Mar. 21 2022.
- [Enabling large language models to generate text with citations](https://arxiv.org/pdf/2305.14627), Oct. 31 2023. [code](https://github.com/princeton-nlp/ALCE).
- [Chain-of-verification reduces hallucination in large language models](https://arxiv.org/pdf/2309.11495), Sep. 25 2023.
- [Self iterative label refinement via robust unlabeled learning](https://arxiv.org/pdf/2502.12565), Feb. 18 2025.

#### Survey papers

- [A survey on LLM-as-a-judge](https://arxiv.org/pdf/2411.15594), Nov. 2024.
- [From generation to judgement: Opportunities and challenges of LLM-as-a-judge](https://arxiv.org/pdf/2411.16594), Nov. 2024.
- [LLMs-as-judges: A comprehensive survey on llm-based evaluation methods](https://arxiv.org/pdf/2412.05579), Dec. 10 2024.
- [Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies](https://arxiv.org/abs/2308.03188), TACL 2024.
- [Verdict: A library for scaling judge-time compute](https://arxiv.org/pdf/2502.18018), Feb. 25 2025.

#### Fine-tuned judge, debate, collaborate

- [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375), Sep. 28 2022.
- [Fine-tuning language models to find agreement among humans with diverse preferences](https://openreview.net/pdf?id=G5ADoRKiTyJ), NeurIPS 2022.
- [The goldilocks of pragmatic understanding: Fine-tuning strategy matters for implicature resolution by LLMs](https://proceedings.neurips.cc/paper_files/paper/2023/file/4241fec6e94221526b0a9b24828bb774-Paper-Conference.pdf), NeurIPS 2023.
- [Training language models to win debates with self-play improves judge accuracy](https://arxiv.org/pdf/2409.16636), Sep. 2024.
- [Melting Pot Context: Charting the future of generalized cooperative intelligence](https://proceedings.neurips.cc/paper_files/paper/2024/file/1d3ea22480873b389a3365d711eb1e91-Paper-Datasets_and_Benchmarks_Track.pdf), NeurIPS 2024.
- [Improving factuality and reasoning in language models through multiagent debate](https://openreview.net/pdf?id=zj7YuTE4t8), ICML 2024. [code](https://composable-models.github.io/llm_debate/).
- [Cooperation, competition, and maliciousness: LLM-stakeholders interactive negotiation](https://proceedings.neurips.cc/paper_files/paper/2024/file/984dd3db213db2d1454a163b65b84d08-Paper-Datasets_and_Benchmarks_Track.pdf), NeurIPS 2024.
- [How well can LLMs negotiate? NegotiationAreana platform and analysis](https://arxiv.org/abs/2402.05863), Feb. 8 2024.
- [Debating with more persuasive LLMs leads to more truthful answers](https://arxiv.org/pdf/2402.06782), Jun. 25 2024.
- [Training language models to win debates with self-play improves judge accuracy](https://arxiv.org/pdf/2409.16636v1), Sep. 25 2024. [code](https://github.com/samuelarnesen/nyu-debate-modeling).
- [LLM-deliberation: Evaluating LLMs with interactive multi-agent negotiation game](https://openreview.net/forum?id=cfL8zApofK), ICLR 2024.
- [Large language model agents can coordinate beyond human scale](https://arxiv.org/pdf/2409.02822), Dec. 22 2024.
- [Evaluating language model agency through negotiations](https://arxiv.org/abs/2401.04536), Jan. 9 2024.
- [Multi-agent consensus seeking via large language models](https://arxiv.org/pdf/2310.20151), Jan. 21 2025.
- [Great models think alike and this undermines AI oversight](https://arxiv.org/pdf/2502.04313), Feb. 6 2025. [code](https://github.com/model-similarity/lm-similarity).
- [AI debate aids assessment of controversial claims](https://arxiv.org/pdf/2506.02175), Jun. 2 2025. [code](https://github.com/salman-lui/ai-debate).
- [An alignment safety case sketch based on debate](https://arxiv.org/pdf/2505.03989?), May 23 2025.

### LLM-as-an-annotator

- [Large language models as annotators: Enhancing generalization of NLP models at minimal cost](https://arxiv.org/pdf/2306.15766), Jun. 27 2023.
- [LLMaAA: Making large language models as active annotators](https://arxiv.org/pdf/2310.19596), Oct. 31 2023. [code](https://github.com/ridiculouz/LLMAAA).
- [Best practices for text annotation with large language models](https://arxiv.org/pdf/2402.05129), Feb. 2024.
- [MEGAnno+: A human-LLM collaborative annotation system](https://arxiv.org/pdf/2402.18050), Feb. 28 2024. [code](https://github.com/megagonlabs/meganno-client).
- [The effectiveness of LLMs as annotators: A comparative overview and empirical analysis of direct representation](https://aclanthology.org/2024.nlperspectives-1.11.pdf), 2024.
- [The promises and pitfalls of LLM annotations in dataset labeling: A case study on media bias detection](https://aclanthology.org/2025.findings-naacl.75.pdf), NAACL 2025. [code](https://github.com/Media-Bias-Group/llm-annotations-annomatic).
- [Can unconfident LLM annotations be used for confident conclusions?](https://arxiv.org/pdf/2408.15204), Feb. 8 2025. [code](https://github.com/kristinagligoric/confidence-driven-inference).
- [Can reasoning help large language models capture human annotator disagreement?](https://arxiv.org/pdf/2506.19467), Aug. 4 2025. [code](https://github.com/EdisonNi-hku/Disagreement_Prediction).
- [Large language model hacking: Quantifying the hidden risks of using LLMs for text annotation](https://arxiv.org/pdf/2509.08825), Sep. 10 2025.
- [Evaluating large language models as expert annotators](https://openreview.net/forum?id=DktAODDdbt#discussion), COLM 2025.




