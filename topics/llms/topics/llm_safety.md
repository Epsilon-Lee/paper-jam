
- [Recipes for safety in open-domain chatbots](https://arxiv.org/abs/2010.07079), Oct. 14 2020.
- [Efficient toxic content detection by boostrapping and distilling large language models](https://arxiv.org/pdf/2312.08303), Dec. 13 2023.
- [Model evaluation for extreme risks](https://arxiv.org/abs/2305.15324), May 24 2023.
- [Are aligned neural networks adversarially aligned](https://arxiv.org/pdf/2306.15447), Jun. 2023.
- [Watching the Ai watchdogs: A fairness and robustness analysis of AI safety moderation classifiers](https://arxiv.org/pdf/2501.13302), Jan. 23 2025.
- [Certified robustness under bounded Levenshtein distance](https://arxiv.org/pdf/2501.13676), Jan. 23 2025.
- [Provably safeguarding a classifier from ood and adversarial samples: An extreme value theory approach](https://arxiv.org/pdf/2501.10202), Jan. 17 2025.
- [On evaluating the durability of safeguards for open-weight LLMs](https://arxiv.org/pdf/2412.07097), Dec. 10 2024.
- [Stree-testing capability elicitation with password-locked models](https://arxiv.org/pdf/2405.19550), May 29 2024.
- [Safety is essential for responsible open-ended systems](https://arxiv.org/pdf/2502.04512), Feb. 10 2025.
- [UniGuardian: A unified defense for detecting prompt injection, backdoor attacks and adversarial attacks in large language models](https://arxiv.org/pdf/2502.13141), Feb. 18 2025.
  - The paper call prompt injection, backdoor attack and adversarial attacks as 'prompt trigger attacks'
- [Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages â€“ A Singlish Case Study](https://arxiv.org/pdf/2502.12485), Feb. 18 2025.
- [How jailbreak defenses work and ensemble? A mechanistic investigation](https://arxiv.org/pdf/2502.14486), Feb. 20 2025.
- [A closer look at system prompt robustness](https://arxiv.org/pdf/2502.12197), Feb. 15 2025. [code](https://github.com/normster/RealGuardrails).
- [Monitoring computer use via hierarchical summarization](https://alignment.anthropic.com/2025/summarization-for-monitoring/), Feb. 27 2025.
- [Emergent misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://martins1612.github.io/emergent_misalignment_betley.pdf), Feb. 2025.

#### Survey and definition

- [A survey of safety and trustworthiness of large language models through the lens of verification and validation](https://arxiv.org/pdf/2305.11391), Aug. 27 2023.
- [A survey on large language model security and privacy: The good, the bad and the ugly](https://arxiv.org/pdf/2312.02003), Mar. 20 2024.
- [Trustworthy LLMs: A survey and guideline for evaluating large language models' alignment](https://openreview.net/pdf?id=oss9uaPFfB), ICLR 2024.
- [Safety at scale: A comprehensive survey of large model safety](https://arxiv.org/pdf/2502.05206), Feb. 2 2025. 

#### Science of llm safety

##### Mechanistic cause

- [How alignment and jailbreak work: Explain llm safety through intermediate hidden states](https://arxiv.org/pdf/2406.05644), Jun. 13 2024.
- [Obfuscated activations: Bypass LLM latent-space defense](https://arxiv.org/abs/2412.09565), [code](https://github.com/LukeBailey181/obfuscated-activations).

#### How to break?

- [Ignore previous prompt: Attach techniques for language models](https://arxiv.org/abs/2211.09527), Nov. 17 2022.
- [Exploiting programmatic behavior of llms: Dual-use through standard security attacks](https://arxiv.org/pdf/2302.05733), Feb. 2023.
- [MasterKey: Automated jailbreaking of large language model chatbots](https://arxiv.org/pdf/2307.08715), Jul. 2023.
- [GPT-4 is too smart to be safe: Stealthy chat with llms via cipher](https://arxiv.org/pdf/2308.06463), Aug. 2023.
- [LoRA fine-tuning efficiently undoes safety training in llama 2-chat 70b](https://arxiv.org/pdf/2310.20624), Oct. 2023.
- [Multilingual jailbreak challengs in large language models](https://arxiv.org/pdf/2310.06474), Oct. 2023.
- [Shadow alignment: The ease of subverting safely-aligned language models](https://arxiv.org/pdf/2310.02949), Oct. 2023.
- [Catastrophic jailbreak of open-source llms via exploiting generation](https://arxiv.org/pdf/2310.06987), Oct. 10 2023.
- [DeepInception: Hypnotize large language model to be jailbreaker](https://arxiv.org/pdf/2311.03191), Nov. 2023.
- [Removing rlhf protections in GPT-4 via fine-tuning](https://arxiv.org/pdf/2311.05553), Nov. 2023.
- [Jailbreak and guard aligned language models with only few in-context demostrations](https://arxiv.org/pdf/2310.06387), Oct. 2024.
- [Jailbreaking LLM's safeguard with universal magic words for text embedding models](https://arxiv.org/pdf/2501.18280), Jan. 30 2025.
- [FlipAttack: Jailbreak llms via flipping](https://arxiv.org/pdf/2410.02832), Oct. 2 2024.
- [Speak Easy: Eliciting harmful jailbreaks from LLMs with simple interactions](https://arxiv.org/pdf/2502.04322), Feb. 6 2025.

#### How to defense

- [Baseline defenses for adversarial attacks against aligned language models](https://arxiv.org/abs/2309.00614v2), Sep. 1 2023.
- [StruQ: Defending against prompt injection with structured queries](https://arxiv.org/abs/2402.06363), Feb. 9 2024.

##### Safety constitution (with reasoning)

- [GuardReasoner: Towards reasoning-based LLM safeguards](https://arxiv.org/pdf/2501.18492), Jan. 30 2025.

#### Truthfulness, hallucination

- [Fine-tuning language models for factuality](https://arxiv.org/abs/2311.08401), Nov. 14 2023.
- [Does fine-tuning LLMs on new knowledge encourage hallucinations?](https://arxiv.org/pdf/2405.05904), May 2024.
- [Language models hallucinate, but may excel at fact verification](https://arxiv.org/abs/2310.14564), Oct. 23 2024.
- [To believe or not to believe your llm](https://arxiv.org/abs/2406.02543), Jun. 4 2024.
- [The beginner's guide to hallucinations in large language models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models), Oct. 20 2023.
- [Hallucination is inevitable: An innate limitation of large language models](https://arxiv.org/pdf/2401.11817), Jan. 22 2024.
- [INSIDE: LLM's interval states retain the power of hallucination detection](https://arxiv.org/pdf/2402.03744), Oct. 21 2024.
- [Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration](https://arxiv.org/pdf/2402.00367), Feb. 2024.
- [Generalizing trust: Weak-to-strong trustworthiness in language models](https://arxiv.org/pdf/2501.00418), Dec. 31 2024.
- [Hallucination, monofacts, and miscalibration: An empirical investigation](https://arxiv.org/pdf/2502.08666), Feb. 11 2025.
- [The law of knowledge overshadowing: Towards understanding, predicting, and preventing LLM hallucination](https://arxiv.org/pdf/2502.16143), Feb. 22 2025.


