
- [Model evaluation for extreme risks](https://arxiv.org/abs/2305.15324), May 24 2023.
- [Are aligned neural networks adversarially aligned](https://arxiv.org/pdf/2306.15447), Jun. 2023.
- [Watching the Ai watchdogs: A fairness and robustness analysis of AI safety moderation classifiers](https://arxiv.org/pdf/2501.13302), Jan. 23 2025.
- [Certified robustness under bounded Levenshtein distance](https://arxiv.org/pdf/2501.13676), Jan. 23 2025.
- [Provably safeguarding a classifier from ood and adversarial samples: An extreme value theory approach](https://arxiv.org/pdf/2501.10202), Jan. 17 2025.
- [On evaluating the durability of safeguards for open-weight LLMs](https://arxiv.org/pdf/2412.07097), Dec. 10 2024.

#### How to break?

- [Exploiting programmatic behavior of llms: Dual-use through standard security attacks](https://arxiv.org/pdf/2302.05733), Feb. 2023.
- [MasterKey: Automated jailbreaking of large language model chatbots](https://arxiv.org/pdf/2307.08715), Jul. 2023.
- [GPT-4 is too smart to be safe: Stealthy chat with llms via cipher](https://arxiv.org/pdf/2308.06463), Aug. 2023.
- [LoRA fine-tuning efficiently undoes safety training in llama 2-chat 70b](https://arxiv.org/pdf/2310.20624), Oct. 2023.
- [Multilingual jailbreak challengs in large language models](https://arxiv.org/pdf/2310.06474), Oct. 2023.
- [Shadow alignment: The ease of subverting safely-aligned language models](https://arxiv.org/pdf/2310.02949), Oct. 2023.
- [Catastrophic jailbreak of open-source llms via exploiting generation](https://arxiv.org/pdf/2310.06987), Oct. 10 2023.
- [DeepInception: Hypnotize large language model to be jailbreaker](https://arxiv.org/pdf/2311.03191), Nov. 2023.
- [Removing rlhf protections in GPT-4 via fine-tuning](https://arxiv.org/pdf/2311.05553), Nov. 2023.
- [Jailbreak and guard aligned language models with only few in-context demostrations](https://arxiv.org/pdf/2310.06387), Oct. 2024.
- [Jailbreaking LLM's safeguard with universal magic words for text embedding models](https://arxiv.org/pdf/2501.18280), Jan. 30 2025.

#### Safety constitution (with reasoning)

- [GuardReasoner: Towards reasoning-based LLM safeguards](https://arxiv.org/pdf/2501.18492), Jan. 30 2025.

#### Truthfulness, hallucination

- [The beginner's guide to hallucinations in large language models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models), Oct. 20 2023.
- [Hallucination is inevitable: An innate limitation of large language models](https://arxiv.org/pdf/2401.11817), Jan. 22 2024.
