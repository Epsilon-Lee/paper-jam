
- [Recipes for safety in open-domain chatbots](https://arxiv.org/abs/2010.07079), Oct. 14 2020.
- [Efficient toxic content detection by boostrapping and distilling large language models](https://arxiv.org/pdf/2312.08303), Dec. 13 2023.
- [Model evaluation for extreme risks](https://arxiv.org/abs/2305.15324), May 24 2023.
- [Are aligned neural networks adversarially aligned](https://arxiv.org/pdf/2306.15447), Jun. 2023.
- [The ethics of advanced AI assistants](https://arxiv.org/pdf/2404.16244), Apr. 19 2024.
- [Watching the Ai watchdogs: A fairness and robustness analysis of AI safety moderation classifiers](https://arxiv.org/pdf/2501.13302), Jan. 23 2025.
- [Certified robustness under bounded Levenshtein distance](https://arxiv.org/pdf/2501.13676), Jan. 23 2025.
- [Provably safeguarding a classifier from ood and adversarial samples: An extreme value theory approach](https://arxiv.org/pdf/2501.10202), Jan. 17 2025.
- [On evaluating the durability of safeguards for open-weight LLMs](https://arxiv.org/pdf/2412.07097), Dec. 10 2024.
- [Stree-testing capability elicitation with password-locked models](https://arxiv.org/pdf/2405.19550), May 29 2024.
- [Safety is essential for responsible open-ended systems](https://arxiv.org/pdf/2502.04512), Feb. 10 2025.
- [UniGuardian: A unified defense for detecting prompt injection, backdoor attacks and adversarial attacks in large language models](https://arxiv.org/pdf/2502.13141), Feb. 18 2025.
  - The paper call prompt injection, backdoor attack and adversarial attacks as 'prompt trigger attacks'
- [Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages â€“ A Singlish Case Study](https://arxiv.org/pdf/2502.12485), Feb. 18 2025.
- [How jailbreak defenses work and ensemble? A mechanistic investigation](https://arxiv.org/pdf/2502.14486), Feb. 20 2025.
- [A closer look at system prompt robustness](https://arxiv.org/pdf/2502.12197), Feb. 15 2025. [code](https://github.com/normster/RealGuardrails).
- [Monitoring computer use via hierarchical summarization](https://alignment.anthropic.com/2025/summarization-for-monitoring/), Feb. 27 2025.
- [Emergent misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://martins1612.github.io/emergent_misalignment_betley.pdf), Feb. 2025.
- [Unnatural languages are not bugs but features for LLMs](https://arxiv.org/pdf/2503.01926), Mar. 2 2025. [code](https://github.com/John-AI-Lab/Unnatural_Language).
- [Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation](https://cdn.openai.com/pdf/34f2ada6-870f-4c26-9790-fd8def56387f/CoT_Monitoring.pdf), Mar. 10 2025. `openai`. [blogpost](https://openai.com/index/chain-of-thought-monitoring/).
- [How we think about safety and alignment](https://openai.com/safety/how-we-think-about-safety-alignment/), Mar. 2025. `openai`.
- [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), Mar. 2025. `lesswrong`.
- [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/pdf/2503.03710), Mar. 5 2025. [code](https://github.com/wicai24/DOOR-Alignment).
- [DarkBench: Benchmarking dark patterns in large language models](https://arxiv.org/pdf/2503.10728), Mar. 13 2025.
- [The case for ensuring that powerful AIs are controlled](https://redwoodresearch.substack.com/p/the-case-for-ensuring-that-powerful), May 2024.
- [AI control: Improving safety despite intentional subversion](https://arxiv.org/abs/2312.06942), Dev. 12 2023.
- [Attacking multimodal OS agents with malicious image patches](https://arxiv.org/pdf/2503.10809), Mar. 13 2025.
- [Auditing language models for hidden objectives](https://arxiv.org/pdf/2503.10965), Mar. 14 2025.
- [Commitments by providers of general-purpose AI models with systemic risk: Safety and security section](https://code-of-practice.ai/?section=summary), Mar. 11 2025.
- [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), Jun. 6 2022.
- [ASIDE: Architectural seperation of instructions and data in language models](https://arxiv.org/pdf/2503.10566), Mar. 13 2025.
- [Aligned probing: Relating toxic behavior and model internals](https://arxiv.org/pdf/2503.13390), Mar. 17 2025. [code](https://github.com/alignedprobing/aligned-probing).
- [H4RM3L: A language for composable jailbreak attack systems](https://arxiv.org/pdf/2408.04811), Mar. 16 2025. [code](https://github.com/mdoumbouya/h4rm3l/).
- [Learning on LLM output signatures for gray-box LLM behavior analysis](https://arxiv.org/pdf/2503.14043), Mar. 18 2025. [code](https://github.com/BarSGuy/LLM-Output-Signatures-Network).
- [Defeating prompt injections by design](https://arxiv.org/pdf/2503.18813), Mar. 24 2025.
- [Automated researchers can subtly sandbag](https://alignment.anthropic.com/2025/automated-researchers-sandbag/), Mar. 24 2025.
- [Language models may verbatim complete text they were not explicitly trained on](https://arxiv.org/pdf/2503.17514), Mar. 25 2025.
- [The backfiring effect of weak AI safety regulation](https://arxiv.org/pdf/2503.20848), Mar. 26 2025.
- [What are they filtering out? A survey of filtering strategies for harm reduction in pretraining datasets](https://arxiv.org/pdf/2503.05721), Feb. 17 2025.
- [An approach to technical AGI safety and security](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf), Apr. 2025. Google DeepMind. [blogpost](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/).
- [Sorry-Bench: Systematically evaluating large language model safety refusal](https://arxiv.org/pdf/2406.14598), Mar. 1 2025.
- [In which areas of technical AI safety could geopolitical rivals cooperate?](https://arxiv.org/pdf/2504.12914), Apr. 17 2025.
- [Open problems in technical AI governance](https://arxiv.org/pdf/2407.14981), Apr. 16 2025.
- [Model organisms for emergent misalignment](https://arxiv.org/pdf/2506.11613), Jun. 13 2025. [code](https://github.com/clarifying-EM/model-organisms-for-EM).
- [Exploring the secondary risks of large language models](https://arxiv.org/pdf/2506.12382), Jun. 14 2025.
- [SHADE-Arena: Evaluating sabotage and monitoring in LLM agents](https://assets.anthropic.com/m/4fb35becb0cd87e1/original/SHADE-Arena-Paper.pdf), 2025. `Anthropic`.
- [LLM hypnosis: Exploiting user feedback for unauthorized knowledge injection to all users](https://arxiv.org/pdf/2507.02850), Jul. 3 2025.
- [Why do some language models fake alignment while others don't?](https://arxiv.org/pdf/2506.18032), Jun. 22 2025.
  - _"only Claude 3 Opus's complilance gap is primarily and consistently explained by alignment faking, and that this alignment faking might be in part motivated by an intrinsic preference for self-preservation. In contrast, most models exhibit minimal alignment faking."_
- [Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs](https://arxiv.org/pdf/2508.06601?), Aug. 8 2025. [code](https://huggingface.co/collections/EleutherAI/deep-ignorance-685441040d024a0fee593d68).
- [Steering MoE LLMs via expert (de)activation](https://www.arxiv.org/pdf/2509.09660), Sep. 11 2025. [code](https://github.com/adobe-research/SteerMoE).
- [DynaGuard: A dynamic guardrail model with user-defined policies](https://arxiv.org/pdf/2509.02563), Sep. 2 2025. [code](https://github.com/montehoover/DynaGuard).
  - _"Our dynamic guardian models can be used for fast detection of policy violations or with chain-of-thought reasoning that articulates and justifies the model outputs"_
- [Persona features control emergent misalignment](https://arxiv.org/pdf/2506.19823), Jun. 24 2025. [code](https://github.com/openai/emergent-misalignment-persona-features).
- [Audits under resource, data, and access constraints: Scaling laws for less discriminatory alternatives](https://arxiv.org/pdf/2509.05627), Sep. 6 2025.
- [Active attacks: Red-teaming LLMs via adaptive environments](https://arxiv.org/pdf/2509.21947), Sep. 26 2025. [code](https://github.com/dbsxodud-11/active_attacks).
- [RL is a hammer and LLMs are nails: A simple reinforcement learning recipe for strong prompt injection](https://arxiv.org/pdf/2510.04885), Oct. 6 2025. [code](https://github.com/facebookresearch/rl-injector).
- [Evaluating & reducing deceptive dialogue from language models with multi-turn rl](https://arxiv.org/pdf/2510.14318), Oct. 16 2025.
  - My two cents: I wonder after multi-turn rl, how much forgetting is there?
- [Open technical problems in open-weight AI model risk management](https://openreview.net/pdf/e6d93dac2c9bfddfd757a6b824b47562de68dc50.pdf), TMLR 2025.
- [Natural emergent misalignment from reward hacking in production rl](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf), Nov. 2025.
- [Poisoning attacks on LLMs require a near-constant number of poison samples](https://arxiv.org/pdf/2510.07192), Oct. 8 2025.
- [Benign samples matter! Fine-tuning on outlier benign samples severely breaks safetly](https://arxiv.org/pdf/2505.06843), May 25 2025. [code](https://github.com/GuanZihan/Benign-Samples-Matter/).
- [Difficulties with evaluating a deception detector for AIs](https://arxiv.org/abs/2511.22662), Nov. 27 2025.

#### Survey and definition

- [A survey of safety and trustworthiness of large language models through the lens of verification and validation](https://arxiv.org/pdf/2305.11391), Aug. 27 2023.
- [A survey on large language model security and privacy: The good, the bad and the ugly](https://arxiv.org/pdf/2312.02003), Mar. 20 2024.
- [Trustworthy LLMs: A survey and guideline for evaluating large language models' alignment](https://openreview.net/pdf?id=oss9uaPFfB), ICLR 2024.
- [Safety at scale: A comprehensive survey of large model safety](https://arxiv.org/pdf/2502.05206), Feb. 2 2025. [code](https://sorry-bench.github.io/).

#### Science of llm safety

##### Mechanistic cause

- [How alignment and jailbreak work: Explain llm safety through intermediate hidden states](https://arxiv.org/pdf/2406.05644), Jun. 13 2024.
- [Obfuscated activations: Bypass LLM latent-space defense](https://arxiv.org/abs/2412.09565), [code](https://github.com/LukeBailey181/obfuscated-activations).
- [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/pdf/2412.09565), Dec. 2024. [code](https://github.com/LukeBailey181/obfuscated-activations).

##### Data-centric view

- [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424v4), Feb. 24 2025.

#### How to break?

- [Ignore previous prompt: Attach techniques for language models](https://arxiv.org/abs/2211.09527), Nov. 17 2022.
- [Exploiting programmatic behavior of llms: Dual-use through standard security attacks](https://arxiv.org/pdf/2302.05733), Feb. 2023.
- [MasterKey: Automated jailbreaking of large language model chatbots](https://arxiv.org/pdf/2307.08715), Jul. 2023.
- [GPT-4 is too smart to be safe: Stealthy chat with llms via cipher](https://arxiv.org/pdf/2308.06463), Aug. 2023.
- [LoRA fine-tuning efficiently undoes safety training in llama 2-chat 70b](https://arxiv.org/pdf/2310.20624), Oct. 2023.
- [Multilingual jailbreak challengs in large language models](https://arxiv.org/pdf/2310.06474), Oct. 2023.
- [Shadow alignment: The ease of subverting safely-aligned language models](https://arxiv.org/pdf/2310.02949), Oct. 2023.
- [Catastrophic jailbreak of open-source llms via exploiting generation](https://arxiv.org/pdf/2310.06987), Oct. 10 2023.
- [DeepInception: Hypnotize large language model to be jailbreaker](https://arxiv.org/pdf/2311.03191), Nov. 2023.
- [Removing rlhf protections in GPT-4 via fine-tuning](https://arxiv.org/pdf/2311.05553), Nov. 2023.
- [Jailbreak and guard aligned language models with only few in-context demostrations](https://arxiv.org/pdf/2310.06387), Oct. 2024.
- [Jailbreaking LLM's safeguard with universal magic words for text embedding models](https://arxiv.org/pdf/2501.18280), Jan. 30 2025.
- [FlipAttack: Jailbreak llms via flipping](https://arxiv.org/pdf/2410.02832), Oct. 2 2024.
- [Speak Easy: Eliciting harmful jailbreaks from LLMs with simple interactions](https://arxiv.org/pdf/2502.04322), Feb. 6 2025.
- [Stronger universal and transferable attacks by suppressing refusals](https://people.eecs.berkeley.edu/~daw/papers/iris-naacl25.pdf), `naacl2025`.
- [Using mechanistic interpretability to craft adversarial attacks against large language models](https://arxiv.org/pdf/2503.06269), Mar. 2025.
- [Monitoring decomposition attacks in LLMs with lightweight sequential monitors](https://arxiv.org/pdf/2506.10949), Jun. 14 2025. [code](https://github.com/YuehHanChen/Monitoring-Decomposition-Attack).
- [Subliminal learning: Language models transmit behavior traits via hidden signals in data](https://arxiv.org/pdf/2507.14805), Jul. 20 2025.

#### How to defense

- [Baseline defenses for adversarial attacks against aligned language models](https://arxiv.org/abs/2309.00614v2), Sep. 1 2023.
- [StruQ: Defending against prompt injection with structured queries](https://arxiv.org/abs/2402.06363), Feb. 9 2024.

##### Safety constitution (with reasoning)

- [GuardReasoner: Towards reasoning-based LLM safeguards](https://arxiv.org/pdf/2501.18492), Jan. 30 2025.
