
- [Recipes for safety in open-domain chatbots](https://arxiv.org/abs/2010.07079), Oct. 14 2020.
- [Efficient toxic content detection by boostrapping and distilling large language models](https://arxiv.org/pdf/2312.08303), Dec. 13 2023.
- [Model evaluation for extreme risks](https://arxiv.org/abs/2305.15324), May 24 2023.
- [Are aligned neural networks adversarially aligned](https://arxiv.org/pdf/2306.15447), Jun. 2023.
- [Watching the Ai watchdogs: A fairness and robustness analysis of AI safety moderation classifiers](https://arxiv.org/pdf/2501.13302), Jan. 23 2025.
- [Certified robustness under bounded Levenshtein distance](https://arxiv.org/pdf/2501.13676), Jan. 23 2025.
- [Provably safeguarding a classifier from ood and adversarial samples: An extreme value theory approach](https://arxiv.org/pdf/2501.10202), Jan. 17 2025.
- [On evaluating the durability of safeguards for open-weight LLMs](https://arxiv.org/pdf/2412.07097), Dec. 10 2024.
- [Stree-testing capability elicitation with password-locked models](https://arxiv.org/pdf/2405.19550), May 29 2024.
- [Safety is essential for responsible open-ended systems](https://arxiv.org/pdf/2502.04512), Feb. 10 2025.
- [UniGuardian: A unified defense for detecting prompt injection, backdoor attacks and adversarial attacks in large language models](https://arxiv.org/pdf/2502.13141), Feb. 18 2025.
  - The paper call prompt injection, backdoor attack and adversarial attacks as 'prompt trigger attacks'
- [Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages â€“ A Singlish Case Study](https://arxiv.org/pdf/2502.12485), Feb. 18 2025.
- [How jailbreak defenses work and ensemble? A mechanistic investigation](https://arxiv.org/pdf/2502.14486), Feb. 20 2025.
- [A closer look at system prompt robustness](https://arxiv.org/pdf/2502.12197), Feb. 15 2025. [code](https://github.com/normster/RealGuardrails).
- [Monitoring computer use via hierarchical summarization](https://alignment.anthropic.com/2025/summarization-for-monitoring/), Feb. 27 2025.
- [Emergent misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://martins1612.github.io/emergent_misalignment_betley.pdf), Feb. 2025.
- [Unnatural languages are not bugs but features for LLMs](https://arxiv.org/pdf/2503.01926), Mar. 2 2025. [code](https://github.com/John-AI-Lab/Unnatural_Language).
- [Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation](https://cdn.openai.com/pdf/34f2ada6-870f-4c26-9790-fd8def56387f/CoT_Monitoring.pdf), Mar. 10 2025. `openai`. [blogpost](https://openai.com/index/chain-of-thought-monitoring/).
- [How we think about safety and alignment](https://openai.com/safety/how-we-think-about-safety-alignment/), Mar. 2025. `openai`.
- [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), Mar. 2025. `lesswrong`.
- [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/pdf/2503.03710), Mar. 5 2025. [code](https://github.com/wicai24/DOOR-Alignment).
- [DarkBench: Benchmarking dark patterns in large language models](https://arxiv.org/pdf/2503.10728), Mar. 13 2025.
- [The case for ensuring that powerful AIs are controlled](https://redwoodresearch.substack.com/p/the-case-for-ensuring-that-powerful), May 2024.
- [AI control: Improving safety despite intentional subversion](https://arxiv.org/abs/2312.06942), Dev. 12 2023.
- [Attacking multimodal OS agents with malicious image patches](https://arxiv.org/pdf/2503.10809), Mar. 13 2025.
- [Auditing language models for hidden objectives](https://arxiv.org/pdf/2503.10965), Mar. 14 2025.
- [Commitments by providers of general-purpose AI models with systemic risk: Safety and security section](https://code-of-practice.ai/?section=summary), Mar. 11 2025.
- [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), Jun. 6 2022.
- [ASIDE: Architectural seperation of instructions and data in language models](https://arxiv.org/pdf/2503.10566), Mar. 13 2025.
- [Aligned probing: Relating toxic behavior and model internals](https://arxiv.org/pdf/2503.13390), Mar. 17 2025. [code](https://github.com/alignedprobing/aligned-probing).
- [H4RM3L: A language for composable jailbreak attack systems](https://arxiv.org/pdf/2408.04811), Mar. 16 2025. [code](https://github.com/mdoumbouya/h4rm3l/).
- [Learning on LLM output signatures for gray-box LLM behavior analysis](https://arxiv.org/pdf/2503.14043), Mar. 18 2025. [code](https://github.com/BarSGuy/LLM-Output-Signatures-Network).
- [Defeating prompt injections by design](https://arxiv.org/pdf/2503.18813), Mar. 24 2025.
- [Automated researchers can subtly sandbag](https://alignment.anthropic.com/2025/automated-researchers-sandbag/), Mar. 24 2025.
- [Language models may verbatim complete text they were not explicitly trained on](https://arxiv.org/pdf/2503.17514), Mar. 25 2025.
- [The backfiring effect of weak AI safety regulation](https://arxiv.org/pdf/2503.20848), Mar. 26 2025.
- [What are they filtering out? A survey of filtering strategies for harm reduction in pretraining datasets](https://arxiv.org/pdf/2503.05721), Feb. 17 2025.
- [An approach to technical AGI safety and security](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf), Apr. 2025. Google DeepMind. [blogpost](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/).
- [Sorry-Bench: Systematically evaluating large language model safety refusal](https://arxiv.org/pdf/2406.14598), Mar. 1 2025.

#### Survey and definition

- [A survey of safety and trustworthiness of large language models through the lens of verification and validation](https://arxiv.org/pdf/2305.11391), Aug. 27 2023.
- [A survey on large language model security and privacy: The good, the bad and the ugly](https://arxiv.org/pdf/2312.02003), Mar. 20 2024.
- [Trustworthy LLMs: A survey and guideline for evaluating large language models' alignment](https://openreview.net/pdf?id=oss9uaPFfB), ICLR 2024.
- [Safety at scale: A comprehensive survey of large model safety](https://arxiv.org/pdf/2502.05206), Feb. 2 2025. [code](https://sorry-bench.github.io/).

#### Science of llm safety

##### Mechanistic cause

- [How alignment and jailbreak work: Explain llm safety through intermediate hidden states](https://arxiv.org/pdf/2406.05644), Jun. 13 2024.
- [Obfuscated activations: Bypass LLM latent-space defense](https://arxiv.org/abs/2412.09565), [code](https://github.com/LukeBailey181/obfuscated-activations).
- [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/pdf/2412.09565), Dec. 2024. [code](https://github.com/LukeBailey181/obfuscated-activations).

##### Data-centric view

- [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424v4), Feb. 24 2025.

#### How to break?

- [Ignore previous prompt: Attach techniques for language models](https://arxiv.org/abs/2211.09527), Nov. 17 2022.
- [Exploiting programmatic behavior of llms: Dual-use through standard security attacks](https://arxiv.org/pdf/2302.05733), Feb. 2023.
- [MasterKey: Automated jailbreaking of large language model chatbots](https://arxiv.org/pdf/2307.08715), Jul. 2023.
- [GPT-4 is too smart to be safe: Stealthy chat with llms via cipher](https://arxiv.org/pdf/2308.06463), Aug. 2023.
- [LoRA fine-tuning efficiently undoes safety training in llama 2-chat 70b](https://arxiv.org/pdf/2310.20624), Oct. 2023.
- [Multilingual jailbreak challengs in large language models](https://arxiv.org/pdf/2310.06474), Oct. 2023.
- [Shadow alignment: The ease of subverting safely-aligned language models](https://arxiv.org/pdf/2310.02949), Oct. 2023.
- [Catastrophic jailbreak of open-source llms via exploiting generation](https://arxiv.org/pdf/2310.06987), Oct. 10 2023.
- [DeepInception: Hypnotize large language model to be jailbreaker](https://arxiv.org/pdf/2311.03191), Nov. 2023.
- [Removing rlhf protections in GPT-4 via fine-tuning](https://arxiv.org/pdf/2311.05553), Nov. 2023.
- [Jailbreak and guard aligned language models with only few in-context demostrations](https://arxiv.org/pdf/2310.06387), Oct. 2024.
- [Jailbreaking LLM's safeguard with universal magic words for text embedding models](https://arxiv.org/pdf/2501.18280), Jan. 30 2025.
- [FlipAttack: Jailbreak llms via flipping](https://arxiv.org/pdf/2410.02832), Oct. 2 2024.
- [Speak Easy: Eliciting harmful jailbreaks from LLMs with simple interactions](https://arxiv.org/pdf/2502.04322), Feb. 6 2025.
- [Stronger universal and transferable attacks by suppressing refusals](https://people.eecs.berkeley.edu/~daw/papers/iris-naacl25.pdf), `naacl2025`.
- [Using mechanistic interpretability to craft adversarial attacks against large language models](https://arxiv.org/pdf/2503.06269), Mar. 2025.

#### How to defense

- [Baseline defenses for adversarial attacks against aligned language models](https://arxiv.org/abs/2309.00614v2), Sep. 1 2023.
- [StruQ: Defending against prompt injection with structured queries](https://arxiv.org/abs/2402.06363), Feb. 9 2024.

##### Safety constitution (with reasoning)

- [GuardReasoner: Towards reasoning-based LLM safeguards](https://arxiv.org/pdf/2501.18492), Jan. 30 2025.

#### Truthfulness, hallucination

- [Belief in the machine: Investigating epistemological blind spots of language models](https://arxiv.org/pdf/2410.21195), Oct. 28 2024.
- [Fine-tuning language models for factuality](https://arxiv.org/abs/2311.08401), Nov. 14 2023.
- [Does fine-tuning LLMs on new knowledge encourage hallucinations?](https://arxiv.org/pdf/2405.05904), May 2024.
- [Language models hallucinate, but may excel at fact verification](https://arxiv.org/abs/2310.14564), Oct. 23 2024.
- [To believe or not to believe your llm](https://arxiv.org/abs/2406.02543), Jun. 4 2024.
- [The beginner's guide to hallucinations in large language models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models), Oct. 20 2023.
- [Hallucination is inevitable: An innate limitation of large language models](https://arxiv.org/pdf/2401.11817), Jan. 22 2024.
- [INSIDE: LLM's interval states retain the power of hallucination detection](https://arxiv.org/pdf/2402.03744), Oct. 21 2024.
- [Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration](https://arxiv.org/pdf/2402.00367), Feb. 2024.
- [Generalizing trust: Weak-to-strong trustworthiness in language models](https://arxiv.org/pdf/2501.00418), Dec. 31 2024.
- [Hallucination, monofacts, and miscalibration: An empirical investigation](https://arxiv.org/pdf/2502.08666), Feb. 11 2025.
- [The law of knowledge overshadowing: Towards understanding, predicting, and preventing LLM hallucination](https://arxiv.org/pdf/2502.16143), Feb. 22 2025.


