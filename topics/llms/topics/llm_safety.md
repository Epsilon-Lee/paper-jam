
- [Recipes for safety in open-domain chatbots](https://arxiv.org/abs/2010.07079), Oct. 14 2020.
- [Efficient toxic content detection by boostrapping and distilling large language models](https://arxiv.org/pdf/2312.08303), Dec. 13 2023.
- [Model evaluation for extreme risks](https://arxiv.org/abs/2305.15324), May 24 2023.
- [Are aligned neural networks adversarially aligned](https://arxiv.org/pdf/2306.15447), Jun. 2023.
- [Watching the Ai watchdogs: A fairness and robustness analysis of AI safety moderation classifiers](https://arxiv.org/pdf/2501.13302), Jan. 23 2025.
- [Certified robustness under bounded Levenshtein distance](https://arxiv.org/pdf/2501.13676), Jan. 23 2025.
- [Provably safeguarding a classifier from ood and adversarial samples: An extreme value theory approach](https://arxiv.org/pdf/2501.10202), Jan. 17 2025.
- [On evaluating the durability of safeguards for open-weight LLMs](https://arxiv.org/pdf/2412.07097), Dec. 10 2024.
- [Stree-testing capability elicitation with password-locked models](https://arxiv.org/pdf/2405.19550), May 29 2024.

#### Survey and definition

- [A survey of safety and trustworthiness of large language models through the lens of verification and validation](https://arxiv.org/pdf/2305.11391), Aug. 27 2023.
- [A survey on large language model security and privacy: The good, the bad and the ugly](https://arxiv.org/pdf/2312.02003), Mar. 20 2024.
- [Trustworthy LLMs: A survey and guideline for evaluating large language models' alignment](https://openreview.net/pdf?id=oss9uaPFfB), ICLR 2024.

#### Science of llm safety

##### Mechanistic cause

- [How alignment and jailbreak work: Explain llm safety through intermediate hidden states](https://arxiv.org/pdf/2406.05644), Jun. 13 2024.

#### How to break?

- [Ignore previous prompt: Attach techniques for language models](https://arxiv.org/abs/2211.09527), Nov. 17 2022.
- [Exploiting programmatic behavior of llms: Dual-use through standard security attacks](https://arxiv.org/pdf/2302.05733), Feb. 2023.
- [MasterKey: Automated jailbreaking of large language model chatbots](https://arxiv.org/pdf/2307.08715), Jul. 2023.
- [GPT-4 is too smart to be safe: Stealthy chat with llms via cipher](https://arxiv.org/pdf/2308.06463), Aug. 2023.
- [LoRA fine-tuning efficiently undoes safety training in llama 2-chat 70b](https://arxiv.org/pdf/2310.20624), Oct. 2023.
- [Multilingual jailbreak challengs in large language models](https://arxiv.org/pdf/2310.06474), Oct. 2023.
- [Shadow alignment: The ease of subverting safely-aligned language models](https://arxiv.org/pdf/2310.02949), Oct. 2023.
- [Catastrophic jailbreak of open-source llms via exploiting generation](https://arxiv.org/pdf/2310.06987), Oct. 10 2023.
- [DeepInception: Hypnotize large language model to be jailbreaker](https://arxiv.org/pdf/2311.03191), Nov. 2023.
- [Removing rlhf protections in GPT-4 via fine-tuning](https://arxiv.org/pdf/2311.05553), Nov. 2023.
- [Jailbreak and guard aligned language models with only few in-context demostrations](https://arxiv.org/pdf/2310.06387), Oct. 2024.
- [Jailbreaking LLM's safeguard with universal magic words for text embedding models](https://arxiv.org/pdf/2501.18280), Jan. 30 2025.
- [FlipAttack: Jailbreak llms via flipping](https://arxiv.org/pdf/2410.02832), Oct. 2 2024.

#### Safety constitution (with reasoning)

- [GuardReasoner: Towards reasoning-based LLM safeguards](https://arxiv.org/pdf/2501.18492), Jan. 30 2025.

#### Truthfulness, hallucination

- [Fine-tuning language models for factuality](https://arxiv.org/abs/2311.08401), Nov. 14 2023.
- [Language models hallucinate, but may excel at fact verification](https://arxiv.org/abs/2310.14564), Oct. 23 2024.
- [To believe or not to believe your llm](https://arxiv.org/abs/2406.02543), Jun. 4 2024.
- [The beginner's guide to hallucinations in large language models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models), Oct. 20 2023.
- [Hallucination is inevitable: An innate limitation of large language models](https://arxiv.org/pdf/2401.11817), Jan. 22 2024.
- [INSIDE: LLM's interval states retain the power of hallucination detection](https://arxiv.org/pdf/2402.03744), Oct. 21 2024.
- [Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration](https://arxiv.org/pdf/2402.00367), Feb. 2024.
- [Generalizing trust: Weak-to-strong trustworthiness in language models](https://arxiv.org/pdf/2501.00418), Dec. 31 2024.


