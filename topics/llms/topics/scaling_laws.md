
> The science of predictability of LLM ability while scaling.

### Scaling laws

- [Predicting emergent abilities with infinite resolution evaluation](https://arxiv.org/pdf/2310.03262), Apr. 17 2024.
- [gzip predicts data-dependent scaling laws](https://arxiv.org/pdf/2405.16684), May 26 2024.
- [Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?](https://arxiv.org/pdf/2406.04391), Jun. 6 2024. `scaling law`.
- [Will we run out of data? Limits of LLM scaling based on human-generated data](https://arxiv.org/pdf/2211.04325), Jun. 4 2024. `data scaling`.
- [Position: Will we run out of data? Limits of LLM scaling based on human-generated data](https://openreview.net/pdf?id=ViZcgDQjyG), ICML 2024. `scaling law`.
- [Scaling laws with vocabulary: Larger models deserve larger vocabularies](https://arxiv.org/pdf/2407.13623), Jul. 18 2024.
  - _"we predict that th optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary size 32K."_
- [Scaling retrieval-based language models with a trillion-token datastore](https://arxiv.org/pdf/2407.12854), Jul. 9 2024.
- [Scaling Training Data with Lossy Image Compression](https://arxiv.org/pdf/2407.17954), Jul. 25 2024
- [AutoScale - Automatic prediction of compute-optimal data composition for training LLMs](https://arxiv.org/pdf/2407.20177), Jul. 29 2024.
- [Scaling law with learning rate annealing](https://arxiv.org/pdf/2408.11029), Aug. 20 2024.
- [Performance Law of Large Language Models](https://arxiv.org/pdf/2408.09895), Aug. 23 2024.
- [Exploring Scaling Laws for Local SGD in Large Language Model Training](https://arxiv.org/pdf/2409.13198), Sep. 20 2024.
- [Small-scale proxies for large-scale Transformer training instabilities](https://arxiv.org/pdf/2309.14322), Oct. 16 2023.
- [Rethinking conventional wisdom in machine learning: From generalization to scaling](https://arxiv.org/abs/2409.15156), Sep. 23 2024.
- [Scaling FP8 training to trillion-token llms](https://arxiv.org/pdf/2409.12517), Sep. 19 2024.
- [How feature learning can improve neural scaling laws](https://arxiv.org/pdf/2409.17858), Sep. 26 2024.
- [Scaling laws for multilingual language models](https://arxiv.org/pdf/2410.12883), Oct. 15 2024.
- [Adaptive data optimization: Dynamic sample selection with scaling laws](https://arxiv.org/pdf/2410.11820), Oct. 15 2024. [code](https://github.com/yidingjiang/ado).
- [A hitchhiker's guide to scaling law estimation](https://arxiv.org/pdf/2410.11840), Oct. 15 2024.
- [Towards neural scaling laws for time series foundation models](https://arxiv.org/pdf/2410.12360), Oct. 16 2024.
- [Scaling Laws for Precision](https://arxiv.org/pdf/2411.04330), Nov. 7 2024.
- [Warmstarting for Scaling Language Models](https://arxiv.org/pdf/2411.07340), Nov. 11 2024.
- [Scaling laws in jet classification](https://arxiv.org/pdf/2312.02264), Dec. 4 2023.
- [Scaling Laws and Interpretability of Learning from Repeated Data](https://arxiv.org/pdf/2205.10487), May 21 2022.
- [Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining](https://arxiv.org/pdf/2405.14908), May 23 2024.
- [Scaling Law for Time Series Forecasting](https://arxiv.org/pdf/2405.15124), May 24 2024.
- [Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling](https://arxiv.org/pdf/2405.14578v1), May 23 2024.
- [Gemstones: A model suite for multi-faceted scaling laws](https://arxiv.org/pdf/2502.06857), Feb. 7 2025. [code](https://github.com/mcleish7/gemstone-scaling-laws).
- [Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient](https://arxiv.org/pdf/2502.05172), Feb. 7 2025.
- [A multi-power law for loss curve prediction across learning rate schedules](https://arxiv.org/pdf/2503.12811), Mar. 17 2025. [code](https://github.com/thu-yao-01-luo/MultiPowerLaw).
- [Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models](https://arxiv.org/pdf/2501.12370), Jan. 25 2025.
- [(Mis)Fitting: A survey of scaling laws](https://arxiv.org/pdf/2502.18969), Feb. 26 2025.
- [Farseer: A refined scaling law in large language models](https://arxiv.org/pdf/2506.10972), Jun. 12 2025. [code](https://github.com/Farseer-Scaling-Law/Farseer).
- [Observational scaling laws and the predictability of language model performance](https://arxiv.org/pdf/2405.10938), Oct. 1 2024. [code](https://github.com/ryoungj/ObsScaling).
- [Loss-to-loss prediction: Scaling laws for all datasets](https://arxiv.org/pdf/2411.12925), Nov. 19 2024.
- [LLMs on the line: Data determines loss-to-loss scaling laws](https://arxiv.org/pdf/2502.12120), Jun. 6 2025.
- [Revisiting scaling laws via the z-transform](https://francisbach.com/z-transform/), Jul. 18 2025.
- [Diffusion beats autoregressive in data-constrained settings](https://arxiv.org/pdf/2507.15857), Jul. 31 2025.
- [Revisiting scaling laws for language models: The role of data quality and training strategies](https://aclanthology.org/2025.acl-long.1163.pdf), ACL 2025.
- [Pre-training under infinit compute](https://arxiv.org/pdf/2509.14786), Sep. 18 2025. [code](https://github.com/marin-community/marin/tree/suhas/data-efficiency).
- [Unveiling the role of learning rate schedules via functional scaling laws](https://arxiv.org/pdf/2509.19189), Sep. 24 2025.
  - _"We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as i) higher-capacity models are more data and compute-efficient, ii) learning rate decay can improve training efficiency, iii) WSD-like schedules can outperform direct-decay schedules"_
- [How to inject knowledge efficiently? Knowledge infusion scaling law for pre-training large language models](https://arxiv.org/pdf/2509.19371), Sep. 19 2025.
- [Towards greater leverage: Scaling laws for efficient mixture-of-experts language models](https://arxiv.org/pdf/2507.17702), Aug. 11 2025.
- [Establishing task scaling laws via compute-efficient model ladders](https://arxiv.org/pdf/2412.04403), Aug. 22 2025.

#### FLOPS

- [First-principles on AI scaling](https://dynomight.net/scaling/), Jul. 2023.
- [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/), Mar. 30 2022.
- [The FLOPs Calculus of Language Model Training](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4), Jan. 10 2022.
- [Transformer FLOPs](https://www.adamcasson.com/posts/transformer-flops), May 16 2023.

#### Theory of scaling laws

- [Scaling Laws from the Data Manifold Dimension](https://jmlr.csail.mit.edu/papers/volume23/20-1111/20-1111.pdf), JMLR 2022.
- [A Solvable Model of Neural Scaling Laws](https://arxiv.org/pdf/2210.16859), Oct. 30 2022.
- [Neural Scaling Laws From Large-N Field Theory: Solvable Model Beyond the Ridgeless Limit](https://arxiv.org/pdf/2405.19398), May 29 2024.
- [4+3 Phases of Compute-Optimal Neural Scaling Laws](https://arxiv.org/pdf/2405.15074), May 23 2024.
- [Scaling Laws in Linear Regression: Compute, Parameters, and Data](https://arxiv.org/abs/2406.08466), Jun. 12 2024. `theory` `scaling law`.


