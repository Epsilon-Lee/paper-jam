
> How to enhance the reasoning ability so as to largely improve intelligence of LLMs.

### Reasoning

- [How truncating weights improves reasoning in language models](https://arxiv.org/pdf/2406.03068), Jun. 5 2024. `reasoning`.
- [From explicit cot to implicit cot: Learning to internalize cot step by step](https://arxiv.org/pdf/2405.14838), May 23 2024. `reasoning`.
- [Towards understanding how transformer perform multi-step reasoning with matching operation](https://arxiv.org/pdf/2405.15302), May 24 2024. `reasoning`.
- [Learning beyond pattern matching? Assaying mathematical understanding in LLMs](https://arxiv.org/pdf/2405.15485), May 24 2024. `reasoning`.
- [STaR- Self-Taught Reasoner: Boostrapping reasoning with reasoning](https://arxiv.org/pdf/2203.14465), May 20 2022.
- [Reflexion: Language agents with verbal reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf), NeurIPS 20224.
- [Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks](https://arxiv.org/pdf/2211.12588), Oct. 23 2023. [code](https://github.com/TIGER-AI-Lab/Program-of-Thoughts).
- [How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad](https://arxiv.org/pdf/2406.06467), Jun. 10 2024. `reasoning`.
- [System-1.x: Learning to Balance Fast and Slow Planning with Language Models](https://arxiv.org/pdf/2407.14414), Jul. 19 2024.
- [Does reasoning emerge? Examing the probabilities of causation in large language models](https://arxiv.org/pdf/2408.08210), Aug. 15 2024.
  - _"to what extent do LLMs perform actual reasoning"_
  - two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS)
  - Use PN and PS to assess LLMs.
  - [Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model?](https://arxiv.org/abs/2504.13837), NeurIPS 2025 oral. [code](https://github.com/LeapLabTHU/limit-of-RLVR).
- [Can Large Language Models Understand Symbolic Graphics Programs?](https://arxiv.org/pdf/2408.08313), Aug. 15 2024.
- [To cot or not to cot? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/pdf/2409.12183), Sep. 18 2024.
- [Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://arxiv.org/pdf/2408.16737), Aug. 29 2024.
- [How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs](https://arxiv.org/pdf/2410.13857), Oct. 17 2024.
- [MathGAP: Out-of-distribution evaluation on problems with arbitrarily complex proofs](https://arxiv.org/pdf/2410.13502), Oct. 17 2024.
- [awesome-o1](https://github.com/srush/awesome-o1/).
- [LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench](https://arxiv.org/abs/2409.13373), Sep 20 2024.
- [Sampling Language from Latent System 2 Reasoning](https://openreview.net/pdf?id=OdUqJwu0Gr), NeurIPS 2024.
- [Thinking LLMs: General instruction following with thought generation](https://arxiv.org/pdf/2410.10630), Oct. 14 2024.
- [Building math agents with multi-turn iterative preference learning](https://arxiv.org/pdf/2409.02392), Sep. 4 2024.
- [Can large language models act as symbolic reasoners?](https://arxiv.org/pdf/2410.21490), Oct. 30 2024.
- [Mixture of parrots: Experts improve memorization more than reasoning](https://arxiv.org/pdf/2410.19034), Oct. 24 2024.
- [The surprising effectiveness of test-time training for abstract reasoning](https://arxiv.org/pdf/2411.07279), Nov. 11 2024.
- [Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level](https://arxiv.org/pdf/2411.03562), Nov. 5 2024.
- [Procedural knowledge in pretraining drives reasoning in large language models](https://arxiv.org/pdf/2411.12580), Nov. 19 2024.
- [Understanding chain-of-thought in LLMs through information theory](https://arxiv.org/pdf/2411.11984), Nov. 18 2024.
- [Out-of-Distribution Generalization as Reasoning: Are LLMs Competitive?](https://www.youtube.com/watch?v=rvLUo0xiSxg), Simons Institute talk by Les Viliant.
  - [Robust logics](https://dl.acm.org/doi/pdf/10.1145/301250.301425), STOC 1999.
  - [Knowledge infusion](https://cdn.aaai.org/AAAI/2006/AAAI06-247.pdf), AAAI 2006.
- [Embedding trajectory for out-of-distribution detection in mathematical reasoning](https://arxiv.org/pdf/2405.14039), Oct. 30 2024. [code](https://github.com/Alsace08/OOD-Math-Reasoning).
- [LLMs Do Not Think Step-by-step In Implicit Reasoning](https://arxiv.org/pdf/2411.15862), Nov. 24 2024.
  - _"We probe the info. of intermediate steps from the model's hidden states when it is performing implicit CoT. indicate that LLMs hardly think about intermediate steps, suggesting they may just rely on experience rather than strict step-by-step reasoning"_
- [Let's think var-by-var: Large language models enable ad hoc probabilistic reasoning](https://arxiv.org/pdf/2412.02081), Dec. 3 2024.
- [Artificial expert intelligence through pac-reasoning](https://arxiv.org/pdf/2412.02441), Dec. 3 2024.
- [Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding](https://arxiv.org/abs/2411.04282), Nov. 6 2024.
- [Formal Mathematical Reasoning: A New Frontier in AI](https://arxiv.org/pdf/2412.16075), Dec. 20 2024.
- [Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs](https://arxiv.org/pdf/2405.15485), ICML 2024.
- [Tree of Problems: Improving structured problem solving with compositionality](https://arxiv.org/pdf/2410.06634), Oct. 9 2024.
- [CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis](https://arxiv.org/pdf/2501.01668), Jan. 3 2025. [code](https://github.com/RUCKBReasoning/CoT-based-Synthesizer).
  - _"leverages CoT reasoning to synthesize superior answers by analyzing complementary info from multi candidate responses"_
- [Free process rewards without process labels](https://arxiv.org/pdf/2412.01981), Dec. 2 2024. [code](https://github.com/PRIME-RL/ImplicitPRM).
- [PRIME](https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f), [code](https://github.com/PRIME-RL/PRIME).
- [Reinforcing Thinking through Reasoning-Enhanced Reward Models](https://arxiv.org/pdf/2501.01457), Dec. 31 2024. `self-critique`. [code](https://github.com/dyang39/DRR).
- [Improve mathematical reasoning in language models by automated process supervision](https://arxiv.org/pdf/2406.06592), Dec. 11 2024.
- [GPT as Monte Carlo language tree: A probabilistic perspective](https://arxiv.org/pdf/2501.07641), Jan. 13 2025.
- [DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning](https://arxiv.org/pdf/2501.12948), Jan. 22 2025.
- [Step-KTO: Optimizing mathematical reasoning through stepwise binary feedback](https://arxiv.org/pdf/2501.10799), Jan. 18 2025.
- [LLMs can plan only if we tell them](https://arxiv.org/pdf/2501.13545), Jan. 23 2025.
- [RAG-Reward: Optimizing RAG with reward modeling and RLHF](https://arxiv.org/pdf/2501.13264), Jan. 22 2025.
- [LLM Reasoning: Key ideas and limitations](https://dennyzhou.github.io/LLM-Reasoning-Berkeley.pdf), talk by Denny Zhou from Google DeepMind.
- [s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393), Jan. 31 2025. [code](https://github.com/simplescaling/s1).
- [Demystifying long chain-of-thought reasoning in LLMs](https://arxiv.org/pdf/2502.03373), Feb. 5 2025. [code](https://github.com/eddycmu/demystify-long-cot).
- [Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2](https://arxiv.org/pdf/2502.03544), Feb. 5 2025.
- [ZebraLogic: On the scaling limits of LLMs for logic reasoning](https://arxiv.org/pdf/2502.01100), Feb. 3 2025.
- [Domaino1s: Guiding LLM reasoning for explainable answers in high-stakes domains](https://arxiv.org/pdf/2501.14431), Jan. 24 2025.
- [UGMathBench: A diverse and dynamic benchmark for undergraduate-level mathematical reasoning with large language models](https://arxiv.org/pdf/2501.13766), Jan. 23 2025.
- [STP: Self-play LLM theorem provers with iterative conjecturing and proving](https://arxiv.org/pdf/2502.00212), Feb. 4 2025. [code](https://github.com/kfdong/STP).
- [MATH-Perturb: Benchmarking LLM's math reasoning abilities against hard perturbations](https://arxiv.org/abs/2502.06453), Feb. 10 2025. [leaderboard](https://math-perturb.github.io/).
- [Scaling up test-time compute with latent reasoning: A recurrent depth approach](https://www.arxiv.org/pdf/2502.05171), Feb. 7 2025. [code](https://github.com/seal-rg/recurrent-pretraining).
- [An analysis for reasoning bias of language models with small initialization](https://arxiv.org/pdf/2502.04375), Feb 5 2025.
- [Towards large reasoning models: A survey on scaling LLM reasoning capabilities](https://arxiv.org/pdf/2501.09686), Jan. 17 2025.
- [On the reasoning capacity of AI models and how to quantify it](https://arxiv.org/pdf/2501.13833), Jan. 23 2025.
- [Competitive programming with large reasoning models](https://arxiv.org/pdf/2502.06807), Feb. 3 2025.
- [Goedel-Prover: A frontier model for open-source automated theorem proving](https://arxiv.org/pdf/2502.07640), Feb. 11 2025.
- [Goedel-Prover-V2: Scaling formal theorem proving with scaffolded data synthesis and self-correction](https://arxiv.org/pdf/2508.03613), Aug. 5 2025.
- [When more is less: Understanding chain-of-thought length in LLMs](https://arxiv.org/pdf/2502.07266), Feb. 11 2025.
- [Exploring the limit of outcome reward for learning mathematical reasoning](https://arxiv.org/pdf/2502.06781), Feb. 10 2025.
- [ReasonFlux: Hierarchical LLM reasoning via scaling thought templates](https://arxiv.org/pdf/2502.06772), Feb. 10 2025.
- [Can 1B LLM surpass 405B LLM? Rethinking compute-optimal test-time scaling](https://arxiv.org/pdf/2502.06703), Feb. 10 2025.
- [Large language models meet symbolic provers for logical reasoning evaluation](https://arxiv.org/pdf/2502.06563), Feb. 10 2025. [code](https://github.com/opendatalab/ProverGen).
- [Examining false positives under inference scaling for mathematical reasoning](https://arxiv.org/pdf/2502.06217), Feb. 10 2025.
- [GSM-Infini: How do your LLMs behave over infinitely increasing context length and reasoning complexity](https://arxiv.org/pdf/2502.05252), Feb. 7 2025. [code](https://github.com/Infini-AI-Lab/gsm_infinite/).
- [On the emergence of thinking in LLMs I: Searching for the right intuition](https://arxiv.org/pdf/2502.06773), Feb. 10 2025.
- [Scalable oversight for superhuman AI via recursive self-critiquing](https://arxiv.org/pdf/2502.04675), Feb. 7 2025.
- [LLMs can easily learn to reason from demonstrations structure, not content, is what matters](https://arxiv.org/pdf/2502.07374), Feb. 11 2025. [code](https://arxiv.org/pdf/2502.07374).
- [CODEI/O: Condensing reasoning patterns via code input-output Prediction](https://arxiv.org/pdf/2502.07316), Feb. 12 2025. [code](https://github.com/hkust-nlp/CodeIO).
- [Diverse inference and verification for advanced reasoning](https://arxiv.org/pdf/2502.09955), Feb. 14 2025.
- [Direct value optimization: Improving chain-of-thought reasoning in LLMs with refined values](https://arxiv.org/pdf/2502.13723), Feb. 19 2025.
  - _"DVO utilizes value signals at individual reasoning steps, optimizing models via a mean square error loss"_
  - _"target values within the DVO are estimated using either MCTS or an outcome value model"_
- [Logic-RL: Unleashing LLM reasoning with rule-based reinforcement learning](https://arxiv.org/pdf/2502.14768), Feb. 20 2025.
- [Measuring faithfulness of chains of thought by unlearning reasoning steps](https://arxiv.org/pdf/2502.14829), Feb. 20 2025. [code](https://github.com/technion-cs-nlp/parametric-faithfulness).
- [Open-Reasoner-Zero](https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903), Feb. 10 2025. [code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero).
- [Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs](https://arxiv.org/pdf/2503.01307), Mar. 3 2025. [code](https://github.com/kanishkg/cognitive-behaviors).
  - [Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases](https://alignment.anthropic.com/2025/distill-paraphrases/), blogpost by Anthropic.
- [Compositional Reasoning with Transformers, RNNs, and Chain of Thought](https://arxiv.org/pdf/2503.01544), Mar. 3 2025.
- [The language of thought is not languages: Evidence from formal logical reasoning](https://alexanderdfung.github.io/assets/pdf/kean2024lotlang_extendedabstract.pdf), Mar. 2025.
- [All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning](https://arxiv.org/abs/2503.01067), Mar. 3 2025. [youtube](https://www.youtube.com/watch?v=E4b3cSirpsg).
- [MALT: Improving Reasoning with Multi-Agent LLM Training](https://arxiv.org/pdf/2412.01928), Feb. 27 2025.
- [The Interface Between Reinforcement Learning Theory and Language Model Post-Training](https://www.let-all.com/blog/2025/03/05/the-interface-between-reinforcement-learning-theory-and-language-model-post-training/), Mar. 5 2025.
- [What do learning dynamics reveal about generalization in LLM reasoning?](https://arxiv.org/pdf/2411.07681), Nov. 18 2024. [code](https://github.com/katiekang1998/reasoning_generalization).
- [A theory of learning with autoregressive chain-of-thought](https://arxiv.org/pdf/2503.07932), Mar. 11 2025.
- [MetaScale: Test-time scaling with evolving meta-thoughts](https://arxiv.org/pdf/2503.13447), Mar. 17 2025.
- [TAO: Using test-time compute to train efficient LLMs without labeled data](https://www.databricks.com/blog/tao-using-test-time-compute-train-efficient-llms-without-labeled-data), Mar. 25 2025.
- [PENCIL: Long thoughts with short memory](https://openreview.net/pdf?id=KRI2Fmffqr), ICLR 2025.
- [MegaMath: Pushing the limits of open math corpora](https://arxiv.org/pdf/2504.02807), Apr. 3 2025. [dataset](https://hf.co/datasets/LLM360/MegaMath).
- [Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning Length?](https://arxiv.org/pdf/2504.01935), Apr. 2 2025. [code](https://github.com/celine-lee/critical_thinking).
- [THINKPRUNE: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning](https://arxiv.org/abs/2504.01296), Apr. 2 2025. [code](https://github.com/UCSB-NLP-Chang/ThinkPrune).
- [Reasoning Models Don’t Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf), Apr. 2025. `Anthropic`.
- [Generative evaluation of complex reasoning in large language models](https://arxiv.org/pdf/2504.02810), Apr. 3 2025.
- [Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning](https://arxiv.org/abs/2504.03635), Apr. 4 2025.
- [Inference-time scaling for generalist reward modeling](https://arxiv.org/pdf/2504.02495), Apr. 5 2025.
- [Adversarial training of reward models](https://arxiv.org/pdf/2504.06141), Apr. 11 2025.
- [On the Markov property of neural algorithmic reasoning: Analyses and methods](https://arxiv.org/abs/2403.04929), Mar. 7 2024.
- [Inference-time scaling for complex tasks: Where we stand and what lies ahead](https://arxiv.org/pdf/2504.00294), Mar. 31 2025.
- [The "think" tool: Enabling Claude to stop and think in complex tool use situations](https://www.anthropic.com/engineering/claude-think-tool), Mar. 20 2925.
- [The first few tokens are all you need: An efficient and effective unsupervised prefix fine-tuning method for reasoning models](https://arxiv.org/pdf/2503.02875), Mar. 4 2025.
- [Cognitive Behaviors that Enable Self-Improving Reasoners](https://arxiv.org/pdf/2503.01307), May 3 2025. [code](https://github.com/kanishkg/cognitive-behaviors).
- [SPaR: Self-play with tree-search refinement to improve instruction-following in large language models](https://arxiv.org/pdf/2412.11605), Mar. 16 2025.
- [Process reward models that think](https://arxiv.org/pdf/2504.16828), Apr. 23 2025. [code](https://github.com/mukhal/thinkprm).
- [PipelineRL](https://huggingface.co/blog/ServiceNow/pipelinerl), Apr. 25 2025. [code](https://github.com/ServiceNow/PipelineRL).
- [Reinforcement learning for reasoning in large language models with one training example](https://arxiv.org/pdf/2504.20571), Apr. 29 2025. [code](https://github.com/ypwang61/One-Shot-RLVR).
- [Self-improvement in language models: The sharpening mechanism](https://arxiv.org/pdf/2412.01951), Dec. 4 2024.
- [AdaptMI: Adaptive skill-based in-context math instructions for small language models](https://arxiv.org/pdf/2505.00147), Apr. 30 2025. [code](https://github.com/princeton-pli/AdaptMI).
- 【Absolute zero: Reinforced self-play reasoning with zero data](https://arxiv.org/pdf/2505.03335), May 7 2025. [code]().
- [Think when you need: Self-adaptive chain-of-thought learning](https://arxiv.org/pdf/2504.03234), Apr. 4 2025.
- [Implicit chain of thought reasoning via knowledge distillation](https://arxiv.org/pdf/2311.01460), Nov. 2 2023. [code](https://github.com/da03/implicit_chain_of_thought/).
- [A study on overfitting in deep reinforcement learning](https://arxiv.org/pdf/1804.06893), Apr. 20 2025.
- [General-Reasoner: Advancing llm reasoning across all domains](https://arxiv.org/pdf/2505.14652v1), May 20 2025. [code](https://github.com/TIGER-AI-Lab/General-Reasoner).
- [Synthetic data rl: Task definition is all you need](https://arxiv.org/abs/2505.17063), May 18 2025. [code](https://github.com/gydpku/Data_Synthesis_RL/).
- [Deep reinforcement learning](https://arxiv.org/pdf/2201.02135), Apr. 23 2023. `book`.
- [Decomposing elements of problem solving: What math does RL teach?](https://arxiv.org/pdf/2505.22756), May 28 2025. [code](https://github.com/cfpark00/RL-Wall).
- [Knowledge or reasoning? A close look at how LLMs think across domains](https://arxiv.org/pdf/2506.02126), Jun. 2 2025. [code](https://github.com/UCSC-VLAA/ReasoningEval).
- [A taxonomy for next-generation reasoning models](https://www.interconnects.ai/p/next-gen-reasoners), Jun. 4 2025. `blogpost`.
  - [Elicitation, the simplest way to understand post-training](https://www.interconnects.ai/p/elicitation-theory-of-post-training), Mar. 11 2025. `blogpost`.
- [Unleashing the reasoning potential of pre-trained LLMs by critique fine-tuning on one problem](https://arxiv.org/pdf/2506.03295), Jun. 3 2025.
- [On learning verifiers for chain-of-thought reasoning](https://arxiv.org/pdf/2505.22650), May 28 2025.
- [Disentangling reasoning and knowledge in medical large language models](https://arxiv.org/pdf/2505.11462), May 16 2025. [code](https://github.com/togethercomputer/BioMed-R1).
- [Don't overthink passage reranking: Is reasoning truly necessary](https://arxiv.org/abs/2505.16886), May 22 2025.
- [TTRL: Test-time reinforcement learning](https://arxiv.org/pdf/2504.16084), Apr. 22 2025. [code](https://github.com/PRIME-RL/TTRL).
- [Reinforcing compositional retrieval: Retrieving step-by-step for composing informative contexts](https://arxiv.org/pdf/2504.11420), Apr. 15 2025. [code](https://github.com/ruyue0001/Step-by-Step-Retrieval).
- [Med-RLVR: Emerging medical reasoning from a 3b base model via reinforcement learning](https://arxiv.org/pdf/2502.19655), Feb. 27 2025.
- [Distill not only data but also rewards: Can smaller language models surpass larger ones?](https://arxiv.org/pdf/2502.19557), Feb. 26 2025.
- [What makes math problems hard for reinforcement learning: A case study](https://arxiv.org/pdf/2408.15332v2), Feb. 11 2025.
- [Atom of thoughts for Markov LLM test-time scaling](https://arxiv.org/pdf/2502.12018), Mar. 23 2025. [code](https://github.com/qixucen/atom).
- [LLM post-training: A deep dive into reasoning large language models](https://arxiv.org/pdf/2502.21321), Mar. 24 2025.
- [Empowering LLMs with logical reasoning: A comprehensive survey](https://arxiv.org/pdf/2502.15652), Jun. 4 2025.
- [Computational-statistical tradeoffs at the next-token prediction barrier: Autoregressive and imitation learning under misspecification](https://arxiv.org/pdf/2502.12465), Feb. 18 2025.
- [Self-steering language models](https://arxiv.org/pdf/2504.07081), Apr. 9 2025.
- [Reinforcement pre-training](https://arxiv.org/pdf/2506.08007), Jun. 9 2025.
- [AbstRal: Augmenting LLMs' reasoning by reinforcing abstract thinking](https://arxiv.org/pdf/2506.07751), Jun. 11 2025.
- [OctoThinker: Revisiting mid-training in the era of rl scaling](https://natural-rugby-f7c.notion.site/OctoThinker-Revisiting-Mid-Training-In-the-Era-of-RL-Scaling-1d20b810e2d680c494a9f9dad0a90d53), 2025. [code](https://github.com/GAIR-NLP/OctoThinker).
- [Reasoning models don't always say what they think](https://arxiv.org/pdf/2505.05410), May 8 2025.
- [Advancing conversational diagnostic AI with multimodal reasoning](https://arxiv.org/abs/2505.04653), May 6 2025.
- [Perception, reason, think, and plan: A survey on large multimodal reasoning models](https://arxiv.org/pdf/2505.04921), May 8 2025.
- [Scalable chain of thoughts via elastic reasoning](https://arxiv.org/pdf/2505.05315), May 21 2025.
- [Beyond theorem proving: Formulation, framework and benchmark for formal problem-solving](https://arxiv.org/pdf/2505.04528), May 7 2025.
- [When reasoning beats scale: A 1.5b reasoning model outranks 13b LLMs as discriminator](https://arxiv.org/pdf/2505.03786), Apr. 30 2025.
- [Tina: Tiny reasoning models via LoRA](https://arxiv.org/pdf/2504.15777), Apr. 22 2025.
- [LiveCodeBench Pro: How do Olympiad medalists judge LLMs in competitive programming](https://arxiv.org/pdf/2506.11928), Jun. 13 2025.
- [Evaluation is all you need: Strategic overclaiming of LLM reasoning capabilities through evaluation design](https://arxiv.org/pdf/2506.04734), Jun. 10 2025.
- [Beyond gold standards: Epistemic ensemble of LLM judges for formal mathematical reasoning](https://arxiv.org/pdf/2506.10903), Jun. 12 2025.
- [Direct reasoning optimization: LLMs can reward and refine their own reasoning for open-ended tasks](https://arxiv.org/pdf/2506.13351), Jun. 16 2025.
- [Training large language models to reason in a continuous latent space](https://arxiv.org/pdf/2412.06769), Dec. 11 2024.
- [What do learning dynamics reveal about generalization in LLM mathematical reasoning](https://openreview.net/pdf?id=bivU8fTTDo), ICML 2025.
- [Implicit reward as bridge: A unified view of SFT and DPO algorithm](https://arxiv.org/pdf/2507.00018), Jun. 15 2025.
- [Thought anchors: Which LLM reasoning steps matter?](https://arxiv.org/pdf/2506.19143), Jun. 23 2025.
- [AdaCoT: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning](https://arxiv.org/pdf/2505.11896), May 25 2025.
- [Generalizing verifiable instruction following](https://arxiv.org/pdf/2507.02833), Jul. 3 2025. [code](https://github.com/allenai/IFBench).
- [Wider or deeper? Scaling LLM inference-time compute with adaptive branching tree search](https://arxiv.org/pdf/2503.04412), Jun. 27 2025.
- [Polaris: A post-training recipe for scaling reinforcement learning on advanced reasoning models](https://hkunlp.github.io/blog/2025/Polaris/), Jun. 20 2025. [code](https://github.com/ChenxinAn-fdu/POLARIS).
- [SPIRAL: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning](https://arxiv.org/abs/2506.24119), Jun. 30 2025. [code](https://github.com/spiral-rl/spiral).
- [Inference-aware fine-tuning for best-of-n sampling in large language models](https://arxiv.org/pdf/2412.15287), Dec. 18 2024.
- [Dissecting clinical reasoning in language models: A comparative study of prompts and model adaptation strategies](https://arxiv.org/pdf/2507.04142), Jul. 5 2025.
- [Entropys scheduling](https://howetissue.notion.site/#231618bab567801e90c4ca0c063eede3).
- [Inverse scaling in test-time compute](https://arxiv.org/pdf/2507.14417), Jul. 19 2025. [code](https://safety-research.github.io/inverse-scaling-ttc/).
- [Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation](https://www.alphaxiv.org/abs/2507.10524), Jul. 21 2025. [code](https://github.com/raymin0223/mixture_of_recursions).
- [Disentangling logic: The role of context in large language model reasoning capabilities](https://arxiv.org/pdf/2406.02787), Jun. 4 2024. [code](https://github.com/agiresearch/ContextHub).
- [Learning to discover abstractions for LLM reasoning](https://openreview.net/pdf?id=kzccnANFQ7), ICML 2025 Workshop.
- [Does thinking more always help? Understanding test-time scaling in reasoning models](https://arxiv.org/abs/2506.04210), Jun. 4 2025.
- [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/pdf/2508.04440), Aug. 6 2025.
- [On the generalization of SFT: A reinforcement learning perspective with reward recitfication](https://arxiv.org/pdf/2508.05629), Aug. 7 2025. [code](https://github.com/yongliang-wu/DFT).
- [Learning to reason for factuality](https://arxiv.org/pdf/2508.05618), Aug. 7 2025.
- [GHPO: Adaptive guidance for stable and efficient LLM reinforcement learning](https://arxiv.org/pdf/2507.10628), Jul. 16 2025. [code](https://github.com/hkgc-1/GHPO).
- [Frontier LLMs still struggle with simple reasoning tasks](https://arxiv.org/pdf/2507.07313), Jul. 9 2025.
- [Scaling reasoning, losing control: Evaluating instruction following in large reasoning models](https://arxiv.org/pdf/2505.14810), May 25 2025. [code](https://github.com/TingchenFu/MathIF).
- [Re:Form - Reducing human priors in scalable formal software verification with RL in LLMs: A preliminary study on Dafny](https://arxiv.org/pdf/2507.16331), Jul. 25 2025. [code](https://github.com/Veri-Code/ReForm). `verl`.
- [ProRL: Prolonged reinforcement learning expands reasoning boundaries in large language models](https://arxiv.org/pdf/2505.24864), May 30 2025.
- [Seed-Prover: Deep and broad reasoning for automated theorem proving](https://www.alphaxiv.org/abs/2507.23726v2), Aug. 2025.
- [Reinforcement learning with rubric anchors](https://arxiv.org/pdf/2508.12790), AUg. 18 2025. [code](https://github.com/yuh-zha/Vision-G1).
- [OptimalThinkBench: Evaluating over and underthinking in LLMs](https://arxiv.org/pdf/2508.13141), Aug. 18 2025. [code](https://github.com/facebookresearch/RAM/tree/main/projects/otb).
- [Intern-S1: A scientific multimodal foundation model](https://arxiv.org/pdf/2508.15763), Aug. 21 2025.
- [Don't think twice! Over-reasoning impairs confidence calibration](https://arxiv.org/pdf/2508.15050), Aug. 20 2025.
- [Can LLMs reason abstractly over math word problems without CoTs? Disentangling abstract formulation from arithmetic computation](https://arxiv.org/pdf/2505.23701), May 2025.
- [The invisible leash: Why RLVR may not escape its origin](https://arxiv.org/pdf/2507.14843), Jul. 20 2025.
- [rStar2-Agent: Agentic reasoning technical report](https://arxiv.org/pdf/2508.20722), Aug. 28 2025. [code](https://github.com/microsoft/rStar).
- [Reasoning-intensive regression](https://arxiv.org/pdf/2508.21762), Aug. 29 2025.
- [Implicit reasoning in large language models: A comprehensive survey](https://arxiv.org/pdf/2509.02350), Sep. 2 2025. [github](https://github.com/digailab/awesome-llm-implicit-reasoning).
- [e3: Learning to explore enables extrapolation of test-time compute for LLMs](https://arxiv.org/pdf/2506.09026), Jun. 13 2025. [code](https://matthewyryang.com/e3/).
- [FormulaOne: Measuring the depth of algorithmic reasoning beyond competitive programming](https://www.alphaxiv.org/pdf/2507.13337), Jul. 17 2025.
- [Open-Reasoner-Zero: An open source approach on scaling up reinforcement learning on the base model](https://arxiv.org/pdf/2503.24290), Jul. 5 2025. [code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero).
- [Efficient hierarchical reasoning in LLMs through reinforcement learning](https://arxiv.org/pdf/2509.03646), Sep. 3 2025.
- [A comprehensive survey on trutworthiness in reasoning with large language models](https://arxiv.org/pdf/2509.03871), Sep. 4 2025. [code](https://github.com/ybwang119/Awesome-reasoning-safety).
- [RL's razor: Why online reinforcement learning forgets less](https://arxiv.org/pdf/2509.04259), Sep. 4 2025.
- [Quiet-STaR: Language models can teach themselves to think before speaking](https://arxiv.org/pdf/2403.09629?), COLM 2024.
- [Emergent hierarchical reasoning in LLMs through reinforcement learning](https://arxiv.org/pdf/2509.03646), Sep. 3 2025.
- [Trading-R1: Financial trading with LLM reasoning via reinforcement learning](https://arxiv.org/abs/2509.11420), Sep. 14 2025. [code](https://arxiv.org/abs/2509.11420).
  - How to develop a domain-specific reasoning LLMs?
  - _"Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum"_
  - _"Training uses Tauric-TR1-DB, a 100K-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources"_
- [Recursive inference scaling: A winning path to scalable inference in language and multimodal systems](https://arxiv.org/pdf/2502.07503), May 8 2025.
- [Metacognitive reuse: Turning recurring LLM reasoning into concise behaviors](https://arxiv.org/pdf/2509.13237), Sep. 16 2025.
- [DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf), 2025. `appendix`.
- [Beyond ten turns: Unlocking long-horizon abentic search with large-scale asychronous RL](https://arxiv.org/abs/2508.07976), Aug. 11 2025.
- [RAST: Reasoning activation in LLMs via small-model transfer](https://arxiv.org/pdf/2506.15710), May 30 2025. [code](https://github.com/ozyyshr/RAST/).
- [Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards](https://arxiv.org/pdf/2505.24760), May 30 2025. [code](https://arxiv.org/pdf/2505.24760).
- [The majority is not always right: RL training for solution aggregation](https://arxiv.org/pdf/2509.06870), Sep. 8 2025.
- [PipelineRL: Faster on-policy reinforcement learning for long sequence generation](https://arxiv.org/pdf/2509.19128), Sep. 23 2025.
  - Remind me of Moonshot's [checkpoint engine](https://github.com/MoonshotAI/checkpoint-engine).
- [Reinforcement learning on pre-training data](https://arxiv.org/pdf/2509.19249), Sep. 23 2025.
- [Thinking augmented pre-training](https://arxiv.org/pdf/2509.20186), Sep. 24 2025.
- [SIM-CoT: Supervised implicit chain-of-thought](https://arxiv.org/pdf/2509.20317), Sep. 24 2025. [code](https://github.com/InternLM/SIM-CoT).
- [Language models that think, chat better](https://arxiv.org/pdf/2509.20357), Sep. 24 2025. [code](https://github.com/princeton-pli/RLMT).
- [AlphaZero-like tree-search can guide large language model decoding and training](https://arxiv.org/pdf/2309.17179), Feb. 9 2024.
- [The era of real-world human interaction: RL from user conversations](https://arxiv.org/pdf/2509.25137), Sep. 29 2025.
- [Random policy valuation is enough for LLM reasoning with verified rewards](https://arxiv.org/pdf/2509.24981), Sep. 29 2025. [code](https://github.com/tinnerhrhe/ROVER/).
- [Enhancing large language model reasoning with reward models: An analytical survey](https://arxiv.org/pdf/2510.01925), Oct. 3 2025.
- [Personalized reasoning: Just-in-time personalization and why LLMs fail at it](https://arxiv.org/pdf/2510.00177), Sep. 30 2025.
- [RLAD: Training LLMs to discover abstractions for solving reasoning problems](https://arxiv.org/pdf/2510.02263), Oct. 2 2025. [code](https://cohenqu.github.io/rlad.github.io/).
- [Evolution strategies at scale: LLM fine-tuning beyond reinforcement learning](https://arxiv.org/pdf/2509.24372), Sep. 29 2025. [code](https://github.com/VsonicV/es-fine-tuning-paper).
- [MobileLLM-R1: Exploring the limits of sub-billion language model reasoners with open training recipes](https://arxiv.org/pdf/2509.24945), Sep. 29 2025. [code](https://arxiv.org/pdf/2509.24945).
- [Retrieval-of-thought: Efficient reasoning via reusing thoughts](https://arxiv.org/pdf/2509.21743), Sep. 26 2025.
- [Benefits and pitfalls of reinforcement learning for language model planning: A theoretical perspective](), Sep. 26 2025.
- [Random policy valuation is enough for LLM reasoning with verifiable rewards](https://arxiv.org/pdf/2509.24981), Sep. 29 2025. [code](https://arxiv.org/pdf/2509.24981).
- [Why can't transformers learn multiplication? Reverse-engineering reveals long-range dependency pitfalls](https://arxiv.org/abs/2510.00184), Sep. 30 2025. [code](https://github.com/ajyl/icot).
- [RLBFF: Binary flexible feedback to bridge between human feedback and verifiable rewards](https://arxiv.org/pdf/2509.21319), Sep. 25 2025.
- [DELTA-Code: How does RL unlock and transfer new programming algorithms in LLMs?](https://arxiv.org/pdf/2509.21016), Sep. 25 2025. [code](https://github.com/sunblaze-ucb/rl-grok-recipe).
- [ScaleDiff: Scaling difficult problems for advanced mathematical reasoning](https://arxiv.org/pdf/2509.21070), Sep. 25 2025. [code](https://github.com/QizhiPei/ScaleDiff).
- [Online rubrics elicitation from pairwise comparisons](https://arxiv.org/abs/2510.07284), Oct. 8 2025.
- [The Markovian thinker](https://arxiv.org/pdf/2510.06557), Oct. 8 2025.
- [OpenThoughts: Data recipes for reasoning models](https://arxiv.org/pdf/2506.04178), Jun. 5 2025.
- [Taming imperfect process verifiers: A sampling perspective on backtracking](https://www.arxiv.org/pdf/2510.03149), Oct. 3 2025.
- [ReasoningBank: Scaling agent self-evolving with reasoning memory](https://arxiv.org/pdf/2509.25140), Sep. 29 2025.
- [RAM 2: Reasoning, attention & memory - 10 years on](https://facebookresearch.github.io/RAM/workshop/COLM_2025/), Workshop COLM 2025.
- [On the design of KL-regularized policy gradient algorithms for LLM reasoning](https://arxiv.org/pdf/2505.17508), May 23 2025. [code](https://github.com/complex-reasoning/RPG).
- [A state-of-the-art SQL reasoning model using RLVR](https://arxiv.org/pdf/2509.21459), Sep. 25 2025.
- [The art of scaling reinforcement learning compute for LLMs](https://arxiv.org/pdf/2510.13786), Oct. 15 2025.
- [Reward hacking research update](https://blog.eleuther.ai/reward_hacking/), Oct. 7 2025.
- [Reasoning with sampling: Your base model is smarter than you think](https://arxiv.org/abs/2510.14901), Oct. 16 2025. [code](https://github.com/aakaran/reasoning-with-sampling).
- [Not all bits are equal: Scale-dependent memory optimization strategies for reasoning models](https://arxiv.org/pdf/2510.10964), Oct. 13 2025.
- [Front-loading reasoning: The synergy between pretraining and post-training data](https://arxiv.org/pdf/2510.03264), Sep. 26 2025.
- [Deep self-evolving reasoning](https://arxiv.org/pdf/2510.17498), Oct. 20 2025.
- [ReasonIF: Large reasoning models fail to follow instructions during reasoning](https://arxiv.org/pdf/2510.15211), Oct. 17 2025. [code](https://github.com/ykwon0407/reasonIF).
- [From r to Q^*: Your language model is secretly a Q-function](https://arxiv.org/pdf/2404.12358), Aug. 12 2024.
- [Scaf-GRPO: Scaffolded group relative policy optimization for enhancing LLM reasoning](https://arxiv.org/pdf/2510.19807), Oct. 22 2025. [code](https://github.com/dvlab-research/Scaf-GRPO).
- [Supervised reinforcement learning: From expert trajectories to step-wise reasoning](https://arxiv.org/pdf/2510.25992), Oct. 29 2025.
- [OpenSIR: Open-ended self-improving reasoner](https://arxiv.org/pdf/2511.00602), Nov. 1 2025. [code](https://github.com/EdinburghNLP/OpenSIR).
  - Weakness - _"While self-play offers a promising alternative, existing approaches depend on external verifiers or cannot learn open-endedly"_
  - Solution - _"OpenSIR, a self-play framework where an LLM learns to generate and solve novel problems by alternating teacher and student roles without external supervision"_
  - My question(s)
    - I wonder how sample-efficient is OpenSIR? And how to choose or construct the best seed problem?
- [RLAC: Reinforcement learning with adversarial critic for free-form generation tasks](https://arxiv.org/abs/2511.01758), Nov. 3 2025. [code](https://mianwu01.github.io/RLAC_website/).
  - _"This enables verifying outputs on free-form generation tasks without needing to enumerate or identify all possible rubrics or manually engineer robust reward models"_
- [Chasing the tail: Effective rubric-based reward modelling for large language model post-training](https://arxiv.org/pdf/2509.21500), Sep. 25 2025. [code](https://github.com/Jun-Kai-Zhang/rubrics).
- [Beyond Markovian: Reflective exploration via Bayes-adaptive rl for LLM reasoning](https://arxiv.org/abs/2505.20561), May 26 2025. [code](https://github.com/shenao-zhang/BARL).
- [The peril of preference: Why grpo fails on ordinal rewards](https://arxiv.org/abs/2511.04439), Nov. 6 2025.
  - Methods: _"propose Correctness Relative Policy Optimization, solving GRPO's flaw where it assigns a positive advantage to failed trajectories and reinforces incorrect behavior"_
- [The effect of sampling temperature on problem solving in large language models](https://aclanthology.org/2024.findings-emnlp.432.pdf), EMNLP 2024. [code](https://github.com/matthewrenze/jhu-llm-temperature).
- [Optimizing temperature for language models with multi-sample inference](https://arxiv.org/pdf/2502.05234), Jun. 26 2025. [code](https://github.com/StigLidu/TURN).
- [P1: Mastering physics olympiads with reinforcement learning](https://arxiv.org/pdf/2511.13612), Nov. 17 2025. [code](https://github.com/PRIME-RL/P1).
- [Dataset reset policy optimization for RLHF](https://arxiv.org/pdf/2404.08495), Apr. 16 2024. [code](https://github.com/Cornell-RL/drpo).
- [The path not taken: RLVR provably learns off the principals](https://arxiv.org/abs/2511.08567), Nov. 11 2025.
- [Cognitive foundations for reasoning and their manifestation in llms](https://arxiv.org/pdf/2511.16660), Nov. 24 2025.

#### Adaptive reasoning

- [ProofOptimizer: Training language models to simplify proofs without human demonstrations](https://arxiv.org/pdf/2510.15700), Oct. 17 2025.
- [DLER: Doing lenght penalty right - Incentivizing more intelligence per token via reinforcement learning](https://arxiv.org/pdf/2510.15110v1), Oct. 16 2025.

#### Latent reasoning

- [Latent reasoning in LLMs as a vocabulary-space superposition](https://arxiv.org/pdf/2510.15522), Oct. 17 2025. [code](https://github.com/DJC-GO-SOLO/Latent-SFT).

#### Interactive learning

- [The era of real-world human interaction: RL from user conversations](https://arxiv.org/pdf/2509.25137), Sep. 29 2025.
- [Language models can learn from verbal feedback without scalar rewards](https://arxiv.org/pdf/2509.22638), Sep. 26 2025. [code](https://github.com/sail-sg/feedback-conditional-policy).

#### Reasoning for VLMs

- [Vision-G1: Towards general vision language reasoning with multi-domain data curation](https://arxiv.org/pdf/2508.12680), Aug. 18 2025.
- [OpenVLThinker: Complex vision-language reasoning via iteratiev SFT-RL cycles](https://arxiv.org/pdf/2503.17352), Jul. 22 2025. [code](https://github.com/yihedeng9/OpenVLThinker).
- [VISCO: Benchmarking fine-grained critique and correction towards self-improvement in visual reasoning](https://arxiv.org/pdf/2412.02172), Mar. 18 2025. [code](https://github.com/PlusLabNLP/VISCO).
- [When seeing is not enough: Revealing the limits of active reasoning in MLLMs](https://arxiv.org/pdf/2510.15421), Oct. 17 2025. 
- [Train a unified multimodal data quality classifier with synthetic data](https://arxiv.org/pdf/2510.15162), Oct. 16 2025. [code](https://github.com/Victorwz/UniFilter). [post-training data](https://huggingface.co/datasets/weizhiwang/unifilter_train_data/viewer).
- [Composition-grounded instruction synthesis for visual reasoning](https://arxiv.org/pdf/2510.15040), Oct. 16 2025.
- [OpenMMReasoner: Pushing the frontiers for multimodal reasoning with an open and general recipe](https://arxiv.org/pdf/2511.16334), Nov. 20 2025. [code](https://github.com/EvolvingLMMs-Lab/OpenMMReasoner).

#### RL algorithmic innovations

- [A tutorial: An intuitive explanation of offline reinforcement learning theory](https://arxiv.org/pdf/2508.07746), Aug. 11 2025.
- [Asymmetric REINFORCE for off-policy reinforcement learning: Balancing positive and negative rewards](https://arxiv.org/pdf/2506.20520), Jun. 25 2025.
- [Part I: Tricks or traps? A deep dive into RL for LLM reasoning](https://arxiv.org/pdf/2508.08221), Aug. 11 2025. [code](https://github.com/alibaba/ROLL).

#### Understanding reasoning

- [Towards a mechanistic interpretation of multi-step reasoning capabilities of language models](https://arxiv.org/pdf/2310.14491), Oct. 23 2023. [code](https://github.com/yifan-h/MechanisticProbe).
- [How well can reasoning models identify and recover from unhelpful thoughts](https://arxiv.org/pdf/2506.10979), Jun. 12 2025.
- [Reasoning by superposition: A theoretical perspective on chain of continuous thought](https://arxiv.org/pdf/2505.12514), May 18 2025.
- [Demystifing long chain-of-thought reasnoning](https://openreview.net/pdf?id=OLodUbcWjB), ICML 2025. [code](https://github.com/eddycmu/demystify-long-cot).
- [Theoretical modeling of LLM self-improvement training dynaics through solver-verifier gap](https://arxiv.org/pdf/2507.00075), Jun. 29 2025.
- [A theory of inference compute scaling: Reasoning through directed stochastic skill search](https://arxiv.org/pdf/2507.00004), Jun. 10 2025.
- [The serial scaling hypothesis](https://arxiv.org/abs/2507.12549), Jul. 16 2025. [tweet](https://x.com/layer07_yuxi/status/1947473214898377108).
- [CoT-Space: A theoretical framework for internal slow-thinking via reinforcement learning](https://arxiv.org/pdf/2509.04027), Sep. 4 2025.
- [Understanding the thinking process of reasoning models: A perspective from Schoenfeld's Episode Theory](https://arxiv.org/pdf/2509.14662), Sep. 18 2025. [code](https://github.com/MingLiiii/Schoenfeld_Reasoning).
- [What characterizes effective reasoning? Revisiting length, review, and structure of CoT](https://arxiv.org/pdf/2509.19284), Sep. 23 2025.
- [Base models know how to reason, thinking models learn when](https://arxiv.org/pdf/2510.07364), Oct. 8 2025.
- [Prior knowledge makes it possible: From sublinear graph algorithms to LLM test-time methods](https://arxiv.org/pdf/2510.16609), Oct. 18 2025.
- [Analyzing the power of chain of thought through memorization capabilities](https://arxiv.org/abs/2511.01190), Nov. 3 2025.
  - _"Does CoT expand the capability of transformers across all reasoning tasks?"_, _"give a complete description of the memorization capabilities of fixed-precision transformers with or without CoT and give a negative answer to the above-mentioned question"_
  - _"We demonstrate that reasoning with transformers is essentially a memorization problem for reasoning datasets"_
- [Reasoning planning for language models](https://arxiv.org/abs/2511.00521), Nov. 1 2025. [code](https://github.com/nguyenngocbaocmt02/EPIC).
  - A typical reasoning method - _"generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy"_
  - _"We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fied generation distributions and candidate sizes"_
- [Are language models efficient reasoners? A perspective from logic programming](https://arxiv.org/pdf/2510.25626), Oct. 29 2025. [code](https://github.com/rycolab/reasoning-efficiency).
- [How reinforcement learning after next-token prediction facilitates learning](https://arxiv.org/pdf/2510.11495), Oct. 13 2025.
- [Output supervision can obfuscate the chain-of-thought](https://arxiv.org/pdf/2511.11584), Oct. 11 2025. [code]().


