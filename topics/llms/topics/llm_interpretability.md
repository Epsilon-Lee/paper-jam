

> The science of dissecting LLMs.

### Interpretability

- [A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning](https://arxiv.org/pdf/2406.12255), Jun. 18 2024. `interpretability`.
- [Exploring the Impact of a Transformerâ€™s Latent Space Geometry on Downstream Task Performance](https://arxiv.org/pdf/2406.12159), Jun. 18 2024.
- [Pre-trained Large Language Models Use Fourier Features to Compute Addition](https://arxiv.org/pdf/2406.03445), Jun. 5 2024. `mechanistic interpretability`.
- [Compact Proofs of Model Performance via Mechanistic Interpretability](https://arxiv.org/pdf/2406.11779), Jun. 17 2024. `mechanistic interpretability`.
- [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/pdf/2406.11233), Jun. 17 2024. [tweet](https://x.com/siyan_zhao/status/1805277462890492321). `interpretability`.
- [Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs](https://arxiv.org/pdf/2406.20086), Jun. 28 2024. [code](https://footprints.baulab.info/). `mechanistic interpretability`.
- [Transcoders Find Interpretable LLM Feature Circuits](https://arxiv.org/pdf/2406.11944), Jun. 17 2024. `mechanistic interpretability`.
- [Estimating Knowledge in Large Language Models Without Generating a Single Token](https://arxiv.org/pdf/2406.12673), Jun. 18 2024.
- [Transformer Layers as Painters](https://arxiv.org/pdf/2407.09298), Jul. 12 2024. `representation similarity`.
- [LLM Circuit Analyses Are Consistent Across Training and Scale](https://arxiv.org/pdf/2407.10827), Jul. 15 2024.
  - _"we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70b to 2.8b parameters."_
- [Understanding Counting in Small Transformers: The Interplay between Attention and Feed-Forward Layers](https://arxiv.org/pdf/2407.11542), Jul. 16 2024.
- [Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach](https://arxiv.org/pdf/2407.13594), Jul. 18 2024. [code](https://github.com/nilspalumbo/sat-mi).
- [Adversarial Circuit Evaluation](https://arxiv.org/pdf/2407.15166), Jul. 21 2024. `circuits stability`.
  - _" indicating that more robust circuits are needed for safety-critical applications."_
- [NNsight and NDIF: Democratizing access to foundational model internals](https://arxiv.org/pdf/2407.14561), Jul. 18 2024.
- [Generalization v.s. memorization: Tracing language models' capability back to pretraining data](https://arxiv.org/pdf/2407.14985), Jul. 20 2024.
- [When Can Transformers Count to n?](https://arxiv.org/pdf/2407.15160), Jul. 21 2024.
- [Dissecting Multiplication in Transformers: Insights into LLMs](https://arxiv.org/pdf/2407.15360), Jul. 22 2024.
- [Answer, Assemble, Ace: Understanding How Transformers Answer Multiple Choice Questions](https://arxiv.org/pdf/2407.15018), Jul. 21 2024.
- [Latent causal probing: A formal perspective on probing with causal models of data](https://arxiv.org/pdf/2407.13765), Jul. 18 2024.
- [Demystifying Verbatim Memorization in Large Language Models](https://arxiv.org/pdf/2407.17817), Jul. 25 2024. `verbatim memorization`
  - verbatim meaning: in a way that uses exactly the same words as were originally used.
- [iNNspector: Visual, Interactive Deep Model Debugging](https://arxiv.org/pdf/2407.17998), Jul. 25 2024.
- [Relating the Seemingly Unrelated: Principled Understanding of Generalization for Generative Models in Arithmetic Reasoning Tasks](https://arxiv.org/pdf/2407.17963), Jul. 25 2024.
- [Transformers on Markov Data: Constant Depth Suffices](https://arxiv.org/pdf/2407.17686), Jul. 25 2024. [code](https://github.com/Bond1995/Constant-depth-Transformers).
- [Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models](https://arxiv.org/pdf/2407.18158), Jul. 25 2024.
  - _"With Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve non-vacuous generalization bounds for LLMs as large as LLaMA2-70B"_.
- [Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process](https://arxiv.org/pdf/2407.20311), Jul. 29 2024.
- [Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems](https://arxiv.org/pdf/2408.16293), Aug. 29 2024.
- [Memory-efficient Training of LLMs with Larger Mini-batches](https://arxiv.org/pdf/2407.19580), Jul. 28 2024.
- [The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability](https://arxiv.org/pdf/2408.01416), Aug. 2 2024.
- [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://arxiv.org/pdf/2408.05147), Aug. 9 2024.
- [Mathematical Models of Computation in Superposition](https://arxiv.org/pdf/2408.05451), Aug. 10 2024.
- [Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability](https://arxiv.org/pdf/2301.04709), Aug. 7 2024.
  - [Causal Abstractions of Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf), NeurIPS 2021.
- [A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models](https://arxiv.org/pdf/2408.08590), Aug. 16 2024.
- [Where is the signal in tokenization space?](https://arxiv.org/pdf/2408.08541), Aug. 16 2024.
- [Reframing Human-AI Collaboration for Generating Free-Text Explanations](https://arxiv.org/abs/2112.08674), Dec. 16 2021.
  - _"Our approach is able to consistently filter GPT-3-generated explanations deemed acceptable by humans"_
- [Modularity in Transformers: Investigating Neuron Separability & Specialization](https://arxiv.org/pdf/2408.17324), Aug. 30 2024.
- [Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space](https://arxiv.org/pdf/2406.19370), Jun. 27 2024.
- [CONTEXTCITE: Attributing Model Generation to Context](https://openreview.net/pdf?id=PbIe0MNDQx),  ICML 2024 workshop.
- [A percolation model of emergence: Analyzing Transformers trained on a formal language](https://arxiv.org/pdf/2408.12578), Aug. 22 2024. [code](https://github.com/EkdeepSLubana/ConceptPercolation).
- [CONTEXTCITE: Attributing Model Generation to Context](https://arxiv.org/pdf/2409.00729), Sep. 1 2024.
- [Evaluating Synthetic Activations composed of SAE Latents in GPT-2](https://arxiv.org/pdf/2409.15019), Sep. 23 2024.
- [Investigating Layer Importance in Large Language Model](https://arxiv.org/pdf/2409.14381), Sep. 22 2024.
- [Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion](https://arxiv.org/pdf/2401.12947), Jan. 2024.
- [Residual Stream Analysis with Multi-Layer SAEs](https://arxiv.org/pdf/2409.04185), Sep. 6 2024. [code](https://github.com/tim-lawson/mlsae).
- [Characterizing stable regions in the residual stream of LLM](https://proceedings.mlr.press/v202/li23l/li23l.pdf), Sep. 26 2024.
- [Understanding Transformers via N-gram Statistics](https://www.arxiv.org/pdf/2407.12034), Jun. 30 2024.
- [Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective](https://arxiv.org/pdf/2405.16747), May 27 2024.
- [Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory](https://arxiv.org/pdf/2405.08707), May 14 2024.
- [Talking Heads: Understanding Inter-layer Communication in Transformer Language Models](https://arxiv.org/pdf/2406.09519), Jun. 13 2024.
- [Transcoders Find Interpretable LLM Feature Circuits](https://arxiv.org/pdf/2406.11944), Jun. 17 2024. [github](https://github.com/jacobdunefsky/transcoder_circuits).
- [InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques](https://arxiv.org/pdf/2407.14494), Jul. 19 2024.
- [On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability](https://arxiv.org/abs/2405.16845), May 27 2024.
- [Uncovering mesa-optimization algorithms in transformers](https://arxiv.org/pdf/2309.05858), Sep. 11 2023.
- [Causal Estimation of Memorisation Profiles](https://arxiv.org/pdf/2406.04327), Jun. 6 2024.
- [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/pdf/2404.01413), Apr. 29 2024.
- [Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations](https://arxiv.org/pdf/2408.10920), Aug. 29 2024. [github](https://github.com/robertcsordas/onion_representations).
- [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://arxiv.org/pdf/2406.17746), Jun. 25 2024. `memorization`.
- [Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs](https://arxiv.org/pdf/2410.13835), Oct. 17 2024.
- [Hypothesis Testing the Circuit Hypothesis in LLMs](https://arxiv.org/pdf/2410.13032), Oct. 16 2024. `circuits`.
- [Mechanistic?](https://arxiv.org/pdf/2410.09087), Oct. 7 2024.
- [When attention sink emerges in language models: An empirical view](https://arxiv.org/pdf/2410.10781), Oct. 14 2024.
- [Investigating sensitive directions in GPT-2: An improved baseline and comparative analysis of SAEs](https://arxiv.org/pdf/2410.12555), Oct. 16 2024.
- [Analyzing (In)Abilities of SAEs via Formal Languages](https://arxiv.org/pdf/2410.11767), Oct. 15 2024.
- [VibeCheck: Discover & quantify qualitative differences in large language models](https://arxiv.org/pdf/2410.12851), Oct. 28 2024. [code](https://github.com/lisadunlap/VibeCheck).
- [Distinguishing ignorance from error in LLM hallucinations](https://arxiv.org/pdf/2410.22071), Oct. 29 2024. [code](https://github.com/technion-cs-nlp/hallucination-mitigation).
- [Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning](https://arxiv.org/pdf/2411.10397), Nov. 15 2024.
- [Mechanism and emergence of stacked attention heads in multi-layer transformers](https://arxiv.org/pdf/2411.12118), Nov. 18 2024.
- [AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution](https://arxiv.org/pdf/2411.15102), Nov. 22 2024. [code](https://github.com/r-three/AttriBoT).
- [Does representation matter? Exploring intermediate layers in large language models](https://arxiv.org/abs/2412.09563), Dec. 12 2024.
- [Capturing the temporal dependence of training data influence](https://arxiv.org/pdf/2412.09538), Dec. 12 2024.
- [LatentQA: Teaching llms to decode activations into natural language](https://arxiv.org/pdf/2412.08686), Dec. 11 2024. [code](https://github.com/aypan17/latentqa), [blogpost](https://latentqa.github.io/).
- [The buffer mechanism for multi-step information reasoning in language models](https://arxiv.org/pdf/2405.15302), 2024.
- [Gradient routing: Masking gradients to localize computation in neural networks](https://arxiv.org/pdf/2410.04332), Nov. 29 2024. [blogpost](https://turntrout.com/gradient-routing).
- [Knowledge Mechanisms in Large Language Models: A Survey and Perspective](https://arxiv.org/pdf/2407.15017), Dec. 24 2024.
