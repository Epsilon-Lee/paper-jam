

> The science of dissecting LLMs.

### Interpretability

- [Neuron to graph: Interpreting language model neurons at scale](https://arxiv.org/pdf/2305.19911), May 31 2023.
- [Task-specific skill localization in fine-tuned language models](https://proceedings.mlr.press/v202/panigrahi23a/panigrahi23a.pdf), ICML 2023. [code](https://github.com/abhishekpanigrahi1996/Skill-Localization-by-grafting).
- [Do large language models latently perform multi-hop reasoning?](https://arxiv.org/pdf/2402.16837), Feb. 26 2024.
- [A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning](https://arxiv.org/pdf/2406.12255), Jun. 18 2024. `interpretability`.
- [Exploring the Impact of a Transformerâ€™s Latent Space Geometry on Downstream Task Performance](https://arxiv.org/pdf/2406.12159), Jun. 18 2024.
- [Pre-trained Large Language Models Use Fourier Features to Compute Addition](https://arxiv.org/pdf/2406.03445), Jun. 5 2024. `mechanistic interpretability`.
- [Compact Proofs of Model Performance via Mechanistic Interpretability](https://arxiv.org/pdf/2406.11779), Jun. 17 2024. `mechanistic interpretability`. [code](https://github.com/JasonGross/guarantees-based-mechanistic-interpretability/). [slides](https://docs.google.com/presentation/d/15Ws0UwuIjS-rEEziKTAr5rTfvaMN9M1sFFkBV3MHp94/edit#slide=id.g2eeb89ec19f_0_32).
- [Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs](https://arxiv.org/pdf/2406.20086), Jun. 28 2024. [code](https://footprints.baulab.info/). `mechanistic interpretability`.
- [Estimating Knowledge in Large Language Models Without Generating a Single Token](https://arxiv.org/pdf/2406.12673), Jun. 18 2024.
- [Transformer Layers as Painters](https://arxiv.org/pdf/2407.09298), Jul. 12 2024. `representation similarity`.
- [Understanding Counting in Small Transformers: The Interplay between Attention and Feed-Forward Layers](https://arxiv.org/pdf/2407.11542), Jul. 16 2024.
- [Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach](https://arxiv.org/pdf/2407.13594), Jul. 18 2024. [code](https://github.com/nilspalumbo/sat-mi).
- [Adversarial Circuit Evaluation](https://arxiv.org/pdf/2407.15166), Jul. 21 2024. `circuits stability`.
  - _" indicating that more robust circuits are needed for safety-critical applications."_
- [Generalization v.s. memorization: Tracing language models' capability back to pretraining data](https://arxiv.org/pdf/2407.14985), Jul. 20 2024.
- [When Can Transformers Count to n?](https://arxiv.org/pdf/2407.15160), Jul. 21 2024.
- [Dissecting Multiplication in Transformers: Insights into LLMs](https://arxiv.org/pdf/2407.15360), Jul. 22 2024.
- [Answer, Assemble, Ace: Understanding How Transformers Answer Multiple Choice Questions](https://arxiv.org/pdf/2407.15018), Jul. 21 2024.
- [Latent causal probing: A formal perspective on probing with causal models of data](https://arxiv.org/pdf/2407.13765), Jul. 18 2024.
- [Demystifying Verbatim Memorization in Large Language Models](https://arxiv.org/pdf/2407.17817), Jul. 25 2024. `verbatim memorization`
  - verbatim meaning: in a way that uses exactly the same words as were originally used.
- [iNNspector: Visual, Interactive Deep Model Debugging](https://arxiv.org/pdf/2407.17998), Jul. 25 2024.
- [Relating the Seemingly Unrelated: Principled Understanding of Generalization for Generative Models in Arithmetic Reasoning Tasks](https://arxiv.org/pdf/2407.17963), Jul. 25 2024.
- [Transformers on Markov Data: Constant Depth Suffices](https://arxiv.org/pdf/2407.17686), Jul. 25 2024. [code](https://github.com/Bond1995/Constant-depth-Transformers).
- [Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models](https://arxiv.org/pdf/2407.18158), Jul. 25 2024.
  - _"With Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve non-vacuous generalization bounds for LLMs as large as LLaMA2-70B"_.
- [Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process](https://arxiv.org/pdf/2407.20311), Jul. 29 2024.
- [Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems](https://arxiv.org/pdf/2408.16293), Aug. 29 2024.
- [Memory-efficient Training of LLMs with Larger Mini-batches](https://arxiv.org/pdf/2407.19580), Jul. 28 2024.
- [The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability](https://arxiv.org/pdf/2408.01416), Aug. 2 2024.
- [Does representation matter? Exploring intermediate layers in large language models](https://arxiv.org/abs/2412.09563), Dec. 12 2024.
- [Mathematical Models of Computation in Superposition](https://arxiv.org/pdf/2408.05451), Aug. 10 2024.
- [Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability](https://arxiv.org/pdf/2301.04709), Aug. 7 2024.
  - [Causal Abstractions of Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf), NeurIPS 2021.
- [A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models](https://arxiv.org/pdf/2408.08590), Aug. 16 2024.
- [Where is the signal in tokenization space?](https://arxiv.org/pdf/2408.08541), Aug. 16 2024.
- [Reframing Human-AI Collaboration for Generating Free-Text Explanations](https://arxiv.org/abs/2112.08674), Dec. 16 2021.
  - _"Our approach is able to consistently filter GPT-3-generated explanations deemed acceptable by humans"_
- [Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space](https://arxiv.org/pdf/2406.19370), Jun. 27 2024.
- [A percolation model of emergence: Analyzing Transformers trained on a formal language](https://arxiv.org/pdf/2408.12578), Aug. 22 2024. [code](https://github.com/EkdeepSLubana/ConceptPercolation).
- [CONTEXTCITE: Attributing Model Generation to Context](https://arxiv.org/pdf/2409.00729), Sep. 1 2024.
- [Investigating Layer Importance in Large Language Model](https://arxiv.org/pdf/2409.14381), Sep. 22 2024.
- [Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion](https://arxiv.org/pdf/2401.12947), Jan. 2024.
- [Characterizing stable regions in the residual stream of LLM](https://proceedings.mlr.press/v202/li23l/li23l.pdf), Sep. 26 2024.
- [Understanding Transformers via N-gram Statistics](https://www.arxiv.org/pdf/2407.12034), Jun. 30 2024.
- [Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective](https://arxiv.org/pdf/2405.16747), May 27 2024.
- [Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory](https://arxiv.org/pdf/2405.08707), May 14 2024.
- [Talking Heads: Understanding Inter-layer Communication in Transformer Language Models](https://arxiv.org/pdf/2406.09519), Jun. 13 2024.
- [Transcoders Find Interpretable LLM Feature Circuits](https://arxiv.org/pdf/2406.11944), Jun. 17 2024. [github](https://github.com/jacobdunefsky/transcoder_circuits).
- [InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques](https://arxiv.org/pdf/2407.14494), Jul. 19 2024.
- [On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability](https://arxiv.org/abs/2405.16845), May 27 2024.
- [Uncovering mesa-optimization algorithms in transformers](https://arxiv.org/pdf/2309.05858), Sep. 11 2023.
- [Causal Estimation of Memorisation Profiles](https://arxiv.org/pdf/2406.04327), Jun. 6 2024.
- [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/pdf/2404.01413), Apr. 29 2024.
- [Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations](https://arxiv.org/pdf/2408.10920), Aug. 29 2024. [github](https://github.com/robertcsordas/onion_representations).
- [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://arxiv.org/pdf/2406.17746), Jun. 25 2024. `memorization`.
- [Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs](https://arxiv.org/pdf/2410.13835), Oct. 17 2024.
- [When attention sink emerges in language models: An empirical view](https://arxiv.org/pdf/2410.10781), Oct. 14 2024.
- [Investigating sensitive directions in GPT-2: An improved baseline and comparative analysis of SAEs](https://arxiv.org/pdf/2410.12555), Oct. 16 2024.
- [VibeCheck: Discover & quantify qualitative differences in large language models](https://arxiv.org/pdf/2410.12851), Oct. 28 2024. [code](https://github.com/lisadunlap/VibeCheck).
- [Distinguishing ignorance from error in LLM hallucinations](https://arxiv.org/pdf/2410.22071), Oct. 29 2024. [code](https://github.com/technion-cs-nlp/hallucination-mitigation).
- [Mechanism and emergence of stacked attention heads in multi-layer transformers](https://arxiv.org/pdf/2411.12118), Nov. 18 2024.
- [AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution](https://arxiv.org/pdf/2411.15102), Nov. 22 2024. [code](https://github.com/r-three/AttriBoT).
- [Capturing the temporal dependence of training data influence](https://arxiv.org/pdf/2412.09538), Dec. 12 2024.
- [The buffer mechanism for multi-step information reasoning in language models](https://arxiv.org/pdf/2405.15302), 2024.
- [Gradient routing: Masking gradients to localize computation in neural networks](https://arxiv.org/pdf/2410.04332), Nov. 29 2024. [blogpost](https://turntrout.com/gradient-routing).
- [Open problems in mechanistic interpretability](https://arxiv.org/pdf/2501.16496), Jan. 27 2024.
- [Physics of skill learning](https://arxiv.org/pdf/2501.12391), Jan. 21 2025.
- [Propositional interpretability in artificial intelligence](https://arxiv.org/pdf/2501.15740), Jan. 27 2025.
- [Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers](https://arxiv.org/pdf/2502.03708), Feb. 6 2025. [code](https://github.com/dmbeaglehole/neural_controllers).
- [Layer by layer: Uncovering hidden representations in language models](https://arxiv.org/pdf/2502.02013), Feb. 4 2025.
- [Arithmic without algorithms: Language models solve math with a bag of heuristics](https://arxiv.org/pdf/2410.21272), Oct. 28 2024.
- [Building bridges, not walls - Advancing interpretability by unifying feature, data, and model component attribution](https://arxiv.org/pdf/2501.18887), Jan. 31 2025.
- [Partially rewriting a transformer in natural language](https://arxiv.org/pdf/2501.18838), Jan. 31 2025.
- [Linearity of relation decoding in transformer language models](https://arxiv.org/pdf/2308.09124), Feb. 15 2024.
- [Linear representations of sentiment in large language models](https://arxiv.org/abs/2310.15154), Oct. 23 2023.
- [Eliciting language model behaviors with investigator agents](https://arxiv.org/pdf/2502.01236), Feb. 3 2025.
- [Non-literal understanding of number words by language models](https://arxiv.org/pdf/2502.06204), Feb. 10 2025.
- [Systematic outliers in large language models](https://arxiv.org/pdf/2502.06415), Feb. 10 2025.
  - [Demystifying singular defects in large language models](https://arxiv.org/pdf/2502.07004), Feb. 10 2025.
- [A distributional perspective on word learning in neural language models](https://arxiv.org/pdf/2502.05892), Feb. 9 2025.
- [Emergent response planning in LLM](https://arxiv.org/pdf/2502.06258), Feb. 10 2025.
- [Neurons speak in ranges: Breaking free from discrete neuronal attribution](https://arxiv.org/pdf/2502.06809), Feb. 4 2025.
- [Mechanistic interpretability of emotion inference in large language models](https://arxiv.org/pdf/2502.05489), Feb. 8 2025.
- [Can large language models understand intermediate representations](https://arxiv.org/pdf/2502.06854), Feb. 7 2025.
- [We can't understand AI using our existing vocabulary](https://arxiv.org/pdf/2502.07586), Feb. 11 2025.
- [Language generation in the limit](https://arxiv.org/pdf/2404.06757), Apr. 10 2024.
- [Characterizations of language generation with breadth](https://arxiv.org/pdf/2412.18530), Dec. 24 2024.
- [Non-literal understanding of number words by language models](https://arxiv.org/pdf/2502.06204), Feb. 10 2025.
- [On mechanistic circuits for extractive question-answering](https://arxiv.org/pdf/2502.08059), Feb. 12 2025.
- [InnerThoughts: Disentangling representations and predictions in large language models](https://arxiv.org/pdf/2501.17994), Jan. 29 2025.
- [Paying attention to facts: Quantifying the knowledge capacity of attention layers](https://arxiv.org/pdf/2502.05076), Feb. 7 2025.
- [Which attention heads matter for in-context learning](https://arxiv.org/pdf/2502.14010), Feb. 19 2025. [code](https://github.com/kayoyin/icl-heads).
- [How do LLMs perform two-hop reasoning in contest?](https://arxiv.org/pdf/2502.13913), Feb. 19 2025.
  - _"while transformer-based large language models can make two-hop reasoning, they tend to collapse to random guessing when faced with distracting premises"_
  - _"the training dynamics show two stages: a slow learning phase, where 3-layer transformer performs random guessing liek LLMs, followed by an abrupt phase transition, where the 3-layer transformer suddenly reaches 100% accuracy"_
- [SPEX: Scaling feature interaction explanations for LLMs](https://arxiv.org/pdf/2502.13870), Feb. 19 2025.
- [Elucidating mechanisms of demographic bias in LLMs for healthcare](https://arxiv.org/pdf/2502.13319), Feb. 18 2025.
- [Faithfulness measurable masked language models](https://arxiv.org/pdf/2310.07819), Aug. 27 2024. `ICML 2024`.
- [A survey on mechanistic interpretability for multi-modal foundation models](https://arxiv.org/pdf/2502.17516), Feb. 22 2025.
- [Taxonomy, opportunities, and challenges of representation engineering for large language models](https://arxiv.org/abs/2502.19649), Feb. 27 2025. [tweet](https://x.com/JanWehner436164/status/1895527239007268958).
- [Improving neuron-level interpretability with white-box language models](https://arxiv.org/pdf/2410.16443), Feb. 27 2025.
- [(How) do language models track state?](https://arxiv.org/pdf/2503.02854), Mar. 4 2025. [code](https://github.com/belindal/state-tracking).
- [Are formal and functional linguistic mechanisims dissociated in language models?](https://arxiv.org/pdf/2503.11302), Mar. 14 2025.
- [High-dimensional interlingual representations of large language models](https://arxiv.org/pdf/2503.11280), Mar. 14 2025.
- [TinySQL: A progressive text-to-sql dataset for mechanistic interpretability research](https://arxiv.org/pdf/2503.12730), Mar. 17 2025.
- [Tracing the thoughts of a large language model](https://www.anthropic.com/research/tracing-thoughts-language-model), Mar. 27 2025.
- [Bridging the human-ai knowledge gap: Concept discovery and transfer in AlphaZero](https://arxiv.org/pdf/2310.16410), Oct. 25 2023.
- [Sensing and steering stereotypes: Extracting and applying gender representation vectors in LLMs](https://arxiv.org/pdf/2502.19721), Feb. 27 2925. [code](https://github.com/hannahxchen/gender-bias-steering).
- [Extracting Paragraphs from LLM Token Activations](https://arxiv.org/pdf/2409.06328), Sep. 10 2024.
- [MIB: A mechanistic interpretability benchmark](https://arxiv.org/pdf/2504.13151), Apr. 17 2025.
- [Uncovering latent chain of thought vectors in large language models](https://arxiv.org/pdf/2409.14026), Mar. 20 2025.
- [Investigating task-specific prompts and sparse autoencoders for activation monitoring](https://arxiv.org/pdf/2504.20271), Apr. 28 2025.
- [Why do LLMs attend to the first token?](https://arxiv.org/pdf/2504.02732), May 13 2025.
- [Loss landscape degeneracy drives stagewise development in transformers](https://arxiv.org/pdf/2402.02364), Feb. 13 2025.
- [Exploring how LLMs capture and represent domain-specific knowledge](https://arxiv.org/pdf/2504.16871v1), Apr. 23 2025.
- [Why and how LLMs hallucinate: Connecting the dots with subsequence associations](https://arxiv.org/pdf/2504.12691), Apr. 17 2025.
- [Unveiling simplicities of attention: Adaptive long-context head identification](https://arxiv.org/pdf/2502.09647), Mar. 5 2025.
  - _"can we identify which heads require long-context information to predict the next token accurately?"_
- [Unstructured evidence attribution for long context query focused summarization](https://arxiv.org/pdf/2502.14409), Feb. 20 2025.
  - What are the differences between query-focused summarization and docqa?
- [A theoretical design of concept sets: Improving the predictability of concept bottleneck models](https://proceedings.neurips.cc/paper_files/paper/2024/file/b5a412531110b92961fa13c90938806a-Paper-Conference.pdf), NeurIPS 2024.
- [Mechanisms of projective composition of diffusion models](https://arxiv.org/pdf/2502.04549), May 14 2025.
  - A theory of compositionality.
- [Can LLMs separate instructions fron data? And what do we even mean by that?](https://arxiv.org/pdf/2403.06833), Jan. 31 2025.
- [Understanding in-context learning of addition via activation subspaces](https://arxiv.org/pdf/2505.05145), May 15 2025.
- [Generalization or hallucination? Understanding out-of-context reasoning in transformers](https://arxiv.org/pdf/2506.10887), Jun. 12 2025.
- [Decomposing MLP activations into interpretable features via semi-nonnegative matrix factorization](https://arxiv.org/pdf/2506.10920), Jun. 12 2025. [code](https://github.com/ordavid-s/snmf-mlp-decomposition).
  - _"evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner."_
  - Critics on SAEs: _"struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model"_
  - Future direction: How to scale this method towards circuit graph across layers, like that of circuit tracing work from Anthropic.

#### Sparse autoencoders, dictionary learning and beyond

- [Sparse Autoencoders find highly interpretable features in language models](https://arxiv.org/pdf/2309.08600), Sep. 2023. [code](https://github.com/ai-safety-foundation/sparse_autoencoder).
- [Transcoders Find Interpretable LLM Feature Circuits](https://arxiv.org/pdf/2406.11944), Jun. 17 2024. `mechanistic interpretability`.
- [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://arxiv.org/pdf/2408.05147), Aug. 9 2024.
- [Evaluating Synthetic Activations composed of SAE Latents in GPT-2](https://arxiv.org/pdf/2409.15019), Sep. 23 2024.
- [Residual Stream Analysis with Multi-Layer SAEs](https://arxiv.org/pdf/2409.04185), Sep. 6 2024. [code](https://github.com/tim-lawson/mlsae).
- [Analyzing (In)Abilities of SAEs via Formal Languages](https://arxiv.org/pdf/2410.11767), Oct. 15 2024.
- [Sparse autoencoders do not find canonical units of analysis](https://arxiv.org/pdf/2502.04878), Feb. 7 2025. [code](https://metasaes.streamlit.app/).
- [AxBench: Steering LLMs? Even simple baselines outperforms sparse autoencoders](https://arxiv.org/pdf/2501.17148), Jan. 29 2025. [code](https://github.com/stanfordnlp/axbench).
- [Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning](https://arxiv.org/pdf/2411.10397), Nov. 15 2024.
- [Transcoders beat sparse autoencoders for interpretability](https://arxiv.org/pdf/2501.18823), Jan. 31 2025.
- [Mathematical models of computation in superposition](https://arxiv.org/pdf/2408.05451), Aug. 10 2024.
- [Steering large language model activations in sparse spaces](https://arxiv.org/pdf/2503.00177), Feb. 28 2025.
- [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/pdf/2503.05613), Mar. 7 2025.
- [Sparse autoencoder as a zero-shot classifier for concept erasing in text-to-image diffusion models](https://arxiv.org/pdf/2503.09446), Mar. 18 2025.
- [Negative results for SAEs on downstream tasks and deprioritising SAE research](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks), Mar. 27 2025. [Revolutionary paradigms](https://plato.stanford.edu/entries/incommensurability/#RevoParaThomKuhnInco).
- [Implicit geometry of next-token prediction: From language sparsity patterns to model representations](https://arxiv.org/pdf/2408.15417), Feb. 19 2025. [code](https://github.com/YizeZhao/Implicit_Geometry_of_NTP).
- [Jacobian sparse autoencoders: Sparsify computations, not just activations](https://arxiv.org/pdf/2502.18147), Feb. 25 2025. [code](https://github.com/lucyfarnik/jacobian-saes).
- [Between circuits and Chomsky: Pre-training on formal languages imparts linguistic biases](https://arxiv.org/abs/2502.19249), Feb. 26 2025.
- [Injecting structural hints: Using language models to study inductive biases in language learning](https://aclanthology.org/2023.findings-emnlp.563.pdf), EMNLP 2023.
- [Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition](https://arxiv.org/pdf/2504.00194), Mar. 31 2025. [code](https://github.com/briannachrisman/eigenestimation).
- [Towards principled evaluations of sparse autoencoders for interpretability and control](https://openreview.net/pdf?id=1Njl73JKjB), ICLR 2025.

#### Probing

- [InversionView: A general-purpose method for reading information from neural activations](https://arxiv.org/pdf/2405.17653), May 2024.
- [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/pdf/2406.11233), Jun. 17 2024. [tweet](https://x.com/siyan_zhao/status/1805277462890492321). `interpretability`.
- [LatentQA: Teaching llms to decode activations into natural language](https://arxiv.org/pdf/2412.08686), Dec. 11 2024. [code](https://github.com/aypan17/latentqa), [blogpost](https://latentqa.github.io/).

#### Parameter attribution, circuits

- [Transformer feed-forward layers are key-value memories](https://arxiv.org/pdf/2012.14913), Sep. 5 2021.
- [Transformer feed-forward layers build predictions by prompting concepts in the vocabulary space](https://arxiv.org/pdf/2203.14680), Oct. 2022.
- [Locating and editing factual associations in GPT](https://arxiv.org/abs/2202.05262), Feb. 10 2022.
- [NNsight and NDIF: Democratizing access to foundational model internals](https://arxiv.org/pdf/2407.14561), Jul. 18 2024.
- [Adversarial circuit evaluation](https://arxiv.org/pdf/2407.15166), Jul. 21 2024.
- [LLM Circuit Analyses Are Consistent Across Training and Scale](https://arxiv.org/pdf/2407.10827), Jul. 15 2024.
  - _"we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70b to 2.8b parameters."_- [Modularity in Transformers: Investigating Neuron Separability & Specialization](https://arxiv.org/pdf/2408.17324), Aug. 30 2024.
- [Mechanistic?](https://arxiv.org/pdf/2410.09087), Oct. 7 2024.
- [Hypothesis testing the circuit hypothesis in llms](https://arxiv.org/pdf/2410.13032), Oct. 16 2024. [code](https://github.com/blei-lab/circuitry?tab=readme-ov-file).
- [Understanding factual recall in Transformers via associative memories](https://arxiv.org/pdf/2412.06538v1), Dec. 9 2024.
- [Knowledge Mechanisms in Large Language Models: A Survey and Perspective](https://arxiv.org/pdf/2407.15017), Dec. 24 2024.
- [Interpretability in parameter space: Minimizing mechanistic description length with attribution-based parameter decomposition](https://arxiv.org/pdf/2501.14926), Jan. 29 2025. [code](https://github.com/ApolloResearch/apd).
- [Position-aware automatic circuit discovery](https://arxiv.org/pdf/2502.04577), Feb. 7 2025. [code](https://github.com/technion-cs-nlp/PEAP).

#### Auto-intepretation

- [Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations](https://arxiv.org/pdf/2310.11207), Oct. 17 2023.
- [SelfIE: Self-Interpretation of Large Language Model Embeddings](https://arxiv.org/pdf/2403.10949), Mar. 26 2024.
- [Are self-explanations from Large Language Models faithful?](https://arxiv.org/pdf/2401.07927), May 16 2024.
- [FaithLM: Towards Faithful Explanations for Large Language Models](https://arxiv.org/pdf/2402.04678), Jun. 26 2024. [code](https://github.com/KindXiaoming/physics_of_skill_learning).
- [Language models can predict their own behavior](https://arxiv.org/pdf/2502.13329), Feb. 19 2025.
- [HyperDAS: Towards automatic mechanistic interpretability with hypernetworks](https://arxiv.org/pdf/2503.10894), Mar. 13 2025.

#### Complexity theory of transformers and beyond

- [LSTM recurrent networks learn simple context free and context sensitive languages](https://sferics.idsia.ch/pub/juergen/L-IEEE.pdf), 2001.
- [On the Practical Computational Power of Finite Precision RNNs for Language Recognition(https://arxiv.org/pdf/1805.04908), May 13 2018.
- [On the Turing completeness of mordern neural network architecture](https://arxiv.org/abs/1901.03429), Jan. 10 2019.
- [A Formal Hierarchy of RNN Architectures](https://arxiv.org/pdf/2004.08500), Sep. 19 2020.
- [Theoretical Limitations of Self-Attention in Neural Sequence Models](https://arxiv.org/abs/1906.06755), Feb. 12 2020.
- [Attention is Turing Complete](https://jmlr.org/papers/volume22/20-302/20-302.pdf), JMLR 2021.
- [What Formal Languages Can Transformers Express? A Survey](https://arxiv.org/pdf/2311.00208), Sep. 4 2024.
- [Transformers in DLOGTIME-Uniform TC0](https://arxiv.org/pdf/2409.13629), Sep. 2024.
- [On the practical computational power of finite precision rnns for language recognition](https://aclanthology.org/P18-2117.pdf), ACL 2018.
- [On Efficiently Representing Regular Languages as RNNs](https://aclanthology.org/2024.findings-acl.244.pdf), ACL 2024. [code](https://github.com/rycolab/bpdas).
- [InversionView: A general-purpose method for reading information from neural activations](https://arxiv.org/pdf/2405.17653), Nov. 2 2024. [code](https://github.com/huangxt39/InversionView).

#### Theory of Transformers

- [Hardness of approximate nearest neighbor search](https://arxiv.org/pdf/1803.00904), Mar. 2 2018.
- [Representational Strengths and Limitations of Transformers](https://arxiv.org/pdf/2306.02896), Nov. 16 2023.
- [A mathematical perspective on Transformers](https://arxiv.org/pdf/2312.10794), Aug. 12 2024.
- [How Transformers learn structured data: Insights from hierarchical filtering](https://arxiv.org/pdf/2408.15138), Aug. 27 2024.
- [Out-of-distribution generalization via composition: a lens through induction heads in Transformers](https://arxiv.org/pdf/2408.09503), Aug. 18 2024.
- [Transformers in DLOGTIME-Uniform TC0](https://arxiv.org/pdf/2409.13629), Sep. 20 2024.
- [Optimal Memorization Capacity of Transformers](https://arxiv.org/pdf/2409.17677), Sep. 26 2024.
- [Neural Networks Generalize on Low Complexity Data](https://arxiv.org/pdf/2409.17357), Sep. 25 2024.
- [Learning linear attention in polynomial time](https://arxiv.org/pdf/2410.10101), Oct. 18 2024.
- [Bilinear sequence regression: A model for learning from long sequences of high-dimensional tokens](https://arxiv.org/pdf/2410.18858), Oct. 24 2024. [code](https://github.com/SPOC-group/bilinear-sequence-regression).
- [Circuit Complexity Bounds for RoPE-based Transformer Architecture](https://arxiv.org/pdf/2411.07602), Nov. 12 2024.
- [Unraveling the Gradient Descent Dynamics of Transformers](https://arxiv.org/pdf/2411.07538), Nov. 12 2024.
- [One-Layer Transformer Provably Learns One-Nearest Neighbor In Context](https://arxiv.org/pdf/2411.10830), Nov. 16 2024.
- [When can transformers ground and compose: Insights from compositional generalization benchmarks](https://arxiv.org/pdf/2210.12786), Oct. 31 2022. [code](https://github.com/ankursikarwar/Grounded-Compositional-Generalization).
- [Theoretical limitations of multi-layer Transformer](https://arxiv.org/pdf/2412.02975), Dec. 4 2024.
- [Fundamental limits of learning in sequence multi-index models and deep attention networks: High-dimensional asymptotics and sharp thresholds](https://arxiv.org/pdf/2502.00901), Feb. 2 2025.
- [A unified perspective on the dynamics of deep transformers](https://arxiv.org/pdf/2501.18322), Jan. 30 2025.
- [Fundamental limits of learning in sequence multi-index models and deep attention neteworks: High-dimensional asymptotics and sharp thresholds](https://arxiv.org/pdf/2502.00901), Feb. 2 2025.
- [When do transformers outperform feedforward and recurrent networks? A statistical perspective](https://arxiv.org/pdf/2503.11272), Mar. 14 2025.
- [Disentangling feature structure: A mathematical provable two-stage training dynamics in Transformers](https://arxiv.org/pdf/2502.20681), Feb. 28 2025. [code](https://github.com/zx-gong/Two-Stage-Dynamics).

#### Theory of pre-training

- [A Law of Next-Token Prediction in Large Language Models](https://arxiv.org/pdf/2408.13442), Aug. 24 2024.
- [Non-asymptotic Convergence of Training Transformers for Next-token Prediction](https://arxiv.org/pdf/2409.17335), Sep. 25 2024.
- [Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism](https://arxiv.org/pdf/2409.17625), Sep. 26 2024.
- [Implicit Bias of Next-Token Prediction](https://arxiv.org/pdf/2402.18551), Feb. 28 2024.

#### Theory of icl and prompting

- [One-layer transformers fail to solve the induction heads task](https://arxiv.org/pdf/2408.14332), Aug. 26 2024.
- [Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods](https://arxiv.org/pdf/2408.14511), Aug. 25 2024.
- [In-Context Learning with Representations: Contextual Generalization of Trained Transformers](https://arxiv.org/pdf/2408.10147), Aug. 19 2024.
- [A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks](https://proceedings.mlr.press/v237/abernethy24a/abernethy24a.pdf), alt 2024.
- [Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers](https://arxiv.org/pdf/2409.17357), Sep. 25 2024.
- [Transformers as Algorithms: Generalization and Stability in In-context Learning](https://proceedings.mlr.press/v202/li23l/li23l.pdf), ICML 2023.
- [ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models](https://arxiv.org/pdf/2405.09220). May 27 2024.
- [Can in-context learning really generalize to out-of-distribution tasks?](https://arxiv.org/pdf/2410.09695), Oct. 13 2024.
- [Towards the effect of examples on in-context learning: A theoretical case study](https://arxiv.org/pdf/2410.09411), Oct. 12 2024.
- [Inference and Verbalization Functions During In-Context Learning](https://arxiv.org/pdf/2410.09349), Oct. 12 2024. [code](https://github.com/JunyiTao/infer-then-verbalize-during-icl).
- [How Transformers Implement Induction Heads: Approximation and Optimization Analysis](https://arxiv.org/pdf/2410.11474), Oct. 16 2024.
- [Context-scaling versus task-scaling in in-context learning](https://arxiv.org/pdf/2410.12783), Oct. 16 2024.
- [A theory of emergent in-context learning as implicit structure induction](https://arxiv.org/pdf/2303.07971), Mar. 14 2024.
- [Can generative ai solve your in-context learning problem? A martingale perspective](https://arxiv.org/pdf/2412.06033), Dec. 8 2024.
- [The dual-route model of induction](https://arxiv.org/pdf/2504.03022), Apr. 3 2025. [code](https://dualroute.baulab.info/).


