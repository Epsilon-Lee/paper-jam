
## LLMs related

### Topics

> Some big topics I really care about.

- [LLM safety](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/llm_safety.md).
- [LLM interpretability](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/llm_interpretability.md).
- [Knowledge updating](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/knowledge_updating.md).
- [Training and optimization](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/training_and_optimization.md).
- [Scaling laws](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/scaling_laws.md).
- [Reasoning](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/reasoning.md).
- [Data curation and mixing](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/data_curation_and_mixing.md).
- [Tech. reports](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/tech_reports.md).

### Buffered papers

- [Position: Levels of AGI for Operationalizing Progress on the Path to AGI](https://openreview.net/pdf?id=0ofzEysK2D), ICML 2024.
- [WILDCHAT: 1M CHATGPT INTERACTION LOGS IN THE WILD](https://arxiv.org/pdf/2405.01470), May 2 2024.
  - It is interesting to know the data distribution of queries to ChatGPT.
- [Lessons from the Trenches on Reproducible Evaluation of Language Models](https://arxiv.org/abs/2405.14782), May 23 2024. `evaluation`.
- [How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad](https://arxiv.org/pdf/2406.06467), Jun. 10 2024. `reasoning`.
- [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/pdf/2406.12031), Jun. 2024. `tabular llm`.
- [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/pdf/2406.11931), Jun. 17 2024. `tech report`.
- [Infinite Limits of Multi-head Transformer Dynamics](https://arxiv.org/pdf/2405.15712), May 24 2024. `learning dynamics`.
- [What Variables Affect Out-Of-Distribution Generalization in Pretrained Models?](https://arxiv.org/pdf/2405.15018), May 23 2024.
- [Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs](https://arxiv.org/pdf/2405.15485), May 24 2024. `reasoning`.
- [Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation](https://arxiv.org/pdf/2405.15302), May 24 2024. `reasoning`.
- [From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step](https://arxiv.org/pdf/2405.14838), May 23 2024. `reasoning`.
- [Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models](https://arxiv.org/pdf/2405.15143), May 24 2024. `reasoning` `agent`.
- [From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems](https://arxiv.org/pdf/2405.19883), May 30 2024. `agent`.
- [Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities](https://arxiv.org/pdf/2405.20003), May 30 2024. `uncertainty`.
- [A Language Model's Guide Through Latent Space](https://arxiv.org/pdf/2402.14433), Feb. 22 2024.
- [Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf), ICML 2023.
- [How Truncating Weights Improves Reasoning in Language Models](https://arxiv.org/pdf/2406.03068), Jun. 5 2024. `reasoning`.
- [Does learning the right latent variables necessarily improve in-context learning?](https://arxiv.org/pdf/2405.19162), May 29 2024.
- [Towards an empirical understanding of MoE design choices](https://arxiv.org/pdf/2402.13089), Feb. 20 2024. `moe`.
- [StructLM: Towards Building Generalist Models for Structured Knowledge Grounding](https://arxiv.org/pdf/2402.16671), Apr. 24 2024. `structure knowledge`.
- [Chronos: Learning the Language of Time Series](https://arxiv.org/pdf/2403.07815), May 2 2024.
- [On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability](https://arxiv.org/pdf/2405.16845), May 27 2024.
- [Theoretical Analysis of Weak-to-Strong Generalization](https://arxiv.org/pdf/2405.16043), May 25 2024.
- [Scaling and evaluating sparse autoencoders](https://arxiv.org/pdf/2406.04093), Jun. 6 2024.
- [Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe](https://arxiv.org/pdf/2406.04165), Jun. 6 2024. `llm for embedding`.
- [miniCodeProps: a Minimal Benchmark for Proving Code Properties](https://arxiv.org/pdf/2406.11915), Jun. 16 2024. `coding` `benchmark`.
- [What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering](https://arxiv.org/pdf/2406.12334), Jun. 18 2024. `prompt engineering`.
- [QOG:Question and Options Generation based on Language Model](https://arxiv.org/pdf/2406.12381), Jun. 18 2024. `data synthesis`.
- [CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society](https://arxiv.org/pdf/2303.17760), Nov. 2 2023. `multi-agent`.
- [Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models](https://arxiv.org/pdf/2403.19340), Mar. 28 2024. `data pipeline`.
- [Can language model explain their own classification behavior?](https://arxiv.org/pdf/2405.07436), May 13 2024.
- [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221), Jul. 11 2022.
- [xVal: A Continuous Number Encoding for Large Language Models](https://arxiv.org/abs/2310.02989), Oct. 4 2023.
- [Memory Mosaics](https://arxiv.org/pdf/2405.06394), May 10 2024.
- [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/pdf/2404.01413), Apr. 29 2024. `synthetic data`.
  - _"accumulating the successive generations of synthetic data alongside the original data avoids model collapse"_
- [An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247), May 27 2024. `tutorial`.
- [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/pdf/2402.18563), Feb. 28 2024.
- [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://arxiv.org/pdf/2403.06833), Jun. 3 2024.
- [Data-Centric AI in the Age of Large Language Models](https://arxiv.org/pdf/2406.14473), Jun. 20 2024. `data-centric`.
- [FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery](https://arxiv.org/pdf/2211.08316), May 11 2023.
- [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/pdf/2402.03049), Mar. 21 2024.
- [Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization](https://arxiv.org/pdf/2405.15071), May 27 2024. `mechanistic interpretability`.
- [Theoretical Analysis of Weak-to-Strong Generalization](https://arxiv.org/pdf/2405.16043), May 25 2024.
- [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804), Feb. 6 2024.
- [Machine learning and information theory concepts towards an AI mathematician](https://www.ams.org/journals/bull/2024-61-03/S0273-0979-2024-01839-4/S0273-0979-2024-01839-4.pdf), May 15 2024. `math reasoning`.
- [Monitoring Latent World States in Language Models with Propositional Probes](https://arxiv.org/pdf/2406.19501), Jun. 27 2024.
- [End-To-End Causal Effect Estimation from Unstructured Natural Language Data](https://arxiv.org/pdf/2407.07018), Jul. 9 2024. `causal inference x llms`.
- [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/pdf/2407.04620), Jul. 5 2024. `architectural inductive bias`.
- [How Does Quantization Affect Multilingual LLMs?](https://arxiv.org/pdf/2407.03211), Jul. 3 2024. `quantization`.
- [Chain-of-Thought Reasoning without Prompting](https://arxiv.org/pdf/2402.10200), May 23 2024. `reasoning`.
- [Transformer Alignment in Large Language Models](https://arxiv.org/pdf/2407.07810), Jul. 10 2024.
- [SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/pdf/2407.09025), Jul. 12 2024. `structured knowledge`.
- [LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data](https://arxiv.org/pdf/2407.11418), Jul. 16 2024.
- [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/pdf/2311.17035), Nov. 28 2023. `data extraction`.
- [The Foundations of Tokenization: Statistical and Computational Concerns](https://arxiv.org/pdf/2407.11606), Jul. 16 2024.
- [Representing Rule-based Chatbots with Transformers](https://arxiv.org/pdf/2407.10949), Jul. 15 2024.
- [Learning to Compile Programs to Neural Networks](https://arxiv.org/pdf/2407.15078), Jul. 21 2024.
- [Faithfulness Measurable Masked Language Models](https://arxiv.org/abs/2310.07819), May 9 2024.
- [Fast Matrix Multiplications for Lookup Table-Quantized LLMs](https://arxiv.org/pdf/2407.10960), Jul. 15 2024.
- [On the Benefits of Rank in Attention Layers](https://arxiv.org/pdf/2407.16153), Jul. 24 2024.
  - The question to tackle: _"hyperparameters such as the rank of the attention matrices and the number of heads are scaled nearly the same way in all realizations of this architecture, without theoretical justification"_
- [Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](https://arxiv.org/pdf/2407.16607), Jul. 23 2024.
- [Perceptions of linguistic uncertainty by language models and humans](https://arxiv.org/pdf/2407.15814), Jul. 22 2024.
- [Internal Consistency and Self-Feedback in Large Language Models](https://arxiv.org/pdf/2407.14507), Jul. 19 2024.
- [u-µP: The Unit-Scaled Maximal Update Parametrization](https://arxiv.org/pdf/2407.17465), Jul. 24 2024.
- [Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications](https://arxiv.org/pdf/2407.19262), Jul. 27 2024. `learning dynamics`.
- [Do Language Models Have a Critical Period for Language Acquisition?](https://arxiv.org/pdf/2407.19325), Jul. 27 2024. `learning dynamics`.
- [From Pre-training Corpora to Large Language Models: What Factors Influence LLM Performance in Causal Discovery Tasks?](https://arxiv.org/pdf/2407.19638), Jul. 29 2024. `knowledge extraction`, `causal knowledge`.
- [Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment](https://arxiv.org/pdf/2407.19346), Jul. 27 2024.
- [Understanding the Relationship between Prompts and Response Uncertainty in Large Language Models](https://arxiv.org/pdf/2407.14845), Jul. 20 2024. related to `knowledge extraction`.
  - _"We show that the uncertainty decreases as the prompt's informativeness increases, similar to epistemic uncertainty"_
- [Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences](https://arxiv.org/pdf/2407.09499), Jun. 12 2024. `synthetic data` `theory`.
- [Can LLMs predict the convergence of Stochastic Gradient Descent?](https://arxiv.org/pdf/2408.01736), Aug. 3 2024.
- [STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs](https://arxiv.org/pdf/2408.01803), Aug. 3 2024. `efficient llm`.
- [Self-Taught Evaluators](https://arxiv.org/pdf/2408.02666), Aug. 8 2024. `self-eval`.
- [A Survey of Mamba](https://arxiv.org/pdf/2408.01129), Aug. 2 2024.
- [Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities](https://arxiv.org/pdf/2408.07666), Aug. 14 2024.
- [A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning](https://arxiv.org/pdf/2408.07057), Aug. 13 2024.
- [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/pdf/2408.06292), Aug. 12 2024.
- [Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters](https://arxiv.org/pdf/2408.04093), Aug. 9 2024.
- [FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers](https://arxiv.org/pdf/2408.04816), Aug. 9 2024.
- [Natural Language Outlines for Code: Literate Programming in the LLM Era](https://arxiv.org/pdf/2408.04820), Aug. 9 2024.
- [Can a Bayesian Oracle Prevent Harm from an Agent?](https://arxiv.org/pdf/2408.05284), Aug. 9 2024.
- [Low-Rank Approximation, Adaptation, and Other Tales](https://arxiv.org/pdf/2408.05883), Aug. 12 2024.
- [Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models](https://arxiv.org/pdf/2408.08210), Aug. 15 2024.
- [BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts](https://arxiv.org/pdf/2408.08274), Aug. 15 2024.
- [Out-of-Distribution Learning with Human Feedback](https://arxiv.org/pdf/2408.07772), Aug. 14 2024.
- [Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models](https://arxiv.org/pdf/2408.10189), Aug. 19 2024.
- [KAN 2.0: Kolmogorov-Arnold Networks Meet Science](https://arxiv.org/pdf/2408.10205), Aug. 19 2024.
- [Learning Randomized Algorithms with Transformers](https://arxiv.org/pdf/2408.10818), Aug. 20 2024.
- [Classifier-Free Guidance is a Predictor-Corrector](https://arxiv.org/pdf/2408.09000), Aug. 23 2024.
- [What can Large Language Models Capture about Code Functional Equivalence?](https://arxiv.org/pdf/2408.11081), Aug. 20 2024.
- [Large Language Models for Supply Chain Optimization](https://arxiv.org/pdf/2307.03875), Jul. 13 2023.
  - [HarnessingAI and LLMs to Revolutionize Retail and Supply Chain Management](https://www.pacificdataintegrators.com/hubfs/Website-Whitepapers/Harnessing%20AI%20and%20LLMs%20to%20Revolutionize%20Retail%20and%20Supply%20Chain%20Management-PDF.pdf).
  - [OptiGuide](https://github.com/microsoft/OptiGuide).
  - [Generative AI In Supply Chain](https://www.alvarezandmarsal.com/sites/default/files/article/pdf/Generative%20AI%20in%20Supply%20Chain%20Report%20-%20Compressed%20version.pdf).
- [Performative Prediction on Games and Mechanism Design](https://arxiv.org/pdf/2408.05146), Aug. 9 2024.
- [FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers](https://arxiv.org/pdf/2408.04816), Aug. 9 2024. `COLM2024`. [code](https://github.com/jnwilliams/FUSE_prompt_inversion).
  - _"we propose FUSE (Flexible Unification of Semantic Embeddings), an inexpensive approach to approximating an adapter layer that maps from one model's textual embedding space to another, even across different tokenizers."_
- [COGEN: Learning from Feedback with Coupled Comprehension and Generation](https://arxiv.org/pdf/2408.15992), Aug. 28 2024.
- [SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning](https://arxiv.org/pdf/2408.05517), Aug. 19 2024.
- [The Unbearable Slowness of Being](https://arxiv.org/pdf/2408.10234), Aug. 3 2024.
- [An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models](https://arxiv.org/pdf/2408.00724), Aug. 1 2024.
  - _"We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped wiht more sophisticated decoding algorithms in budget-constrained scenarios, e.g. on-devices, to enhance problem-solving accuracy."_
  - [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/pdf/2406.16838), Jun. 24 2024.
- [Reframing Data Value for Large Language Models Through the Lens of Plausability](https://arxiv.org/pdf/2409.00284), Aug. 30 2024.
- [Extracting Paragraphs from LLM Token Activations](https://arxiv.org/pdf/2409.06328), Sep. 10 2024.
- [Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving](https://arxiv.org/pdf/2405.12205), May 20 2024.
- [Instruct-SkillMix: A powerful pipeline for llm instruction tuning](https://www.arxiv.org/pdf/2408.14774v2), Sep. 9 2024.
- [MambaByte: Token-free Selective State Space Model](https://arxiv.org/pdf/2401.13660), Aug. 9 2024.
- [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/pdf/2402.04333), Jun. 13 2024.
- [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/pdf/2409.12917), Sep. 19 2024.
- [ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty](https://arxiv.org/pdf/2408.14339), Aug. 26 2024.
- [GRIN: GRadient-INformed MoE](https://arxiv.org/pdf/2409.12136), Sep. 18 2024.
- [QWen2.5-Math technical report: Towards mathematical expert model via self-improvement](https://arxiv.org/pdf/2409.12122), Sep. 18 2024.
- [A Controlled Study on Long Context Extension and Generalization in LLMs](https://arxiv.org/pdf/2409.12181), Sep. 23 2024. `long-context`.
- [Apple Intelligence Foundation Language Models](https://arxiv.org/pdf/2407.21075), Jul. 29 2024.
- [Can Large Language Models Unlock Novel Scientific Research Ideas?](https://arxiv.org/pdf/2409.06185), Sep. 10 2024.
- [(Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models](https://arxiv.org/pdf/2409.02628), Sep. 4 2024.
- [Quantifying Emergence in Neural Networks: Insights from Pruning and Training Dynamics](https://arxiv.org/pdf/2409.01568), Sep. 3 2024.
- [Hypothesizing Missing Causal Variables with LLMs](https://arxiv.org/pdf/2409.02604), Sep. 4 2024.
- [NUDGE: Lightweight non-parametric fine-tuning of embeddings for retrieval](https://arxiv.org/pdf/2409.02343), Sep. 4 2024.
- [Unforgettable Generalization in Language Models](https://arxiv.org/pdf/2409.02228), Sep. 3 2024.
- [Inductive Learning of Logical Theories with LLMs: A Complexity-graded Analysis](https://arxiv.org/pdf/2408.16779), Aug. 15 2024.
- [Configurable Foundation Models: Building LLMs from a Modular Perspective](https://arxiv.org/pdf/2409.02877), Sep. 4 2024.
- [Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data](https://arxiv.org/pdf/2409.00096), Aug. 27 2024.
- [A Formal Hierarchy of RNN Architectures](https://blog.allenai.org/a-formal-hierarchy-of-rnn-architectures-94c9d47566b5), Apr. 22 2020.
- [Proof Automation with Large Language Models](https://arxiv.org/pdf/2409.14274), Sep. 22 2024.
- [Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers](https://arxiv.org/pdf/2409.04109), Sep. 6 2024.
- [On the Computational Benefit of Multimodal Learning](https://proceedings.mlr.press/v237/lu24a/lu24a.pdf), alt 2024.
- [Reranking Laws for Language Generation: A Communication-Theoretic Perspectiv](https://arxiv.org/pdf/2409.07131), Sep. 11 2024.
- [Improving pretraining data using perplexity correlations](https://arxiv.org/pdf/2409.05816), Sep. 9 2024.
- [Explaining Datasets in Words: Statistical Models with Natural Language Parameters](https://arxiv.org/abs/2409.08466), Sep. 13 2024.
- [Language Models “Grok” to Copy](https://www.arxiv.org/pdf/2409.09281), Sep. 14 2024.
- [Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models](https://arxiv.org/pdf/2409.17455), Sep. 26 2024.
- [Order of Magnitude Speedups for LLM Membership Inference](https://arxiv.org/pdf/2409.14513), Sep. 24 2024.
- [Estimating Wage Disparities Using Foundation Models](https://www.arxiv.org/pdf/2409.09894), Sep. 15 2024.
- [Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates](https://arxiv.org/pdf/2402.18540), Feb. 28 2024.
- [Safety Alignment Should Be Made More Than Just a Few Tokens Deep](https://arxiv.org/pdf/2406.05946), Jun. 10 2024.
- [Can Models Learn Skill Composition from Examples?](https://openreview.net/pdf?id=YEEsRgkvnU), NeurIPS 2024.
- [See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses](https://arxiv.org/abs/2408.08978#), Aug. 16 2024.
- [Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models](https://arxiv.org/abs/2408.06663), Aug. 13 2024.
- [A Taxonomy for Data Contamination in Large Language Models](https://arxiv.org/abs/2407.08716), Jul. 11 2024.
- [LLM Pruning and Distillation in Practice: The Minitron Approach](https://d1qx31qr3h6wln.cloudfront.net/publications/minitron_tech_report.pdf), Aug. 21 2024.
- [Realistic evaluation of model merging for compositional generalization](https://arxiv.org/pdf/2409.18314), Sep. 26 2024.
- [Transformer Memory as a Differentiable Search Index](https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf), NeurIPS 2022. [github](https://github.com/ArvinZhuang/DSI-transformers).
  - [DSI++: Updating Transformer Memory with New Documents](https://aclanthology.org/2023.emnlp-main.510.pdf), EMNLP 2023.
- [Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers](https://arxiv.org/pdf/2409.20537), Sep. 30 2024. `robotics foundation model`. NeurIPS 2024.
  - _TL;DR: "HPT aligns different embodiment to a shared latent space and investigates the scaling behaviors in policy learning. Put a scalable transformer in the middle of your policy and don't train from scratch!"_
- [Geometric signatures of compositionality across a language model's lifetime](https://arxiv.org/pdf/2410.01444), Oct. 7 2024. `compositionality`.
- [Dont' cut corners: Exact conditions for modularity in biologically inspired representations](https://arxiv.org/pdf/2410.06232v1), Oc.t 8 2024. [code](https://github.com/kylehkhsu/dont-cut-corners/). `compositionality`.
- [Differential transformers](https://arxiv.org/pdf/2410.05258), Oct. 7 2024.
- [LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch](https://arxiv.org/pdf/2410.13213), Oct. 17 2024.
- [A simple baseline for predicting events with auto-regressive tabular transformers](https://arxiv.org/pdf/2410.10648), Oct. 14 2024.
- [DARE the Extreme Ð: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://arxiv.org/pdf/2410.09344), Oct. 12 2024. `pruning`.
- [LoLCATs: On Low-Rank Linearizing of Large Language Models](https://arxiv.org/pdf/2410.10254), Oct. 14 2024. [code](https://github.com/HazyResearch/lolcats).
- [Machines of Loving Grace](https://darioamodei.com/machines-of-loving-grace), Oct. 2024.
- [Predicting Emergent Capabilities by Finetuning](https://openreview.net/pdf?id=vL8BIGuFTF), COLM 2024.
- [End-to-End Ontology Learning with Large Language Models](https://arxiv.org/pdf/2410.23584), Oct. 31 2024. [code](https://github.com/andylolu2/ollm).
- [Language Models as Causal Effect Generators](https://arxiv.org/pdf/2411.08019), Nov. 12 2024. [code](https://github.com/lbynum/sequence-driven-scms).
- [On the limits of language generation: Trade-offs between hallucination and mode collapse](https://arxiv.org/pdf/2411.09642), Nov. 14 2024.
- [SEQ-VCR: Preventing collapse in intermediate transformer representations for enhanced reasoning](https://arxiv.org/pdf/2411.02344), Nov. 4 2024. [code](https://github.com/rarefin/seq_vcr).
- [Generative Agent Simulations of 1,000 People](https://arxiv.org/pdf/2411.10109), Nov. 2024.
- [Does Prompt Formatting Have Any Impact on LLM Performance?](https://arxiv.org/pdf/2411.10541), Nov. 15 2024.
- [Efficient Alignment of Large Language Models via Data Sampling](https://arxiv.org/pdf/2411.10545), Nov. 15 2024.
- [Language-to-Code Translation with a Single Labeled Example](https://aclanthology.org/2024.emnlp-main.462.pdf), 2024.
- [Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis](https://aclanthology.org/2024.findings-emnlp.547.pdf), EMNLP 2024. [code](https://github.com/cogito233/causal-sa).
- [To Believe or Not to Believe Your LLM: Iterative Prompting for Estimating Epistemic Uncertainty](https://openreview.net/pdf?id=k6iyUfwdI9), NeurIPS 2024.
- [Visual autoregressive modeling: Scalable image generation via next-scale prediction](https://arxiv.org/pdf/2404.02905), Jun. 10 2024.
- [Machine unlearning doesn't do what you think: Lessons for generative AI policy, research and practice](https://arxiv.org/pdf/2412.06966), Dec. 9 2024. `machine unlearning`.
- [Normalizing flows are capable generative models](https://arxiv.org/pdf/2412.06329), Dec. 10 2024. [code](https://github.com/apple/ml-tarflow).
- [Toward ai-driven digital organism: A system of multiscale foundation models for predicting, simulating and programming biology at all levels](https://www.cs.cmu.edu/~epxing/papers/2025/AIDO.pdf), Nov. 26 2024.
- [Memory Layers at Scale](https://arxiv.org/pdf/2412.09764), Dec. 12 2024. [code](https://github.com/facebookresearch/memory).
- [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/pdf/2412.13663), Dec. 2024.
- [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/pdf/2312.02780), Dec. 5 2023
  - [A Universal Law of Robustness via Isoperimetry](https://arxiv.org/pdf/2105.12806), Dec. 23 2024.
- [LearnLM: Improving Gemini for Learning](https://arxiv.org/pdf/2412.16429), Dec. 25 2024.
- [The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources](https://openreview.net/pdf?id=tH1dQH20eZ), TMLR 2024.
- [Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts](https://arxiv.org/pdf/2409.13728), Oct. 24 2024. [code](https://github.com/meszarosanna/rule_extrapolation).
- [Complexity control facilitates reasoning-based compositional generalization in transformers](https://arxiv.org/pdf/2501.08537), Jan. 15 2025.
- [Foundations of Large Language Models](https://arxiv.org/pdf/2501.09223), Jan. 16 2025.
- [Enhancing Lexicon-Based Text Embeddings with Large Language Models](https://arxiv.org/pdf/2501.09749), Jan. 16 2025.
- [Critique fine-tuning: Learning to critique is more effective than learning to imitate](https://arxiv.org/pdf/2501.17703), Jan. 29 2025.
- [Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery](https://proceedings.neurips.cc/paper_files/paper/2023/file/a00548031e4647b13042c97c922fadf1-Paper-Conference.pdf), NeurIPS 2023.
- [Automatic prompt optimization with "gradient descent" and beam search](https://arxiv.org/pdf/2305.03495), Oct. 19 2023.
- [Large language models as optimizers](https://arxiv.org/pdf/2309.03409), Apr. 15 2024.
- [Connecting large language models with evolutionary algorithms yields powerful prompt optimizers](https://arxiv.org/pdf/2309.08532), Feb. 27 2024.
- [Tuning LLM judges hyperparameters](https://arxiv.org/pdf/2501.17178), Jan. 24 2025.
- [LLM-Rubric: A multidimensional, calibrated approach to automated evaluation of natural language texts](https://arxiv.org/pdf/2501.00274), Dec. 31 2025. [code](https://github.com/microsoft/llm-rubric).
- [Great models think alike and this undermines AI oversight](https://arxiv.org/pdf/2502.04313), Feb. 6 2025. [code](https://github.com/model-similarity/lm-similarity).
- [Algorithmic causal structure emerging through compression](https://arxiv.org/pdf/2502.04210), Feb. 6 2025.
- [Reflection-window decoding: Text generation with selective refinement](https://arxiv.org/pdf/2502.03678), Feb. 5 2025.
- [Verify with caution: The pitfalls of relying on imperfect factuality metrics](https://arxiv.org/pdf/2501.14883), Jan. 30 2025.
- [SmolLM2: When smol goes big - Data-centric training of small language model](https://arxiv.org/pdf/2502.02737), Feb. 4 2025.
  - _"we release both SmolLM2 as well as all of the datasets we prepared in the course of this project"_
- [ExLM: Rethinking the impact of MASK tokens in masked language models](https://arxiv.org/pdf/2501.13397), Jan. 23 2025.
- [Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection](https://arxiv.org/pdf/2502.06042), Feb. 9 2025.
- [LM2: Large Memory Models](https://arxiv.org/pdf/2502.06049), Feb. 9 2025.
- [Economics of sourcing human data](https://arxiv.org/pdf/2502.07732), Feb. 11 2025.
- [Auditing prompt caching in language model APIs](https://arxiv.org/pdf/2502.07776), Feb. 11 2025.
- [Turning up the heat: Min-p sampling for creative and coherent LLM outputs](https://arxiv.org/pdf/2407.01082), Oct. 13 2024.
- [Preventing rogue agents improves multi-agent collaboration](https://arxiv.org/abs/2502.05986), Feb. 9 2025.
- [Automated capability discovery via foundation model self-exploration](https://arxiv.org/pdf/2502.07577), Feb. 11 2025.
  - [Quality-diversity through AI feedback](https://openreview.net/pdf?id=owokKCrGYr), ICLR 2024.
- [The hyperfitting phenomenon: Sharpening and stabilizing LLMs for open-ended text generation](https://arxiv.org/pdf/2412.04318), Dec. 5 2024.
- [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/pdf/2502.09604), Feb. 13 2025.
- [Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs](https://arxiv.org/pdf/2502.08640), Feb. 12 2025.
- [Rethinking mixture-of-agents: Is mixing different large language models beneficial](https://arxiv.org/pdf/2502.00674), Feb. 2 2025.
- [Rewind-to-Delete: Certified machine unlearning for nonconvex functions](https://arxiv.org/pdf/2409.09778), Jan. 31 2025.
  - Related theoretic works on machine unlearning
    - [Remember What You Want to Forget: Algorithms for Machine Unlearning](https://arxiv.org/abs/2103.03279), 2021.
- [Discrepancies are virtue: Weak-to-strong generalization through lens of intrinsic dimensions](https://arxiv.org/pdf/2502.05075), Feb. 7 2025.
  - [Scalable oversight and weak-to-strong generalization: Compatible approaches to the same problem](https://www.alignmentforum.org/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization), Dec. 2023.
  - [A guide to iterated amplification & debate](https://www.alignmentforum.org/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate), Nov. 16 2020.
- [Self-regulation and requesting interventions](https://arxiv.org/pdf/2502.04576), Feb. 7 2025.
- [Active task disambiguation with LLMs](https://arxiv.org/pdf/2502.04485), Feb. 6 2025.
- [Aligning black-box language models with human judgements](https://arxiv.org/pdf/2502.04997), Feb. 7 2025.
- [Large language diffusion models](https://arxiv.org/pdf/2502.09992), Feb. 14 2025. [code](https://github.com/ML-GSAI/LLaDA).
- [Independence tests for language models](https://arxiv.org/pdf/2502.12292), Feb. 17 2025.
- [From RAG to memory: Non-parametric continual learning for large language models](https://arxiv.org/pdf/2502.14802), Feb. 20 2025. [code](https://github.com/OSU-NLP-Group/HippoRAG).
- [Towards an ai co-scientist](https://arxiv.org/pdf/2502.18864), Feb. 26 2025.
- [An overview of large language models for statisticians](https://arxiv.org/pdf/2502.17814), Feb. 25 2025.
- [Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction](https://arxiv.org/pdf/2502.17541), Feb. 24 2025. [code](https://github.com/MichalBravansky/dataset-featurization).
- [Build a Minimal Transformer from Scratch](https://www.k-a.in/llm3.html), `blogpost`.

### LLM as a judge

- [Judging llm-as-a-judge with mt-bench and chatbot](https://arxiv.org/abs/2306.05685), Jun. 9 2023.
- [From generation to judgement: Opportunities and challenges of LLM-as-a-judge](https://arxiv.org/pdf/2411.16594), Nov. 2024.
- [A survey on LLM-as-a-judge](https://arxiv.org/pdf/2411.15594), Nov. 2024.
- [LLMs-as-judges: A comprehensive survey on llm-based evaluation methods](https://arxiv.org/pdf/2412.05579), Dec. 10 2024.
- [Preference leakage: A contamination problem in LLM-as-a-judge](https://arxiv.org/pdf/2502.01534), Feb. 3 2025.
- [Investigating non-transitivity in LLM-as-a-judge](https://arxiv.org/pdf/2502.14074), Feb. 19 2025.
- [Crowd comparative reasoning: Unlocking comprehensive evaluation for LLM-as-a-judge](https://arxiv.org/pdf/2502.12501), Feb. 18 2025.
- [LLM Juries for Evaluation](https://www.comet.com/site/blog/llm-juries-for-evaluation/), `blogpost`.

### Length generalization

- [Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers](https://arxiv.org/pdf/2408.05506), Aug. 10 2024.
- [A formal framework for understanding length generalization in transformers](https://arxiv.org/pdf/2410.02140), Oct. 3 2024.
- [Self-improving transformers overcome easy-to-hard and length generalization challengs](https://arxiv.org/pdf/2502.01612), Feb. 3 2025.
- [NoLiMa: Long-context evaluation beyond literal matching](https://arxiv.org/pdf/2502.05167), Feb. 7 2025.
- [LongPO: Long context self-evolution of large language models through short-to-long preference optimization](https://www.arxiv.org/pdf/2502.13922), ICLR 2025. [tweet](https://x.com/gzchen3/status/1892870929966883236). [code](https://github.com/DAMO-NLP-SG/LongPO).
- [How to train long-context language models (effectively)](https://arxiv.org/pdf/2410.02660), Oct. 3 2024.
- [LIFT: Improving long context understanding of large language models through long input fine-tuning](https://arxiv.org/pdf/2502.14644), Feb. 2025.
- [The Role of Sparsity for Length Generalization in Transformers](https://arxiv.org/pdf/2502.16792), Feb. 24 2025.

### In-context learning

- [In-context Learning and Induction Heads](https://arxiv.org/pdf/2209.11895), Mar. 8 2022. `in-context-learning`.
- [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://arxiv.org/pdf/2110.10090), Jun. 24 2024. `variable mechanism`.
- [Is Mamba Capable of In-Context Learning?](https://arxiv.org/pdf/2402.03170v2), Apr. 24 2024.
- [Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs](https://arxiv.org/pdf/2405.11880), May 20 2024. `icl`.
- [MLPs Learn In-Context](https://arxiv.org/pdf/2405.15618), May 24 2024. `icl`.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `in-context learning`.
- [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://arxiv.org/pdf/2405.19874), May 30 2024. `icl`.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `icl`.
- [Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning](https://arxiv.org/pdf/2406.11890), Jun. 14 2024. `icl`.
- [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/pdf/2406.11233), Jun. 17 2024. `icl`.
- [Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning](https://arxiv.org/pdf/2406.14022), Jun. 20 2024. `in-context learning`.
- [Transformers are Universal In-context Learners](https://arxiv.org/pdf/2408.01367), Aug. 2 2024. `in-context learning`.
- [Memorization in in-context learning](https://arxiv.org/pdf/2408.11546), Aug. 21 2024.
- [Divide, reweight, and conquer: A logit arithmetic approach for in-context learning](https://arxiv.org/pdf/2410.10074), Oct. 14 2024. [code](https://github.com/Chengsong-Huang/LARA).
- [Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models](https://arxiv.org/pdf/2410.09701), Oct. 13 2024.
- [Re-examing learning linear functions in context](https://arxiv.org/pdf/2411.11465), Nov. 18 2024.
- [Many-Shot In-Context Learning](https://arxiv.org/pdf/2404.11018), Oct. 17 2024.
- [Multi-Attribute Constraint Satisfaction via Language Model Rewriting](https://arxiv.org/abs/2412.19198), Dec. 26 2024.
- [Task vectors in in-context learning: Emergence, formation, and benefits](https://arxiv.org/pdf/2501.09240), Jan. 16 2025.
- [The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities](https://arxiv.org/pdf/2501.08716), Jan. 15 2025.
- [Rapid word learning through meta in-context learning](https://arxiv.org/pdf/2502.14791), Feb. 20 2024.
- [In-Context Learning with Long-Context Models: An In-Depth Exploration](https://arxiv.org/abs/2405.00200), Apr. 30 2024.

### Prompting techniques

- [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608), Jun. 6 2024. `prompting`.
- [A survey of prompt engineering methods in large language models for different nlp tasks](https://arxiv.org/pdf/2407.12994), Jul. 17 2024.

### LLMs for traditional nlp tasks

- [Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study](https://arxiv.org/pdf/2304.04339), Feb. 17 2024.
- [Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT](https://arxiv.org/pdf/2302.10198), Mar. 2 2023.
- [Whitening Not Recommended for Classification Tasks in LLMs](https://arxiv.org/pdf/2407.12886), Jul. 16 2024.

### Efficient training and inference

- [Fitting larger networks into memory](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9), Jan. 14 2018. `gradient checkpointing`. [github](https://github.com/cybertronai/gradient-checkpointing). [pytorch](https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb).
- [Triton Puzzles](https://github.com/srush/Triton-Puzzles), Triton tutorial.
- [Flora: Low-Rank Adapters Are Secretly Gradient Compressors](https://arxiv.org/abs/2402.03293), Feb. 5 2024. `efficient training`. [github](https://github.com/BorealisAI/flora-opt). [blogpost](https://www.borealisai.com/research-blogs/pre-training-multi-billion-parameter-llms-on-a-single-gpu-with-flora/#Incorporating_FLORA_into_your_code).
- [A visual guide to quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7common-data-types), Jul. 22 2024.
- [Foundation of large language model compression - Part 1: Weight quantization](https://arxiv.org/pdf/2409.02026), Sep. 2024.

### Evaluation

- [Evaluating language models as risk scores](https://arxiv.org/pdf/2407.14614), Jul. 19 2024.
- [Training on the test task confounds evaluation and emergence](https://arxiv.org/pdf/2407.07890?), Jul. 10 2024. [code](https://github.com/socialfoundations/training-on-the-test-task).
- [Language model developers should report train-test overlap](https://arxiv.org/abs/2410.08385), Oct. 10 2024. [code](https://github.com/stanford-crfm/data-overlap).
- [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/pdf/2411.00640), Nov. 1 2024.
- [Evaluating Generative AI Systems is a Social Science Measurement Challenge](https://arxiv.org/pdf/2411.10939), Nov. 7 2024.
- [Causality can systematically address the monsters under the bench(marks)](https://arxiv.org/pdf/2502.05085), Feb. 7 2025.
- [How contaminated is your benchmark? Quantifying dataset leakage in large language models with kernel divergence](https://arxiv.org/pdf/2502.00678), Feb. 2 2025.

#### Data contamination

- [Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation](https://arxiv.org/pdf/2406.14644), Jun. 20 2024.
- [Accuracy is not all you need](https://arxiv.org/pdf/2407.09141), Jul. 12 2024.
  - The paper dubs synthetic data from LLMs as generative teaching.

#### Benchmark

- [From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline](https://arxiv.org/pdf/2406.11939), Jun. 17 2024. `benchmark`.
- [Training on the Test Task Confounds Evaluation and Emergence](https://arxiv.org/pdf/2407.07890), Jul. 10 2024.
  - _"We argue that the seeming superriority of one model family over another may be explained by a different degree of training on the test task."_
- [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/pdf/2403.07008), May 28 2024.
- [Benchmarking Complex Instruction-Following with Multiple Constraints Composition](https://arxiv.org/pdf/2406.14491), Jul. 4 2024. [github](https://github.com/thu-coai/ComplexBench).
- [GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2402.19255), Jul. 2 2024.
- [metabench A Sparse Benchmark to Measure General Ability in Large Language Models](https://arxiv.org/pdf/2407.12844), Jul. 4 2024. [code](https://github.com/adkipnis/metabench).
- [AutoBencher: Creating Salient, Novel, Difficult Datasets for Language Models](https://arxiv.org/abs/2407.08351), Jul. 11 2024.
- [Benchmark agreement testing done right: A guide to llm benchmark evaluation](https://arxiv.org/pdf/2407.13696), Jul. 18 2024.
- [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/pdf/2408.08926), Aug. 15 2024.
- [Benchmarking Large Language Models for Math Reasoning Tasks](https://arxiv.org/pdf/2408.10839), Aug. 20 2024.
- [Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization](https://arxiv.org/pdf/2409.18433), Sep. 27 2024.
- [MLE-Bench: Evaluating machine learning agents on machine learning engineering](https://arxiv.org/pdf/2410.07095), Oct. 9 2024. `openai`.
- [HARDMath: A benchmark dataset for challenging problems in applied mathematics](https://arxiv.org/pdf/2410.09988), Oct. 13 2024.
- [SuperGPQA: Scaling LLM evaluation across 285 graduate disciplines](https://arxiv.org/pdf/2502.14739), Feb. 20 2025.

#### Eval toolkit

- [Foundation Model Evaluations Library](https://github.com/aws/fmeval), [paper](https://arxiv.org/pdf/2407.12872), Jul. 15 2024.
- [UltraEval](https://github.com/OpenBMB/UltraEval).
- [simple-evals](https://github.com/openai/simple-evals), OpenAI simple-evals.

### Agent

- [Agentic Workflow新范式，基于大语言模型的工作流、业务流程、智能体大融合【附十篇相关论文】](https://mp.weixin.qq.com/s/i9QB_OtUboHnoZOKn-oKmA), Aug. 5 2024.
- [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/pdf/2406.04692), Jun. 7 2024. `agent`.
- [TextGrad: Automatic "Differentiation" via Text](https://arxiv.org/abs/2406.07496), Jun. 11 2024. `agent`.
- [Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence](https://arxiv.org/pdf/2407.07061), Jul. 10 2024.
- [On the Design and Analysis of LLM-Based Algorithms](https://arxiv.org/pdf/2407.14788), Jul. 20 2024.
- [Recursive Introspection: Teaching Language Model Agents How to Self-Improve](https://arxiv.org/pdf/2407.18219), Jul. 25 2024.
- [ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems](https://arxiv.org/pdf/2408.02248), Aug. 5 2024.
- [ChatDev: Communicative Agents for Software Development](https://arxiv.org/pdf/2307.07924), Jun. 5 2024.
- [OpenDevin: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/pdf/2407.16741), Jul. 23 2024.
- [RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation](https://arxiv.org/pdf/2408.02545), Aug. 5 2024. [code](https://github.com/IntelLabs/RAGFoundry).
- [Automated Design of Agentic Systems](https://arxiv.org/pdf/2408.08435), Aug. 15 2024. [code](https://github.com/ShengranHu/ADAS).
- [Agent Workflow Memory](https://arxiv.org/pdf/2409.07429), Sep. 11 2024.
- [xLAM: A Family of Large Action Models to Empower AI Agent Systems](https://arxiv.org/pdf/2409.03215), Sep. 5 2024.
- [The Impact of Element Ordering on LM Agent Performance](https://arxiv.org/pdf/2409.12089), Sep. 19 2024.
- [MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation](https://arxiv.org/pdf/2310.03302), Apr. 14 2024.
- [AgentScope: A Flexible yet Robust Multi-Agent Platform](https://arxiv.org/pdf/2402.14034), May 20 2024. [github](https://github.com/modelscope/agentscope).
- [TextGrad: Automatic “Differentiation” via Text](https://arxiv.org/pdf/2406.07496), Jun. 11 2024. [github](https://github.com/zou-group/textgrad).
- [AFlow: Automating Agentic Workflow Generation](https://arxiv.org/abs/2410.10762), Oct. 14 2024.
- [AFlow: Automatic agentic workflow generation](https://arxiv.org/pdf/2410.10762), Oct. 14 2024. [code](https://github.com/geekan/MetaGPT).
- [Agent skill acquisition for large language models via CycleQD](https://arxiv.org/pdf/2410.14735), Nov. 27 2024. [code](https://github.com/SakanaAI/CycleQD).
- [Commit0: Library generation from scratch](https://arxiv.org/pdf/2412.01769), Dec. 2 2024.
 [code](https://github.com/commit-0/commit0).
[Magentic-One: A generalist multi-agent system for solving complex tasks](https://arxiv.org/pdf/2411.04468), Nov. 7 2024. [blogpost](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/).
- [Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs](https://arxiv.org/pdf/2406.16218), Nov. 1 2024. [code](https://microsoft.github.io/Trace/).
- [Towards human-guided, data-centric LLM co-pilots](https://arxiv.org/pdf/2501.10321), Jan. 17 2025. `data science`.
- [UI-TARS: Pioneering automated GUI interaction with native agents](https://arxiv.org/abs/2501.12326), Jan. 21 2025. [code](https://github.com/bytedance/UI-TARS?tab=readme-ov-file).
- [browser-use](https://github.com/browser-use/browser-use), Make websites accessible for AI agents.
- [OmniParser for pure vision based GUI agent](https://arxiv.org/abs/2408.00203), Aug. 1 2024.

### Multi-modality

- [A Practitioner’s Guide to Continual Multimodal Pretraining](https://arxiv.org/pdf/2408.14471), Aug. 26 2024.
- [Visual agents as fast and slow thinker](https://arxiv.org/pdf/2408.08862), Aug. 16 2024.
- [NVLM: Open Frontier-Class Multimodal LLMs](https://nvlm-project.github.io/), Sep. 17 2024.
- [From generalist to specialist: Adapting vision language models via task-specific visual instruction tuning](https://arxiv.org/pdf/2410.06456), Oct. 9 2024.


