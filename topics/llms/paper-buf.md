    
### Others

- [Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains](https://arxiv.org/pdf/2405.07414), May 13 2024.
- [ImageNot: A contrast with ImageNet preserves model rankings](https://arxiv.org/pdf/2404.02112), Apr. 2 2024.
- [Training Data Attribution via Approximate Unrolled Differentation](https://arxiv.org/pdf/2405.12186), May 21 2024.
- [Selective Explanations](https://arxiv.org/pdf/2405.19562), May 29 2024. `interpretability`.
- [Local vs. Global Interpretability:A Computational Complexity Perspective](https://arxiv.org/pdf/2406.02981), Jun. 5 2024. `interpretability`.
- [Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize](https://arxiv.org/pdf/2406.03345), Jun. 5 2024. `ood generalization`.
  - _"understanding of fundamental difficulty of ood generalization"_
- [Beyond Discrepancy: A Closer Look at the Theory of Distribution Shift](https://arxiv.org/pdf/2405.19156), May 29 2024. `ood`.
- [Quantifying Task Priority for Multi-Task Optimization](https://arxiv.org/pdf/2406.02996), Jun. 5 2024. `mtl`.
- [Investigating the Impact of Model Instability on Explanations and Uncertainty](https://arxiv.org/pdf/2402.13006), Jun. 4 2024. `interpretability`.
- [Guarantee Regions for Local Explanations](https://arxiv.org/pdf/2402.12737), Feb. 20 2024.
- [Failures and Successes of Cross-Validation for Early-Stopped Gradient Descent](https://arxiv.org/pdf/2402.16793), Feb. 26 2024. `generealization theory`.
- [When does compositional structure yield compositional generalization? A kernel theory.](https://arxiv.org/pdf/2405.16391), May 26 2024. `compositionality theory`.
- [From Neurons to Neutrons: A Case Study in Interpretability](https://arxiv.org/pdf/2405.17425), May 27 2024. `mechanistic interpretability`.
- [Explaining Explainability: Understanding Concept Activation Vectors](https://arxiv.org/pdf/2404.03713), Apr. 4 2024. `interpretability`.
- [On the Benefits of Over-parameterization for Out-of-Distribution Generalization](https://arxiv.org/pdf/2403.17592), Mar. 26 2024. `ood` `overparameterization`.
- [Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization](https://arxiv.org/pdf/2403.07264), Mar. 12 2024. `generalization theory`.
- [On the Generalization Ability of Unsupervised Pretraining](https://arxiv.org/pdf/2403.06871), Mar. 11 2024.
- [Are Logistic Models Really Interpretable?](https://arxiv.org/pdf/2406.13427), Jun. 19 2024. `interpretability`.
- [Optimal synthesis embeddings](https://arxiv.org/pdf/2406.10259), Jun. 10 2024.
- [Transcendence: Generative Models Can Outperform The Experts That Train Them](https://arxiv.org/pdf/2406.11741), Jun. 28 2024.
- [A Label is Worth a Thousand Images in Dataset Distillation](https://arxiv.org/pdf/2406.10485), Jun. 15 2024. `dataset distillation`.
- [Step-by-Step Diffusion: An Elementary Tutorial](https://arxiv.org/pdf/2406.08929), Jun. 23 2024.
- [I Bet You Did Not Mean That: Testing Semantic Importance via Betting](https://arxiv.org/pdf/2405.19146), May 29 2024. `interpretability`.
- [Why Do You Grok? A Theoretical Analysis on Grokking Modular Addition](https://openreview.net/pdf?id=ad5I6No9G1), ICML 2024.
- [Grokking as the transition from lazy to rich training dynamics](https://arxiv.org/pdf/2310.06110), ICLR 2024.
- [Auditing Local Explanations is Hard](https://arxiv.org/pdf/2407.13281), Jul. 2024.
- [Position: Measure Dataset Diversity, Don’t Just Claim It](https://arxiv.org/pdf/2407.08188), Jul. 11 2024.
- [Learning dynamics paper list by Zeke Xie](https://github.com/zeke-xie/deep-learning-dynamics-paper-list). `nn learning dynamics`.
- [RELBENCH: A Benchmark for Deep Learning on Relational Databases](https://arxiv.org/pdf/2407.20060), Jul. 29 2024.
- [Data Debugging is NP-hard for Classifiers Trained with SGD](https://arxiv.org/pdf/2408.01365), Aug. 2 2024. `data attribution`.
- [Data Attribution at Scale](https://ml-data-tutorial.org/), ICML 2024 tutorial.
- [The Principles of Deep Learning Theory](https://deeplearningtheory.com/lectures/).
- [Zero-Shot Object-Centric Representation Learning](https://arxiv.org/pdf/2408.09162), Aug. 17 2024.
- [Representation Norm Amplification for Out-of-Distribution Detection in Long-Tail Learning](https://arxiv.org/pdf/2408.10676), Aug. 20 2024.
- [AlphaCube](https://alphacube.dev/),  [code](https://github.com/kyo-takano/alphacube).
  - [EfficientCube](https://github.com/kyo-takano/EfficientCube), [code](https://github.com/kyo-takano/EfficientCube), [paper](https://openreview.net/forum?id=bnBeNFB27b).
- [A Geometric Perspective on Diffusion Models](https://arxiv.org/pdf/2305.19947), Aug. 22 2024.
- [Learning Tree-Structured Composition of Data Augmentation](https://arxiv.org/pdf/2408.14381), Aug. 26 2024.
- [A more unified theory of transfer learning](https://arxiv.org/pdf/2408.16189), Aug. 29 2024.
- [The Many Faces of Optimal Weak-to-Strong Learning](https://arxiv.org/pdf/2408.17148), Aug. 30 2024.
- [Training independent subnetworks for robust prediction](https://arxiv.org/pdf/2010.06610), ICLR 2021.
- [Interactive Machine Teaching by Labeling Rules and Instances](https://arxiv.org/pdf/2409.05199), Sep. 8 2024.
- [What to align in multimodal contrastive learning?](https://arxiv.org/pdf/2409.07402), Sep. 11 2024.
- [Explanation, debate, align: A weak-to-strong framework for language model generalization](https://arxiv.org/pdf/2409.07335), Sep. 11 2024.
- [Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling](https://arxiv.org/pdf/2409.15156), Sep. 23 2024.
- [What is the Right Notion of Distance between Predict-then-Optimize Tasks?](https://arxiv.org/pdf/2409.06997), Sep. 11 2024.
- [What makes a maze look like a maze?](https://arxiv.org/pdf/2409.08202), Sep. 12 2024.
- [Is merging worth it? Securely evaluating the information gain for causal dataset acquisition](https://arxiv.org/pdf/2409.07215), Sep. 2024.
- [Aligning Machine and Human Visual Representations across Abstraction Levels](https://arxiv.org/pdf/2409.06509), Sep. 16 2024.
- [Input Space Mode Connectivity in Deep Neural Networks](https://arxiv.org/pdf/2409.05800), Sep. 9 2024.
- [Modelling Global Trade with Optimal Transport](https://arxiv.org/pdf/2409.06554), Sep. 21 2024.
- [Do Concept Bottleneck Models Respect Localities?](https://arxiv.org/pdf/2401.01259v3), Aug. 31 2024.
- [From Model Explanation to Data Misinterpretation: Uncovering the Pitfalls of Post Hoc Explainers in Business Research](https://arxiv.org/pdf/2408.16987), Aug. 30 2024.
- [Unifying causal representation learnign with the invariance principle](https://arxiv.org/pdf/2409.02772), Sep. 4 2024. `representation learning`.
- [How does the brain compute with probabilities?](https://arxiv.org/pdf/2409.02709), Sep. 1 2024.
- [Relative-Translation Invariant Wasserstein Distance](https://arxiv.org/pdf/2409.02416), Sep. 4 2024.
- [Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering](https://arxiv.org/pdf/2409.02426), Sep. 4 2024.
- [Interpretable Clustering: A Survey](https://arxiv.org/pdf/2409.00743), Sep. 1 2024.
- [A Unified Causal Framework for Auditing Recommender Systems for Ethical Concerns](https://arxiv.org/pdf/2409.13210), Sep. 20 2024.
- [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185), May 19 2024.
- [Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/pdf/2409.04431), Sep. 6 2024.
- [Accelerating training with neuron interaction and noncasting networks](https://arxiv.org/pdf/2409.04434), Sep. 6 2024.
  - Proposes NiNo network.
- [Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction](https://arxiv.org/pdf/2408.11816), Aug. 21 2024.
- [Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic](https://arxiv.org/pdf/2408.13656), Aug. 24 2024.
- [Distributional Properties of Subword Regularization](https://arxiv.org/pdf/2408.11443), Aug. 21 2024.
- [Building the Vision Transformer From Scratch](https://medium.com/@curttigges/building-the-vision-transformer-from-scratch-d77881edb5ff), Jun. 27 2022.
- [Artificial intelligence for science: The easy and hard problems](https://arxiv.org/pdf/2408.14508), Aug. 24 2024.
- [From lazy to rich: Exact learning dynamics in deep linear networks](https://arxiv.org/pdf/2409.14623), Sep. 22 2024.
- [On the specialization of neural modules](https://arxiv.org/pdf/2409.14981), Sep. 23 2024.
- [Synergy and Symmetry in Deep Learning: Interactions between the Data, Model, and Inference Algorithm](https://arxiv.org/pdf/2207.04612), Jul. 11 2022.
- [Partially Interpretable Models with Guarantees on Coverage and Accuracy](https://proceedings.mlr.press/v237/frost24a/frost24a.pdf), alt 2024.
- [What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing](https://arxiv.org/pdf/2409.09261), Sep. 14 2024.
- [RandALO: Out-of-sample risk estimation in no time flat](https://arxiv.org/pdf/2409.09781), Sep. 15 2024.
- [Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models](https://arxiv.org/pdf/2409.11136), Sep. 17 2024.
- [EUREKA: Evaluating and Understanding Large Foundation Models](https://arxiv.org/pdf/2409.10566), Sep. 13 2024.
- [Is All Learning (Natural) Gradient Descent?](https://arxiv.org/pdf/2409.16422), Sep. 24 2024.
- [A Statistical Viewpoint on Differential Privacy: Hypothesis Testing, Representation and Blackwell’s Theorem](https://www.arxiv.org/pdf/2409.09558), Sep. 14 2024.
- [Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent](https://www.arxiv.org/pdf/2409.09558), Sep. 14 2024.
- [Optimal ablation for interpretability](https://www.arxiv.org/pdf/2409.09951), Sep. 16 2024.
- [Revisiting inverse Hessian vector products for calculating influence functions](https://arxiv.org/pdf/2409.17357), Sep. 25 2024.
- [GFlowNet pretraining with inexpensive rewards](https://arxiv.org/pdf/2409.09702), Sep. 15 2024.
  - [Pre-Training and Fine-Tuning Generative Flow Networks](https://arxiv.org/pdf/2310.03419), Oct. 5 2024.
- [Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution](https://arxiv.org/pdf/2401.15866), Jan. 29 2024.
- [Information-Theoretic Foundations for Machine Learning](https://arxiv.org/pdf/2407.12288), Aug. 20 2024.
- [First Steps Toward Understanding the Extrapolation of Nonlinear Models to Unseen Domains](https://arxiv.org/pdf/2211.11719), Dec. 1 2022.
- [Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent](https://arxiv.org/pdf/2409.08770), Sep. 21 2024.
- [Selecting a classification performance measure: Matching the measure to the problem](https://arxiv.org/pdf/2409.12391), Sep. 19 2024.
- [Bridging OOD Detection and Generalization: A Graph-Theoretic View](https://arxiv.org/pdf/2409.18205), Sep. 26 2024.
- [Life Lessons from the First Half-Century of My Career](https://cacm.acm.org/opinion/life-lessons-from-the-first-half-century-of-my-career/), Oct. 10 2024.
- [Old Optimizer, New Norm: An Anthology](https://arxiv.org/abs/2409.20325), Sep. 30 2024.
- [Generalized group data attribution](https://arxiv.org/pdf/2410.09940), Oct. 13 2024.
- [Feature averaging: An implicit bias of gradient descent leading to non-robustness in neural networks](https://arxiv.org/pdf/2410.10322), Oct. 14 2024.
- [Non-convergence to global minimizers in data driven supervised deep learning: Adam and stochastic gradient descent optimization provably fail to converge to global minimizers in the training of deep neural networks with ReLU activation](https://arxiv.org/pdf/2410.10533), Oct. 14 2024.
- [Scaling Gaussian Processes for Learning Curve Prediction via Latent Kronecker Structure](https://arxiv.org/pdf/2410.09239), Oct. 11 2024.
- [The Epochal Sawtooth Effect: Unveiling Training Loss Oscillations in Adam and Other Optimizers](https://arxiv.org/pdf/2410.10056), Oct. 14 2024.
- [Linear Mode Connectivity and the Lottery Ticket Hypothesis](https://proceedings.mlr.press/v119/frankle20a/frankle20a.pdf), 
- [Simultaneous linear connectivity of neural networks modulo permutation](https://arxiv.org/pdf/2404.06498), Apr. 9 2024.
- [The Non-Local Model Merging Problem: Permutation Symmetries and Variance Collapse](https://arxiv.org/pdf/2410.12766), Oct. 16 2024. [code](https://github.com/ekanshs/tact-merge).
- [Optimal Protocols for Continual Learning via Statistical Physics and Control Theory](https://arxiv.org/pdf/2409.18061),
- [Artificial Kuramoto oscillatory neurons](https://arxiv.org/pdf/2410.13821), Oct. 17 2024. [code](https://github.com/autonomousvision/akorn).
  - [Binding dynamics in rotating features](https://arxiv.org/pdf/2402.05627), Feb. 8 2024.
- [Understanding optimization in deep learning with central flows](https://arxiv.org/pdf/2410.24206), Oct. 31 2024.
- [When do large learning rates lead us?](https://arxiv.org/pdf/2410.22113), Oct. 29 2024. [code](https://github.com/isadrtdinov/understanding-large-lrs).
- [Zipfian Whitening](https://arxiv.org/pdf/2411.00680), Nov. 1 2024.
- [Disentangling Interactions and Dependencies in Feature Attribution](https://arxiv.org/pdf/2410.23772), Oct. 31 2024. `feature attribution`.
- [Provable unlearning in topic modeling and downstream tasks](https://arxiv.org/pdf/2411.12600), Nov. 19 2024.
- [Understanding the gains from repeated self-distillation](https://arxiv.org/pdf/2407.04600), Jul. 5 2024.
  - _"Our analysis shows that the excess risk achieved by multi-step self-distillation can significantly improve upon a single step of self-distillation, reducing the excess risk by a factor as large as d, where d is the input dimension. Empirical results on regression tasks from the UCI repository show a reduction in the learnt model's risk (MSE) by up to 47%."_
  - _"At a high-level, self-distillation can use data more effectively, since it allows us to extract more knowledge from the same training dataset."_
- [Training Data Attribution via Approximate Unrolling](https://openreview.net/pdf?id=3NaqGg92KZ), NeurIPS 2024.
- [A Theory of Initialisation’s Impact on Specialisation](https://openreview.net/pdf?id=OdoxWJMapK), Submitted to the Mathematics of Modern Machine Learning Workshop at NeurIPS 2024.
- [∆-Influence: Unlearning Poisons via Influence Functions](https://arxiv.org/pdf/2411.13731), Nov. 20 2024. [code](https://github.com/andyisokay/delta-influence).
- [A complexity-based theory of compositionality](https://arxiv.org/pdf/2410.14817), Oct. 18 2024.
- [Geometric signatures of compositionality across a language model's lifetime](https://arxiv.org/pdf/2410.01444), Oct. 7 2024.
- [Sometimes I am a Tree: Data Drives Unstable
Hierarchical Generalization](https://arxiv.org/pdf/2412.04619), Dec. 5 2024. [code](https://github.com/sunnytqin/concept_comp).
- [The large learning rate phase of deep learning: the catapult mechanism](https://arxiv.org/pdf/2003.02218), Mar. 4 2020.
- [The birth of self-supervised learning: A supervised theory](https://openreview.net/pdf?id=NhYAjAAdQT), NeurIPS 2024 SSL Workshop.
- [Art-Free generative models: Art creation without graphic art knowledge](https://arxiv.org/pdf/2412.00176), Nov. 29 2024.
- [The Pitfalls of Memorization: When Memorization
Hurts Generalization](https://arxiv.org/pdf/2412.07684), Dec. 10 2024.
- [Beyond Reweighting: On the Predictive Role of Covariate Shift in Effect Generalization](https://arxiv.org/pdf/2412.08869), Dec. 12 2024. [code](https://github.com/ying531/predictive-shift).
- [Of dice and games: A theory of generalized boosting](https://arxiv.org/pdf/2412.08012), Dec. 11 2024.
- [Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations](), ICML 2023.
- [Tutorial on Practical Prediction Theory for Classification](https://www.jmlr.org/papers/volume6/langford05a/langford05a.pdf), JMLR 2005.
- [Machine Learning Reductions Tutorial](https://hunch.net/~reductions_tutorial/reductions_icml09.pdf), ICML 2009.
- [Subtle adversarial image manipulations influence both human and machine perception](https://www.nature.com/articles/s41467-023-40499-0), Nature 2023.
- [Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness](https://arxiv.org/abs/2408.05446), Aug. 8 2024.
- [softmax is not enough (for sharp out-of-distribution)](https://arxiv.org/pdf/2410.01104), Oct. 7 2024.
- [Universality of representation in biological and artificial neural networks](https://www.biorxiv.org/content/10.1101/2024.12.26.629294v1.full), Dec. 26 2024.
- [A Linear Network Theory of Iterated Learning](https://openreview.net/pdf?id=xudmoaroxQ), NeurIPS 2024 workshop.
  - _"Here we theoretically study the emergence of compositional language, and the ability of simple neural networks to leverage this compositionality to systematically generalize"_

### Contrastive learning

- [CLIPScore: A Reference-free Evaluation Metric for Image Captioning](https://arxiv.org/abs/2104.08718), Mar. 23 2022.
- [CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning](https://arxiv.org/pdf/2405.19547), May 29 2024.
- [Learning smooth distance functions via queries](https://arxiv.org/pdf/2412.01290), Dec. 2 2024.
- [On erroneous agreements of CLIP image embeddings](https://arxiv.org/pdf/2411.05195), Nov. 7 2024.

---

## Codebases

- [thefuzz](https://github.com/seatgeek/thefuzz).

### Machine learning

- [mbrs](https://github.com/naist-nlp/mbrs/tree/main), [paper](https://arxiv.org/abs/2408.04167), Oct. 21 2024.
- [alphafold3](https://github.com/google-deepmind/alphafold3), [paper](https://www.nature.com/articles/s41586-024-07487-w), May 8 2024.
- [Mooncake](https://github.com/kvcache-ai/Mooncake), [paper](https://arxiv.org/abs/2407.00079), Jun. 2024.
- [inseq](https://github.com/inseq-team/inseq). Interpretability for sequence generation models 🐛 🔍.

## Cognitive science

- [Two views on the cognitive brain](https://static1.squarespace.com/static/6305272e303de360fa96632b/t/63f4143cbcc9c36bd399dd87/1676940350093/Two+views+on+the+cognitive+brain.pdf), 2021.

---

## LLMs related

### Topics

> Some big topics I really care about.

- [LLM interpretability](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/llm_interpretability.md).
- [Scaling laws](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/scaling_laws.md).
- [Training and optimization](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/training_and_optimization.md).
- [Knowledge updating](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/knowledge_updating.md).
- [Reasoning](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/reasoning.md).

### Buffered papers

- [Position: Levels of AGI for Operationalizing Progress on the Path to AGI](https://openreview.net/pdf?id=0ofzEysK2D), ICML 2024.
- [WILDCHAT: 1M CHATGPT INTERACTION LOGS IN THE WILD](https://arxiv.org/pdf/2405.01470), May 2 2024.
  - It is interesting to know the data distribution of queries to ChatGPT.
- [Lessons from the Trenches on Reproducible Evaluation of Language Models](https://arxiv.org/abs/2405.14782), May 23 2024. `evaluation`.
- [How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad](https://arxiv.org/pdf/2406.06467), Jun. 10 2024. `reasoning`.
- [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/pdf/2406.12031), Jun. 2024. `tabular llm`.
- [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/pdf/2406.11931), Jun. 17 2024. `tech report`.
- [Infinite Limits of Multi-head Transformer Dynamics](https://arxiv.org/pdf/2405.15712), May 24 2024. `learning dynamics`.
- [What Variables Affect Out-Of-Distribution Generalization in Pretrained Models?](https://arxiv.org/pdf/2405.15018), May 23 2024.
- [Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs](https://arxiv.org/pdf/2405.15485), May 24 2024. `reasoning`.
- [Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation](https://arxiv.org/pdf/2405.15302), May 24 2024. `reasoning`.
- [From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step](https://arxiv.org/pdf/2405.14838), May 23 2024. `reasoning`.
- [Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models](https://arxiv.org/pdf/2405.15143), May 24 2024. `reasoning` `agent`.
- [From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems](https://arxiv.org/pdf/2405.19883), May 30 2024. `agent`.
- [Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities](https://arxiv.org/pdf/2405.20003), May 30 2024. `uncertainty`.
- [A Language Model's Guide Through Latent Space](https://arxiv.org/pdf/2402.14433), Feb. 22 2024.
- [Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf), ICML 2023.
- [How Truncating Weights Improves Reasoning in Language Models](https://arxiv.org/pdf/2406.03068), Jun. 5 2024. `reasoning`.
- [Does learning the right latent variables necessarily improve in-context learning?](https://arxiv.org/pdf/2405.19162), May 29 2024.
- [Towards an empirical understanding of MoE design choices](https://arxiv.org/pdf/2402.13089), Feb. 20 2024. `moe`.
- [StructLM: Towards Building Generalist Models for Structured Knowledge Grounding](https://arxiv.org/pdf/2402.16671), Apr. 24 2024. `structure knowledge`.
- [Chronos: Learning the Language of Time Series](https://arxiv.org/pdf/2403.07815), May 2 2024.
- [On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability](https://arxiv.org/pdf/2405.16845), May 27 2024.
- [Theoretical Analysis of Weak-to-Strong Generalization](https://arxiv.org/pdf/2405.16043), May 25 2024.
- [Scaling and evaluating sparse autoencoders](https://arxiv.org/pdf/2406.04093), Jun. 6 2024.
- [Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe](https://arxiv.org/pdf/2406.04165), Jun. 6 2024. `llm for embedding`.
- [miniCodeProps: a Minimal Benchmark for Proving Code Properties](https://arxiv.org/pdf/2406.11915), Jun. 16 2024. `coding` `benchmark`.
- [What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering](https://arxiv.org/pdf/2406.12334), Jun. 18 2024. `prompt engineering`.
- [QOG:Question and Options Generation based on Language Model](https://arxiv.org/pdf/2406.12381), Jun. 18 2024. `data synthesis`.
- [CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society](https://arxiv.org/pdf/2303.17760), Nov. 2 2023. `multi-agent`.
- [Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models](https://arxiv.org/pdf/2403.19340), Mar. 28 2024. `data pipeline`.
- [Can language model explain their own classification behavior?](https://arxiv.org/pdf/2405.07436), May 13 2024.
- [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221), Jul. 11 2022.
- [xVal: A Continuous Number Encoding for Large Language Models](https://arxiv.org/abs/2310.02989), Oct. 4 2023.
- [Memory Mosaics](https://arxiv.org/pdf/2405.06394), May 10 2024.
- [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/pdf/2404.01413), Apr. 29 2024. `synthetic data`.
  - _"accumulating the successive generations of synthetic data alongside the original data avoids model collapse"_
- [An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247), May 27 2024. `tutorial`.
- [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/pdf/2402.18563), Feb. 28 2024.
- [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://arxiv.org/pdf/2403.06833), Jun. 3 2024.
- [Data-Centric AI in the Age of Large Language Models](https://arxiv.org/pdf/2406.14473), Jun. 20 2024. `data-centric`.
- [FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery](https://arxiv.org/pdf/2211.08316), May 11 2023.
- [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/pdf/2402.03049), Mar. 21 2024.
- [Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization](https://arxiv.org/pdf/2405.15071), May 27 2024. `mechanistic interpretability`.
- [Theoretical Analysis of Weak-to-Strong Generalization](https://arxiv.org/pdf/2405.16043), May 25 2024.
- [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804), Feb. 6 2024.
- [Machine learning and information theory concepts towards an AI mathematician](https://www.ams.org/journals/bull/2024-61-03/S0273-0979-2024-01839-4/S0273-0979-2024-01839-4.pdf), May 15 2024. `math reasoning`.
- [Monitoring Latent World States in Language Models with Propositional Probes](https://arxiv.org/pdf/2406.19501), Jun. 27 2024.
- [End-To-End Causal Effect Estimation from Unstructured Natural Language Data](https://arxiv.org/pdf/2407.07018), Jul. 9 2024. `causal inference x llms`.
- [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/pdf/2407.04620), Jul. 5 2024. `architectural inductive bias`.
- [How Does Quantization Affect Multilingual LLMs?](https://arxiv.org/pdf/2407.03211), Jul. 3 2024. `quantization`.
- [Chain-of-Thought Reasoning without Prompting](https://arxiv.org/pdf/2402.10200), May 23 2024. `reasoning`.
- [Transformer Alignment in Large Language Models](https://arxiv.org/pdf/2407.07810), Jul. 10 2024.
- [SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/pdf/2407.09025), Jul. 12 2024. `structured knowledge`.
- [LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data](https://arxiv.org/pdf/2407.11418), Jul. 16 2024.
- [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/pdf/2311.17035), Nov. 28 2023. `data extraction`.
- [The Foundations of Tokenization: Statistical and Computational Concerns](https://arxiv.org/pdf/2407.11606), Jul. 16 2024.
- [Representing Rule-based Chatbots with Transformers](https://arxiv.org/pdf/2407.10949), Jul. 15 2024.
- [Learning to Compile Programs to Neural Networks](https://arxiv.org/pdf/2407.15078), Jul. 21 2024.
- [Faithfulness Measurable Masked Language Models](https://arxiv.org/abs/2310.07819), May 9 2024.
- [Fast Matrix Multiplications for Lookup Table-Quantized LLMs](https://arxiv.org/pdf/2407.10960), Jul. 15 2024.
- [On the Benefits of Rank in Attention Layers](https://arxiv.org/pdf/2407.16153), Jul. 24 2024.
  - The question to tackle: _"hyperparameters such as the rank of the attention matrices and the number of heads are scaled nearly the same way in all realizations of this architecture, without theoretical justification"_
- [Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](https://arxiv.org/pdf/2407.16607), Jul. 23 2024.
- [Perceptions of linguistic uncertainty by language models and humans](https://arxiv.org/pdf/2407.15814), Jul. 22 2024.
- [Internal Consistency and Self-Feedback in Large Language Models](https://arxiv.org/pdf/2407.14507), Jul. 19 2024.
- [u-µP: The Unit-Scaled Maximal Update Parametrization](https://arxiv.org/pdf/2407.17465), Jul. 24 2024.
- [X-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs](https://arxiv.org/pdf/2407.18134), Jul. 25 2024.
- [Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications](https://arxiv.org/pdf/2407.19262), Jul. 27 2024. `learning dynamics`.
- [Do Language Models Have a Critical Period for Language Acquisition?](https://arxiv.org/pdf/2407.19325), Jul. 27 2024. `learning dynamics`.
- [From Pre-training Corpora to Large Language Models: What Factors Influence LLM Performance in Causal Discovery Tasks?](https://arxiv.org/pdf/2407.19638), Jul. 29 2024. `knowledge extraction`, `causal knowledge`.
- [Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment](https://arxiv.org/pdf/2407.19346), Jul. 27 2024.
- [Understanding the Relationship between Prompts and Response Uncertainty in Large Language Models](https://arxiv.org/pdf/2407.14845), Jul. 20 2024. related to `knowledge extraction`.
  - _"We show that the uncertainty decreases as the prompt's informativeness increases, similar to epistemic uncertainty"_
- [Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences](https://arxiv.org/pdf/2407.09499), Jun. 12 2024. `synthetic data` `theory`.
- [Can LLMs predict the convergence of Stochastic Gradient Descent?](https://arxiv.org/pdf/2408.01736), Aug. 3 2024.
- [STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs](https://arxiv.org/pdf/2408.01803), Aug. 3 2024. `efficient llm`.
- [Self-Taught Evaluators](https://arxiv.org/pdf/2408.02666), Aug. 8 2024. `self-eval`.
- [A Survey of Mamba](https://arxiv.org/pdf/2408.01129), Aug. 2 2024.
- [Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities](https://arxiv.org/pdf/2408.07666), Aug. 14 2024.
- [A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning](https://arxiv.org/pdf/2408.07057), Aug. 13 2024.
- [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/pdf/2408.06292), Aug. 12 2024.
- [Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters](https://arxiv.org/pdf/2408.04093), Aug. 9 2024.
- [FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers](https://arxiv.org/pdf/2408.04816), Aug. 9 2024.
- [Natural Language Outlines for Code: Literate Programming in the LLM Era](https://arxiv.org/pdf/2408.04820), Aug. 9 2024.
- [Can a Bayesian Oracle Prevent Harm from an Agent?](https://arxiv.org/pdf/2408.05284), Aug. 9 2024.
- [Low-Rank Approximation, Adaptation, and Other Tales](https://arxiv.org/pdf/2408.05883), Aug. 12 2024.
- [Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models](https://arxiv.org/pdf/2408.08210), Aug. 15 2024.
- [BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts](https://arxiv.org/pdf/2408.08274), Aug. 15 2024.
- [Out-of-Distribution Learning with Human Feedback](https://arxiv.org/pdf/2408.07772), Aug. 14 2024.
- [Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models](https://arxiv.org/pdf/2408.10189), Aug. 19 2024.
- [KAN 2.0: Kolmogorov-Arnold Networks Meet Science](https://arxiv.org/pdf/2408.10205), Aug. 19 2024.
- [Learning Randomized Algorithms with Transformers](https://arxiv.org/pdf/2408.10818), Aug. 20 2024.
- [Classifier-Free Guidance is a Predictor-Corrector](https://arxiv.org/pdf/2408.09000), Aug. 23 2024.
- [What can Large Language Models Capture about Code Functional Equivalence?](https://arxiv.org/pdf/2408.11081), Aug. 20 2024.
- [Large Language Models for Supply Chain Optimization](https://arxiv.org/pdf/2307.03875), Jul. 13 2023.
  - [HarnessingAI and LLMs to Revolutionize Retail and Supply Chain Management](https://www.pacificdataintegrators.com/hubfs/Website-Whitepapers/Harnessing%20AI%20and%20LLMs%20to%20Revolutionize%20Retail%20and%20Supply%20Chain%20Management-PDF.pdf).
  - [OptiGuide](https://github.com/microsoft/OptiGuide).
  - [Generative AI In Supply Chain](https://www.alvarezandmarsal.com/sites/default/files/article/pdf/Generative%20AI%20in%20Supply%20Chain%20Report%20-%20Compressed%20version.pdf).
- [Performative Prediction on Games and Mechanism Design](https://arxiv.org/pdf/2408.05146), Aug. 9 2024.
- [FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers](https://arxiv.org/pdf/2408.04816), Aug. 9 2024. `COLM2024`. [code](https://github.com/jnwilliams/FUSE_prompt_inversion).
  - _"we propose FUSE (Flexible Unification of Semantic Embeddings), an inexpensive approach to approximating an adapter layer that maps from one model's textual embedding space to another, even across different tokenizers."_
- [COGEN: Learning from Feedback with Coupled Comprehension and Generation](https://arxiv.org/pdf/2408.15992), Aug. 28 2024.
- [SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning](https://arxiv.org/pdf/2408.05517), Aug. 19 2024.
- [The Unbearable Slowness of Being](https://arxiv.org/pdf/2408.10234), Aug. 3 2024.
- [An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models](https://arxiv.org/pdf/2408.00724), Aug. 1 2024.
  - _"We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped wiht more sophisticated decoding algorithms in budget-constrained scenarios, e.g. on-devices, to enhance problem-solving accuracy."_
  - [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/pdf/2406.16838), Jun. 24 2024.
- [Olmoe: Open mixture-of-experts language models](https://arxiv.org/pdf/2409.02060), Sep. 3 2024. [data](https://huggingface.co/datasets/allenai/OLMoE-mix-0924).
- [Reframing Data Value for Large Language Models Through the Lens of Plausability](https://arxiv.org/pdf/2409.00284), Aug. 30 2024.
- [Extracting Paragraphs from LLM Token Activations](https://arxiv.org/pdf/2409.06328), Sep. 10 2024.
- [Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving](https://arxiv.org/pdf/2405.12205), May 20 2024.
- [Instruct-SkillMix: A powerful pipeline for llm instruction tuning](https://www.arxiv.org/pdf/2408.14774v2), Sep. 9 2024.
- [MambaByte: Token-free Selective State Space Model](https://arxiv.org/pdf/2401.13660), Aug. 9 2024.
- [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/pdf/2402.04333), Jun. 13 2024.
- [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/pdf/2409.12917), Sep. 19 2024.
- [ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty](https://arxiv.org/pdf/2408.14339), Aug. 26 2024.
- [GRIN: GRadient-INformed MoE](https://arxiv.org/pdf/2409.12136), Sep. 18 2024.
- [QWen2.5-Math technical report: Towards mathematical expert model via self-improvement](https://arxiv.org/pdf/2409.12122), Sep. 18 2024.
- [A Controlled Study on Long Context Extension and Generalization in LLMs](https://arxiv.org/pdf/2409.12181), Sep. 23 2024. `long-context`.
- [Apple Intelligence Foundation Language Models](https://arxiv.org/pdf/2407.21075), Jul. 29 2024.
- [Can Large Language Models Unlock Novel Scientific Research Ideas?](https://arxiv.org/pdf/2409.06185), Sep. 10 2024.
- [(Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models](https://arxiv.org/pdf/2409.02628), Sep. 4 2024.
- [Quantifying Emergence in Neural Networks: Insights from Pruning and Training Dynamics](https://arxiv.org/pdf/2409.01568), Sep. 3 2024.
- [Hypothesizing Missing Causal Variables with LLMs](https://arxiv.org/pdf/2409.02604), Sep. 4 2024.
- [NUDGE: Lightweight non-parametric fine-tuning of embeddings for retrieval](https://arxiv.org/pdf/2409.02343), Sep. 4 2024.
- [Unforgettable Generalization in Language Models](https://arxiv.org/pdf/2409.02228), Sep. 3 2024.
- [Inductive Learning of Logical Theories with LLMs: A Complexity-graded Analysis](https://arxiv.org/pdf/2408.16779), Aug. 15 2024.
- [Configurable Foundation Models: Building LLMs from a Modular Perspective](https://arxiv.org/pdf/2409.02877), Sep. 4 2024.
- [Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data](https://arxiv.org/pdf/2409.00096), Aug. 27 2024.
- [A Formal Hierarchy of RNN Architectures](https://blog.allenai.org/a-formal-hierarchy-of-rnn-architectures-94c9d47566b5), Apr. 22 2020.
- [Proof Automation with Large Language Models](https://arxiv.org/pdf/2409.14274), Sep. 22 2024.
- [Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers](https://arxiv.org/pdf/2409.04109), Sep. 6 2024.
- [On the Computational Benefit of Multimodal Learning](https://proceedings.mlr.press/v237/lu24a/lu24a.pdf), alt 2024.
- [Reranking Laws for Language Generation: A Communication-Theoretic Perspectiv](https://arxiv.org/pdf/2409.07131), Sep. 11 2024.
- [Improving pretraining data using perplexity correlations](https://arxiv.org/pdf/2409.05816), Sep. 9 2024.
- [Explaining Datasets in Words: Statistical Models with Natural Language Parameters](https://arxiv.org/abs/2409.08466), Sep. 13 2024.
- [Language Models “Grok” to Copy](https://www.arxiv.org/pdf/2409.09281), Sep. 14 2024.
- [Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models](https://arxiv.org/pdf/2409.17455), Sep. 26 2024.
- [Order of Magnitude Speedups for LLM Membership Inference](https://arxiv.org/pdf/2409.14513), Sep. 24 2024.
- [Estimating Wage Disparities Using Foundation Models](https://www.arxiv.org/pdf/2409.09894), Sep. 15 2024.
- [Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates](https://arxiv.org/pdf/2402.18540), Feb. 28 2024.
- [Safety Alignment Should Be Made More Than Just a Few Tokens Deep](https://arxiv.org/pdf/2406.05946), Jun. 10 2024.
- [Can Models Learn Skill Composition from Examples?](https://openreview.net/pdf?id=YEEsRgkvnU), NeurIPS 2024.
- [See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses](https://arxiv.org/abs/2408.08978#), Aug. 16 2024.
- [Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models](https://arxiv.org/abs/2408.06663), Aug. 13 2024.
- [A Taxonomy for Data Contamination in Large Language Models](https://arxiv.org/abs/2407.08716), Jul. 11 2024.
- [LLM Pruning and Distillation in Practice: The Minitron Approach](https://d1qx31qr3h6wln.cloudfront.net/publications/minitron_tech_report.pdf), Aug. 21 2024.
- [Realistic evaluation of model merging for compositional generalization](https://arxiv.org/pdf/2409.18314), Sep. 26 2024.
- [Transformer Memory as a Differentiable Search Index](https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf), NeurIPS 2022. [github](https://github.com/ArvinZhuang/DSI-transformers).
  - [DSI++: Updating Transformer Memory with New Documents](https://aclanthology.org/2023.emnlp-main.510.pdf), EMNLP 2023.
- [Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers](https://arxiv.org/pdf/2409.20537), Sep. 30 2024. `robotics foundation model`. NeurIPS 2024.
  - _TL;DR: "HPT aligns different embodiment to a shared latent space and investigates the scaling behaviors in policy learning. Put a scalable transformer in the middle of your policy and don't train from scratch!"_
- [Geometric signatures of compositionality across a language model's lifetime](https://arxiv.org/pdf/2410.01444), Oct. 7 2024. `compositionality`.
- [Dont' cut corners: Exact conditions for modularity in biologically inspired representations](https://arxiv.org/pdf/2410.06232v1), Oc.t 8 2024. [code](https://github.com/kylehkhsu/dont-cut-corners/). `compositionality`.
- [Differential transformers](https://arxiv.org/pdf/2410.05258), Oct. 7 2024.
- [LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch](https://arxiv.org/pdf/2410.13213), Oct. 17 2024.
- [A simple baseline for predicting events with auto-regressive tabular transformers](https://arxiv.org/pdf/2410.10648), Oct. 14 2024.
- [DARE the Extreme Ð: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://arxiv.org/pdf/2410.09344), Oct. 12 2024. `pruning`.
- [LoLCATs: On Low-Rank Linearizing of Large Language Models](https://arxiv.org/pdf/2410.10254), Oct. 14 2024. [code](https://github.com/HazyResearch/lolcats).
- [Machines of Loving Grace](https://darioamodei.com/machines-of-loving-grace), Oct. 2024.
- [Predicting Emergent Capabilities by Finetuning](https://openreview.net/pdf?id=vL8BIGuFTF), COLM 2024.
- [End-to-End Ontology Learning with Large Language Models](https://arxiv.org/pdf/2410.23584), Oct. 31 2024. [code](https://github.com/andylolu2/ollm).
- [Language Models as Causal Effect Generators](https://arxiv.org/pdf/2411.08019), Nov. 12 2024. [code](https://github.com/lbynum/sequence-driven-scms).
- [On the limits of language generation: Trade-offs between hallucination and mode collapse](https://arxiv.org/pdf/2411.09642), Nov. 14 2024.
- [SEQ-VCR: Preventing collapse in intermediate transformer representations for enhanced reasoning](https://arxiv.org/pdf/2411.02344), Nov. 4 2024. [code](https://github.com/rarefin/seq_vcr).
- [Generative Agent Simulations of 1,000 People](https://arxiv.org/pdf/2411.10109), Nov. 2024.
- [Does Prompt Formatting Have Any Impact on LLM Performance?](https://arxiv.org/pdf/2411.10541), Nov. 15 2024.
- [Efficient Alignment of Large Language Models via Data Sampling](https://arxiv.org/pdf/2411.10545), Nov. 15 2024.
- [Language-to-Code Translation with a Single Labeled Example](https://aclanthology.org/2024.emnlp-main.462.pdf), 2024.
- [Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis](https://aclanthology.org/2024.findings-emnlp.547.pdf), EMNLP 2024. [code](https://github.com/cogito233/causal-sa).
- [To Believe or Not to Believe Your LLM: Iterative Prompting for Estimating Epistemic Uncertainty](https://openreview.net/pdf?id=k6iyUfwdI9), NeurIPS 2024.
- [Visual autoregressive modeling: Scalable image generation via next-scale prediction](https://arxiv.org/pdf/2404.02905), Jun. 10 2024.
- [Machine unlearning doesn't do what you think: Lessons for generative AI policy, research and practice](https://arxiv.org/pdf/2412.06966), Dec. 9 2024. `machine unlearning`.
- [Normalizing flows are capable generative models](https://arxiv.org/pdf/2412.06329), Dec. 10 2024. [code](https://github.com/apple/ml-tarflow).
- [Toward ai-driven digital organism: A system of multiscale foundation models for predicting, simulating and programming biology at all levels](https://www.cs.cmu.edu/~epxing/papers/2025/AIDO.pdf), Nov. 26 2024.
- [Memory Layers at Scale](https://arxiv.org/pdf/2412.09764), Dec. 12 2024. [code](https://github.com/facebookresearch/memory).
- [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/pdf/2412.13663), Dec. 2024.
- [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/pdf/2312.02780), Dec. 5 2023
  - [A Universal Law of Robustness via Isoperimetry](https://arxiv.org/pdf/2105.12806), Dec. 23 2024.
- [LearnLM: Improving Gemini for Learning](https://arxiv.org/pdf/2412.16429), Dec. 25 2024.
- [The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources](https://openreview.net/pdf?id=tH1dQH20eZ), TMLR 2024.
- [Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts](https://arxiv.org/pdf/2409.13728), Oct. 24 2024. [code](https://github.com/meszarosanna/rule_extrapolation).

### Length generalization

- [Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers](https://arxiv.org/pdf/2408.05506), Aug. 10 2024.
- [A formal framework for understanding length generalization in transformers](https://arxiv.org/pdf/2410.02140), Oct. 3 2024.

### In-context learning

- [In-context Learning and Induction Heads](https://arxiv.org/pdf/2209.11895), Mar. 8 2022. `in-context-learning`.
- [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://arxiv.org/pdf/2110.10090), Jun. 24 2024. `variable mechanism`.
- [Is Mamba Capable of In-Context Learning?](https://arxiv.org/pdf/2402.03170v2), Apr. 24 2024.
- [Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs](https://arxiv.org/pdf/2405.11880), May 20 2024. `icl`.
- [MLPs Learn In-Context](https://arxiv.org/pdf/2405.15618), May 24 2024. `icl`.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `in-context learning`.
- [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://arxiv.org/pdf/2405.19874), May 30 2024. `icl`.
- [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/pdf/2405.19592), May 30 2024. `icl`.
- [Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning](https://arxiv.org/pdf/2406.11890), Jun. 14 2024. `icl`.
- [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/pdf/2406.11233), Jun. 17 2024. `icl`.
- [Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning](https://arxiv.org/pdf/2406.14022), Jun. 20 2024. `in-context learning`.
- [Transformers are Universal In-context Learners](https://arxiv.org/pdf/2408.01367), Aug. 2 2024. `in-context learning`.
- [Memorization in in-context learning](https://arxiv.org/pdf/2408.11546), Aug. 21 2024.
- [Divide, reweight, and conquer: A logit arithmetic approach for in-context learning](https://arxiv.org/pdf/2410.10074), Oct. 14 2024. [code](https://github.com/Chengsong-Huang/LARA).
- [Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models](https://arxiv.org/pdf/2410.09701), Oct. 13 2024.
- [Re-examing learning linear functions in context](https://arxiv.org/pdf/2411.11465), Nov. 18 2024.
- [Many-Shot In-Context Learning](https://arxiv.org/pdf/2404.11018), Oct. 17 2024.
- [Multi-Attribute Constraint Satisfaction via Language Model Rewriting](https://arxiv.org/abs/2412.19198), Dec. 26 2024.

### Prompting techniques

- [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608), Jun. 6 2024. `prompting`.
- [A survey of prompt engineering methods in large language models for different nlp tasks](https://arxiv.org/pdf/2407.12994), Jul. 17 2024.

### LLMs for traditional nlp tasks

- [Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study](https://arxiv.org/pdf/2304.04339), Feb. 17 2024.
- [Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT](https://arxiv.org/pdf/2302.10198), Mar. 2 2023.
- [Whitening Not Recommended for Classification Tasks in LLMs](https://arxiv.org/pdf/2407.12886), Jul. 16 2024.

### Efficient training and inference

- [Fitting larger networks into memory](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9), Jan. 14 2018. `gradient checkpointing`. [github](https://github.com/cybertronai/gradient-checkpointing). [pytorch](https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb).
- [Triton Puzzles](https://github.com/srush/Triton-Puzzles), Triton tutorial.
- [Flora: Low-Rank Adapters Are Secretly Gradient Compressors](https://arxiv.org/abs/2402.03293), Feb. 5 2024. `efficient training`. [github](https://github.com/BorealisAI/flora-opt). [blogpost](https://www.borealisai.com/research-blogs/pre-training-multi-billion-parameter-llms-on-a-single-gpu-with-flora/#Incorporating_FLORA_into_your_code).
- [A visual guide to quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7common-data-types), Jul. 22 2024.
- [Foundation of large language model compression - Part 1: Weight quantization](https://arxiv.org/pdf/2409.02026), Sep. 2024.

### Data curation and mixing

- [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/pdf/2305.10429), Nov. 21 2023.
- [TinyGSM: achieving > 80% on GSM8k with small language models](https://arxiv.org/pdf/2312.09241), Dec. 14 2023.
- [What makes good data for alignment? A comphrehensive study of automatic data selection in instruction tuning](https://arxiv.org/pdf/2312.15685), Apr. 16 2024.
- [MetaMath: Bootstrap your own methematical questions for large language models](https://arxiv.org/pdf/2309.12284), May 3 2024.
- [Instruction Mining: When Data Mining Meets Large Language Model Finetuning](https://arxiv.org/abs/2307.06290), COLM 2024.
- [Data, Data Everywhere: A Guide for Pretraining Dataset Construction](https://www.arxiv.org/pdf/2407.06380), Jul. 8 2024. `data curation`.
  - _"we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain [...] we show how such attribute information can be used to further refine and improve the quality of a pretraining set"_
- [On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](https://arxiv.org/pdf/2406.15126), Jun. 14 2024. `data synthesis`.
- [Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG](https://arxiv.org/abs/2406.13069), Jun. 24 2024.
- [The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale](https://arxiv.org/pdf/2406.17557v1), Jun. 25 2024.
- [Entropy Law: The Story Behind Data Compression and LLM Performance](https://arxiv.org/pdf/2407.06645), Jul. 11 2024. `data selection` `learning dynamics`.
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://arxiv.org/pdf/2407.03502), Jul. 3 2024.
- [How NuminaMath Won the 1st AIMO Progress Prize](https://huggingface.co/blog/winning-aimo-progress-prize), Jul. 11 2024. `blogpost`.
- [TAGCOS: Task-agnostic gradient clustered coreset selection for instruction tuning data](https://arxiv.org/pdf/2407.15235), Jul. 21 2024. [code](https://github.com/2003pro/TAGCOS).
- [Consent in Crisis: The Rapid Decline of the AI Data Commons](https://arxiv.org/pdf/2407.14933), Jul. 20 2024.
- [Open Artificial Knowledge](https://oakdataset.org/), Jul. 19 2024
- [Programming every example: Lifting pretraining data quality like experts at scale](https://arxiv.org/pdf/2409.17115), Sep. 25 2024.
- [CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation](https://arxiv.org/pdf/2409.02098), Sep. 3 2024.
- [Data-Prep-Kit: getting your data ready for LLM application development](https://arxiv.org/pdf/2409.18164), 2024.
- [A Little Human Data Goes A Long Way](https://arxiv.org/pdf/2410.13098), Oct. 17 2024.
- [DEM: Distribution Edited Model for Training with Mixed Data Distributions](https://arxiv.org/pdf/2406.15570), Nov. 5 2024.
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://www.microsoft.com/en-us/research/uploads/prod/2024/07/AgentInstruct.pdf), [blogpost](https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/), Jul. 2024.
- [Not all tokens are what you need for pretraining](https://openreview.net/pdf?id=0NMzBwqaAJ), NeurIPS 2024 best paper runner up. [code](https://github.com/microsoft/rho).
- [GREATS: Online selection of high-quality data for llm training in every iteration](https://openreview.net/pdf/ac1fcb465f6684c753ea5fbbef2742c636cf326a.pdf), NeurIPS 2024.
- [RedStone: Curating general, code, math, and QA data for large language models](https://arxiv.org/pdf/2412.03398), Dec. 4 2024. [code](https://github.com/microsoft/redstone).

### Evaluation

- [Evaluating language models as risk scores](https://arxiv.org/pdf/2407.14614), Jul. 19 2024.
- [Training on the test task confounds evaluation and emergence](https://arxiv.org/pdf/2407.07890?), Jul. 10 2024. [code](https://github.com/socialfoundations/training-on-the-test-task).
- [Language model developers should report train-test overlap](https://arxiv.org/abs/2410.08385), Oct. 10 2024. [code](https://github.com/stanford-crfm/data-overlap).
- [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/pdf/2411.00640), Nov. 1 2024.
- [Evaluating Generative AI Systems is a Social Science Measurement Challenge](https://arxiv.org/pdf/2411.10939), Nov. 7 2024.

#### Data contamination

- [Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation](https://arxiv.org/pdf/2406.14644), Jun. 20 2024.
- [Accuracy is not all you need](https://arxiv.org/pdf/2407.09141), Jul. 12 2024.
  - The paper dubs synthetic data from LLMs as generative teaching.

#### Benchmark

- [From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline](https://arxiv.org/pdf/2406.11939), Jun. 17 2024. `benchmark`.
- [Training on the Test Task Confounds Evaluation and Emergence](https://arxiv.org/pdf/2407.07890), Jul. 10 2024.
  - _"We argue that the seeming superriority of one model family over another may be explained by a different degree of training on the test task."_
- [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/pdf/2403.07008), May 28 2024.
- [Benchmarking Complex Instruction-Following with Multiple Constraints Composition](https://arxiv.org/pdf/2406.14491), Jul. 4 2024. [github](https://github.com/thu-coai/ComplexBench).
- [GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2402.19255), Jul. 2 2024.
- [metabench A Sparse Benchmark to Measure General Ability in Large Language Models](https://arxiv.org/pdf/2407.12844), Jul. 4 2024. [code](https://github.com/adkipnis/metabench).
- [AutoBencher: Creating Salient, Novel, Difficult Datasets for Language Models](https://arxiv.org/abs/2407.08351), Jul. 11 2024.
- [Benchmark agreement testing done right: A guide to llm benchmark evaluation](https://arxiv.org/pdf/2407.13696), Jul. 18 2024.
- [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/pdf/2408.08926), Aug. 15 2024.
- [Benchmarking Large Language Models for Math Reasoning Tasks](https://arxiv.org/pdf/2408.10839), Aug. 20 2024.
- [Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization](https://arxiv.org/pdf/2409.18433), Sep. 27 2024.
- [MLE-Bench: Evaluating machine learning agents on machine learning engineering](https://arxiv.org/pdf/2410.07095), Oct. 9 2024. `openai`.
- [HARDMath: A benchmark dataset for challenging problems in applied mathematics](https://arxiv.org/pdf/2410.09988), Oct. 13 2024.

#### Eval toolkit

- [Foundation Model Evaluations Library](https://github.com/aws/fmeval), [paper](https://arxiv.org/pdf/2407.12872), Jul. 15 2024.
- [UltraEval](https://github.com/OpenBMB/UltraEval).
- [simple-evals](https://github.com/openai/simple-evals), OpenAI simple-evals.

### Agent

- [Agentic Workflow新范式，基于大语言模型的工作流、业务流程、智能体大融合【附十篇相关论文】](https://mp.weixin.qq.com/s/i9QB_OtUboHnoZOKn-oKmA), Aug. 5 2024.
- [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/pdf/2406.04692), Jun. 7 2024. `agent`.
- [TextGrad: Automatic "Differentiation" via Text](https://arxiv.org/abs/2406.07496), Jun. 11 2024. `agent`.
- [Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence](https://arxiv.org/pdf/2407.07061), Jul. 10 2024.
- [On the Design and Analysis of LLM-Based Algorithms](https://arxiv.org/pdf/2407.14788), Jul. 20 2024.
- [Recursive Introspection: Teaching Language Model Agents How to Self-Improve](https://arxiv.org/pdf/2407.18219), Jul. 25 2024.
- [ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems](https://arxiv.org/pdf/2408.02248), Aug. 5 2024.
- [ChatDev: Communicative Agents for Software Development](https://arxiv.org/pdf/2307.07924), Jun. 5 2024.
- [OpenDevin: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/pdf/2407.16741), Jul. 23 2024.
- [RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation](https://arxiv.org/pdf/2408.02545), Aug. 5 2024. [code](https://github.com/IntelLabs/RAGFoundry).
- [Automated Design of Agentic Systems](https://arxiv.org/pdf/2408.08435), Aug. 15 2024. [code](https://github.com/ShengranHu/ADAS).
- [Agent Workflow Memory](https://arxiv.org/pdf/2409.07429), Sep. 11 2024.
- [xLAM: A Family of Large Action Models to Empower AI Agent Systems](https://arxiv.org/pdf/2409.03215), Sep. 5 2024.
- [The Impact of Element Ordering on LM Agent Performance](https://arxiv.org/pdf/2409.12089), Sep. 19 2024.
- [MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation](https://arxiv.org/pdf/2310.03302), Apr. 14 2024.
- [AgentScope: A Flexible yet Robust Multi-Agent Platform](https://arxiv.org/pdf/2402.14034), May 20 2024. [github](https://github.com/modelscope/agentscope).
- [TextGrad: Automatic “Differentiation” via Text](https://arxiv.org/pdf/2406.07496), Jun. 11 2024. [github](https://github.com/zou-group/textgrad).
- [AFlow: Automating Agentic Workflow Generation](https://arxiv.org/abs/2410.10762), Oct. 14 2024.
- [AFlow: Automatic agentic workflow generation](https://arxiv.org/pdf/2410.10762), Oct. 14 2024. [code](https://github.com/geekan/MetaGPT).
- [Agent skill acquisition for large language models via CycleQD](https://arxiv.org/pdf/2410.14735), Nov. 27 2024. [code](https://github.com/SakanaAI/CycleQD).
- [Commit0: Library generation from scratch](https://arxiv.org/pdf/2412.01769), Dec. 2 2024.
 [code](https://github.com/commit-0/commit0).
[Magentic-One: A generalist multi-agent system for solving complex tasks](https://arxiv.org/pdf/2411.04468), Nov. 7 2024. [blogpost](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/).
- [Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs](https://arxiv.org/pdf/2406.16218), Nov. 1 2024. [code](https://microsoft.github.io/Trace/).

### Multi-modality

- [A Practitioner’s Guide to Continual Multimodal Pretraining](https://arxiv.org/pdf/2408.14471), Aug. 26 2024.
- [Visual agents as fast and slow thinker](https://arxiv.org/pdf/2408.08862), Aug. 16 2024.
- [NVLM: Open Frontier-Class Multimodal LLMs](https://nvlm-project.github.io/), Sep. 17 2024.
- [From generalist to specialist: Adapting vision language models via task-specific visual instruction tuning](https://arxiv.org/pdf/2410.06456), Oct. 9 2024.


