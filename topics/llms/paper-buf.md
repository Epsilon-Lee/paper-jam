
## LLMs related

### Topics

> Some big topics I really care about.

- [LLM safety](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/llm_safety.md).
- [LLM interpretability](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/llm_interpretability.md).
- [Knowledge updating](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/knowledge_updating.md).
- [Training pipeline](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/training_pipeline.md).
- [Scaling laws](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/scaling_laws.md).
- [Reasoning](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/reasoning.md).
- [Data curation and mixing](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/data_curation_and_mixing.md).
- [Tech. reports](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/tech_reports.md).
- [Agent](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/agent.md).
- [Long-context](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/long_context.md).
- [LLMs-as-judge](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/llms_as_judge.md).
- [Prompting techniques](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/prompting.md).
- [Evaluation](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/eval.md).
- [In-context learning](https://github.com/Epsilon-Lee/paper-jam/blob/main/topics/llms/topics/icl.md).

### Buffered papers

- [Position: Levels of AGI for Operationalizing Progress on the Path to AGI](https://openreview.net/pdf?id=0ofzEysK2D), ICML 2024.
- [WILDCHAT: 1M CHATGPT INTERACTION LOGS IN THE WILD](https://arxiv.org/pdf/2405.01470), May 2 2024.
  - It is interesting to know the data distribution of queries to ChatGPT.
- [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/pdf/2406.12031), Jun. 2024. `tabular llm`.
- [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/pdf/2406.11931), Jun. 17 2024. `tech report`.
- [Infinite Limits of Multi-head Transformer Dynamics](https://arxiv.org/pdf/2405.15712), May 24 2024. `learning dynamics`.
- [What variables affect out-of-distribution generalization in pretrained models?](https://arxiv.org/pdf/2405.15018), May 23 2024.
- [A language model's guide through latent space](https://arxiv.org/pdf/2402.14433), Feb. 22 2024.
- [Same pre-training loss, better downstream: Implicit bias matters for language models](https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf), ICML 2023.
- [Does learning the right latent variables necessarily improve in-context learning?](https://arxiv.org/pdf/2405.19162), May 29 2024.
- [Towards an empirical understanding of MoE design choices](https://arxiv.org/pdf/2402.13089), Feb. 20 2024. `moe`.
- [StructLM: Towards Building Generalist Models for Structured Knowledge Grounding](https://arxiv.org/pdf/2402.16671), Apr. 24 2024. `structure knowledge`.
- [Chronos: Learning the Language of Time Series](https://arxiv.org/pdf/2403.07815), May 2 2024.
- [On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability](https://arxiv.org/pdf/2405.16845), May 27 2024.
- [Theoretical Analysis of Weak-to-Strong Generalization](https://arxiv.org/pdf/2405.16043), May 25 2024.
- [Scaling and evaluating sparse autoencoders](https://arxiv.org/pdf/2406.04093), Jun. 6 2024.
- [Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe](https://arxiv.org/pdf/2406.04165), Jun. 6 2024. `llm for embedding`.
- [miniCodeProps: a Minimal Benchmark for Proving Code Properties](https://arxiv.org/pdf/2406.11915), Jun. 16 2024. `coding` `benchmark`.
- [QOG:Question and Options Generation based on Language Model](https://arxiv.org/pdf/2406.12381), Jun. 18 2024. `data synthesis`.
- [Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models](https://arxiv.org/pdf/2403.19340), Mar. 28 2024. `data pipeline`.
- [Can language model explain their own classification behavior?](https://arxiv.org/pdf/2405.07436), May 13 2024.
- [Language models (mostly) know what they know](https://arxiv.org/abs/2207.05221), Jul. 11 2022.
- [xVal: A Continuous Number Encoding for Large Language Models](https://arxiv.org/abs/2310.02989), Oct. 4 2023.
- [Memory mosaics](https://arxiv.org/pdf/2405.06394), May 10 2024.
- [Is model collapse inevitable? Breaking the curse of recursion by accumulating real and synthetic Data](https://arxiv.org/pdf/2404.01413), Apr. 29 2024. `synthetic data`.
  - _"accumulating the successive generations of synthetic data alongside the original data avoids model collapse"_
- [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/pdf/2402.18563), Feb. 28 2024.
- [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://arxiv.org/pdf/2403.06833), Jun. 3 2024.
- [Data-Centric AI in the Age of Large Language Models](https://arxiv.org/pdf/2406.14473), Jun. 20 2024. `data-centric`.
- [FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery](https://arxiv.org/pdf/2211.08316), May 11 2023.
- [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/pdf/2402.03049), Mar. 21 2024.
- [Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization](https://arxiv.org/pdf/2405.15071), May 27 2024. `mechanistic interpretability`.
- [Theoretical Analysis of Weak-to-Strong Generalization](https://arxiv.org/pdf/2405.16043), May 25 2024.
- [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804), Feb. 6 2024.
- [Machine learning and information theory concepts towards an AI mathematician](https://www.ams.org/journals/bull/2024-61-03/S0273-0979-2024-01839-4/S0273-0979-2024-01839-4.pdf), May 15 2024. `math reasoning`.
- [Monitoring Latent World States in Language Models with Propositional Probes](https://arxiv.org/pdf/2406.19501), Jun. 27 2024.
- [End-To-End Causal Effect Estimation from Unstructured Natural Language Data](https://arxiv.org/pdf/2407.07018), Jul. 9 2024. `causal inference x llms`.
- [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/pdf/2407.04620), Jul. 5 2024. `architectural inductive bias`.
- [How Does Quantization Affect Multilingual LLMs?](https://arxiv.org/pdf/2407.03211), Jul. 3 2024. `quantization`.
- [Chain-of-Thought Reasoning without Prompting](https://arxiv.org/pdf/2402.10200), May 23 2024. `reasoning`.
- [Transformer Alignment in Large Language Models](https://arxiv.org/pdf/2407.07810), Jul. 10 2024.
- [SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/pdf/2407.09025), Jul. 12 2024. `structured knowledge`.
- [LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data](https://arxiv.org/pdf/2407.11418), Jul. 16 2024.
- [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/pdf/2311.17035), Nov. 28 2023. `data extraction`.
- [The Foundations of Tokenization: Statistical and Computational Concerns](https://arxiv.org/pdf/2407.11606), Jul. 16 2024.
- [Representing Rule-based Chatbots with Transformers](https://arxiv.org/pdf/2407.10949), Jul. 15 2024.
- [Learning to Compile Programs to Neural Networks](https://arxiv.org/pdf/2407.15078), Jul. 21 2024.
- [Faithfulness Measurable Masked Language Models](https://arxiv.org/abs/2310.07819), May 9 2024.
- [Fast Matrix Multiplications for Lookup Table-Quantized LLMs](https://arxiv.org/pdf/2407.10960), Jul. 15 2024.
- [On the Benefits of Rank in Attention Layers](https://arxiv.org/pdf/2407.16153), Jul. 24 2024.
  - The question to tackle: _"hyperparameters such as the rank of the attention matrices and the number of heads are scaled nearly the same way in all realizations of this architecture, without theoretical justification"_
- [Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](https://arxiv.org/pdf/2407.16607), Jul. 23 2024.
- [Internal Consistency and Self-Feedback in Large Language Models](https://arxiv.org/pdf/2407.14507), Jul. 19 2024.
- [u-µP: The Unit-Scaled Maximal Update Parametrization](https://arxiv.org/pdf/2407.17465), Jul. 24 2024.
- [Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications](https://arxiv.org/pdf/2407.19262), Jul. 27 2024. `learning dynamics`.
- [Do Language Models Have a Critical Period for Language Acquisition?](https://arxiv.org/pdf/2407.19325), Jul. 27 2024. `learning dynamics`.
- [From Pre-training Corpora to Large Language Models: What Factors Influence LLM Performance in Causal Discovery Tasks?](https://arxiv.org/pdf/2407.19638), Jul. 29 2024. `knowledge extraction`, `causal knowledge`.
- [Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences](https://arxiv.org/pdf/2407.09499), Jun. 12 2024. `synthetic data` `theory`.
- [Can LLMs predict the convergence of Stochastic Gradient Descent?](https://arxiv.org/pdf/2408.01736), Aug. 3 2024.
- [STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs](https://arxiv.org/pdf/2408.01803), Aug. 3 2024. `efficient llm`.
- [Self-Taught Evaluators](https://arxiv.org/pdf/2408.02666), Aug. 8 2024. `self-eval`.
- [A Survey of Mamba](https://arxiv.org/pdf/2408.01129), Aug. 2 2024.
- [Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities](https://arxiv.org/pdf/2408.07666), Aug. 14 2024.
- [A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning](https://arxiv.org/pdf/2408.07057), Aug. 13 2024.
- [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/pdf/2408.06292), Aug. 12 2024.
- [Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters](https://arxiv.org/pdf/2408.04093), Aug. 9 2024.
- [Natural Language Outlines for Code: Literate Programming in the LLM Era](https://arxiv.org/pdf/2408.04820), Aug. 9 2024.
- [Can a Bayesian Oracle Prevent Harm from an Agent?](https://arxiv.org/pdf/2408.05284), Aug. 9 2024.
- [Low-Rank Approximation, Adaptation, and Other Tales](https://arxiv.org/pdf/2408.05883), Aug. 12 2024.
- [Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models](https://arxiv.org/pdf/2408.08210), Aug. 15 2024.
- [BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts](https://arxiv.org/pdf/2408.08274), Aug. 15 2024.
- [Out-of-Distribution Learning with Human Feedback](https://arxiv.org/pdf/2408.07772), Aug. 14 2024.
- [Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models](https://arxiv.org/pdf/2408.10189), Aug. 19 2024.
- [KAN 2.0: Kolmogorov-Arnold Networks Meet Science](https://arxiv.org/pdf/2408.10205), Aug. 19 2024.
- [Learning Randomized Algorithms with Transformers](https://arxiv.org/pdf/2408.10818), Aug. 20 2024.
- [Classifier-Free Guidance is a Predictor-Corrector](https://arxiv.org/pdf/2408.09000), Aug. 23 2024.
- [What can Large Language Models Capture about Code Functional Equivalence?](https://arxiv.org/pdf/2408.11081), Aug. 20 2024.
- [Large Language Models for Supply Chain Optimization](https://arxiv.org/pdf/2307.03875), Jul. 13 2023.
  - [HarnessingAI and LLMs to Revolutionize Retail and Supply Chain Management](https://www.pacificdataintegrators.com/hubfs/Website-Whitepapers/Harnessing%20AI%20and%20LLMs%20to%20Revolutionize%20Retail%20and%20Supply%20Chain%20Management-PDF.pdf).
  - [OptiGuide](https://github.com/microsoft/OptiGuide).
  - [Generative AI In Supply Chain](https://www.alvarezandmarsal.com/sites/default/files/article/pdf/Generative%20AI%20in%20Supply%20Chain%20Report%20-%20Compressed%20version.pdf).
- [Performative Prediction on Games and Mechanism Design](https://arxiv.org/pdf/2408.05146), Aug. 9 2024.
  - _"we propose FUSE (Flexible Unification of Semantic Embeddings), an inexpensive approach to approximating an adapter layer that maps from one model's textual embedding space to another, even across different tokenizers."_
- [COGEN: Learning from Feedback with Coupled Comprehension and Generation](https://arxiv.org/pdf/2408.15992), Aug. 28 2024.
- [SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning](https://arxiv.org/pdf/2408.05517), Aug. 19 2024.
- [The Unbearable Slowness of Being](https://arxiv.org/pdf/2408.10234), Aug. 3 2024.
- [An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models](https://arxiv.org/pdf/2408.00724), Aug. 1 2024.
  - _"We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped wiht more sophisticated decoding algorithms in budget-constrained scenarios, e.g. on-devices, to enhance problem-solving accuracy."_
  - [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/pdf/2406.16838), Jun. 24 2024.
- [Reframing Data Value for Large Language Models Through the Lens of Plausability](https://arxiv.org/pdf/2409.00284), Aug. 30 2024.
- [Extracting Paragraphs from LLM Token Activations](https://arxiv.org/pdf/2409.06328), Sep. 10 2024.
- [Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving](https://arxiv.org/pdf/2405.12205), May 20 2024.
- [Instruct-SkillMix: A powerful pipeline for llm instruction tuning](https://www.arxiv.org/pdf/2408.14774v2), Sep. 9 2024.
- [MambaByte: Token-free Selective State Space Model](https://arxiv.org/pdf/2401.13660), Aug. 9 2024.
- [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/pdf/2402.04333), Jun. 13 2024.
- [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/pdf/2409.12917), Sep. 19 2024.
- [ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty](https://arxiv.org/pdf/2408.14339), Aug. 26 2024.
- [GRIN: GRadient-INformed MoE](https://arxiv.org/pdf/2409.12136), Sep. 18 2024.
- [QWen2.5-Math technical report: Towards mathematical expert model via self-improvement](https://arxiv.org/pdf/2409.12122), Sep. 18 2024.
- [A Controlled Study on Long Context Extension and Generalization in LLMs](https://arxiv.org/pdf/2409.12181), Sep. 23 2024. `long-context`.
- [Apple Intelligence Foundation Language Models](https://arxiv.org/pdf/2407.21075), Jul. 29 2024.
- [Can Large Language Models Unlock Novel Scientific Research Ideas?](https://arxiv.org/pdf/2409.06185), Sep. 10 2024.
- [Quantifying Emergence in Neural Networks: Insights from Pruning and Training Dynamics](https://arxiv.org/pdf/2409.01568), Sep. 3 2024.
- [Hypothesizing Missing Causal Variables with LLMs](https://arxiv.org/pdf/2409.02604), Sep. 4 2024.
- [NUDGE: Lightweight non-parametric fine-tuning of embeddings for retrieval](https://arxiv.org/pdf/2409.02343), Sep. 4 2024.
- [Unforgettable Generalization in Language Models](https://arxiv.org/pdf/2409.02228), Sep. 3 2024.
- [Inductive Learning of Logical Theories with LLMs: A Complexity-graded Analysis](https://arxiv.org/pdf/2408.16779), Aug. 15 2024.
- [Configurable Foundation Models: Building LLMs from a Modular Perspective](https://arxiv.org/pdf/2409.02877), Sep. 4 2024.
- [Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data](https://arxiv.org/pdf/2409.00096), Aug. 27 2024.
- [A Formal Hierarchy of RNN Architectures](https://blog.allenai.org/a-formal-hierarchy-of-rnn-architectures-94c9d47566b5), Apr. 22 2020.
- [Proof Automation with Large Language Models](https://arxiv.org/pdf/2409.14274), Sep. 22 2024.
- [Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers](https://arxiv.org/pdf/2409.04109), Sep. 6 2024.
- [On the Computational Benefit of Multimodal Learning](https://proceedings.mlr.press/v237/lu24a/lu24a.pdf), alt 2024.
- [Reranking Laws for Language Generation: A Communication-Theoretic Perspectiv](https://arxiv.org/pdf/2409.07131), Sep. 11 2024.
- [Improving pretraining data using perplexity correlations](https://arxiv.org/pdf/2409.05816), Sep. 9 2024.
- [Explaining Datasets in Words: Statistical Models with Natural Language Parameters](https://arxiv.org/abs/2409.08466), Sep. 13 2024.
- [Language Models “Grok” to Copy](https://www.arxiv.org/pdf/2409.09281), Sep. 14 2024.
- [Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models](https://arxiv.org/pdf/2409.17455), Sep. 26 2024.
- [Order of Magnitude Speedups for LLM Membership Inference](https://arxiv.org/pdf/2409.14513), Sep. 24 2024.
- [Estimating Wage Disparities Using Foundation Models](https://www.arxiv.org/pdf/2409.09894), Sep. 15 2024.
- [Safety Alignment Should Be Made More Than Just a Few Tokens Deep](https://arxiv.org/pdf/2406.05946), Jun. 10 2024.
- [Can Models Learn Skill Composition from Examples?](https://openreview.net/pdf?id=YEEsRgkvnU), NeurIPS 2024.
- [See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses](https://arxiv.org/abs/2408.08978#), Aug. 16 2024.
- [Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models](https://arxiv.org/abs/2408.06663), Aug. 13 2024.
- [A Taxonomy for Data Contamination in Large Language Models](https://arxiv.org/abs/2407.08716), Jul. 11 2024.
- [LLM Pruning and Distillation in Practice: The Minitron Approach](https://d1qx31qr3h6wln.cloudfront.net/publications/minitron_tech_report.pdf), Aug. 21 2024.
- [Realistic evaluation of model merging for compositional generalization](https://arxiv.org/pdf/2409.18314), Sep. 26 2024.
- [Transformer Memory as a Differentiable Search Index](https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf), NeurIPS 2022. [github](https://github.com/ArvinZhuang/DSI-transformers).
  - [DSI++: Updating Transformer Memory with New Documents](https://aclanthology.org/2023.emnlp-main.510.pdf), EMNLP 2023.
- [Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers](https://arxiv.org/pdf/2409.20537), Sep. 30 2024. `robotics foundation model`. NeurIPS 2024.
  - _TL;DR: "HPT aligns different embodiment to a shared latent space and investigates the scaling behaviors in policy learning. Put a scalable transformer in the middle of your policy and don't train from scratch!"_
- [Geometric signatures of compositionality across a language model's lifetime](https://arxiv.org/pdf/2410.01444), Oct. 7 2024. `compositionality`.
- [Dont' cut corners: Exact conditions for modularity in biologically inspired representations](https://arxiv.org/pdf/2410.06232v1), Oc.t 8 2024. [code](https://github.com/kylehkhsu/dont-cut-corners/). `compositionality`.
- [Differential transformers](https://arxiv.org/pdf/2410.05258), Oct. 7 2024.
- [LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch](https://arxiv.org/pdf/2410.13213), Oct. 17 2024.
- [A simple baseline for predicting events with auto-regressive tabular transformers](https://arxiv.org/pdf/2410.10648), Oct. 14 2024.
- [DARE the Extreme Ð: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://arxiv.org/pdf/2410.09344), Oct. 12 2024. `pruning`.
- [LoLCATs: On Low-Rank Linearizing of Large Language Models](https://arxiv.org/pdf/2410.10254), Oct. 14 2024. [code](https://github.com/HazyResearch/lolcats).
- [Machines of Loving Grace](https://darioamodei.com/machines-of-loving-grace), Oct. 2024.
- [Predicting Emergent Capabilities by Finetuning](https://openreview.net/pdf?id=vL8BIGuFTF), COLM 2024.
- [End-to-End Ontology Learning with Large Language Models](https://arxiv.org/pdf/2410.23584), Oct. 31 2024. [code](https://github.com/andylolu2/ollm).
- [Language Models as Causal Effect Generators](https://arxiv.org/pdf/2411.08019), Nov. 12 2024. [code](https://github.com/lbynum/sequence-driven-scms).
- [On the limits of language generation: Trade-offs between hallucination and mode collapse](https://arxiv.org/pdf/2411.09642), Nov. 14 2024.
- [SEQ-VCR: Preventing collapse in intermediate transformer representations for enhanced reasoning](https://arxiv.org/pdf/2411.02344), Nov. 4 2024. [code](https://github.com/rarefin/seq_vcr).
- [Efficient Alignment of Large Language Models via Data Sampling](https://arxiv.org/pdf/2411.10545), Nov. 15 2024.
- [Language-to-Code Translation with a Single Labeled Example](https://aclanthology.org/2024.emnlp-main.462.pdf), 2024.
- [Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis](https://aclanthology.org/2024.findings-emnlp.547.pdf), EMNLP 2024. [code](https://github.com/cogito233/causal-sa).
- [Visual autoregressive modeling: Scalable image generation via next-scale prediction](https://arxiv.org/pdf/2404.02905), Jun. 10 2024.
- [Machine unlearning doesn't do what you think: Lessons for generative AI policy, research and practice](https://arxiv.org/pdf/2412.06966), Dec. 9 2024. `machine unlearning`.
- [Normalizing flows are capable generative models](https://arxiv.org/pdf/2412.06329), Dec. 10 2024. [code](https://github.com/apple/ml-tarflow).
- [Toward ai-driven digital organism: A system of multiscale foundation models for predicting, simulating and programming biology at all levels](https://www.cs.cmu.edu/~epxing/papers/2025/AIDO.pdf), Nov. 26 2024.
- [Memory Layers at Scale](https://arxiv.org/pdf/2412.09764), Dec. 12 2024. [code](https://github.com/facebookresearch/memory).
- [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/pdf/2412.13663), Dec. 2024.
- [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/pdf/2312.02780), Dec. 5 2023
  - [A Universal Law of Robustness via Isoperimetry](https://arxiv.org/pdf/2105.12806), Dec. 23 2024.
- [LearnLM: Improving Gemini for Learning](https://arxiv.org/pdf/2412.16429), Dec. 25 2024.
- [The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources](https://openreview.net/pdf?id=tH1dQH20eZ), TMLR 2024.
- [Complexity control facilitates reasoning-based compositional generalization in transformers](https://arxiv.org/pdf/2501.08537), Jan. 15 2025.
- [Foundations of Large Language Models](https://arxiv.org/pdf/2501.09223), Jan. 16 2025.
- [Enhancing Lexicon-Based Text Embeddings with Large Language Models](https://arxiv.org/pdf/2501.09749), Jan. 16 2025.
- [Critique fine-tuning: Learning to critique is more effective than learning to imitate](https://arxiv.org/pdf/2501.17703), Jan. 29 2025.
- [Large language models as optimizers](https://arxiv.org/pdf/2309.03409), Apr. 15 2024.
- [Connecting large language models with evolutionary algorithms yields powerful prompt optimizers](https://arxiv.org/pdf/2309.08532), Feb. 27 2024.
- [Tuning LLM judges hyperparameters](https://arxiv.org/pdf/2501.17178), Jan. 24 2025.
- [LLM-Rubric: A multidimensional, calibrated approach to automated evaluation of natural language texts](https://arxiv.org/pdf/2501.00274), Dec. 31 2025. [code](https://github.com/microsoft/llm-rubric).
- [Great models think alike and this undermines AI oversight](https://arxiv.org/pdf/2502.04313), Feb. 6 2025. [code](https://github.com/model-similarity/lm-similarity).
- [Algorithmic causal structure emerging through compression](https://arxiv.org/pdf/2502.04210), Feb. 6 2025.
- [Reflection-window decoding: Text generation with selective refinement](https://arxiv.org/pdf/2502.03678), Feb. 5 2025.
- [Verify with caution: The pitfalls of relying on imperfect factuality metrics](https://arxiv.org/pdf/2501.14883), Jan. 30 2025.
- [SmolLM2: When smol goes big - Data-centric training of small language model](https://arxiv.org/pdf/2502.02737), Feb. 4 2025.
  - _"we release both SmolLM2 as well as all of the datasets we prepared in the course of this project"_
- [ExLM: Rethinking the impact of MASK tokens in masked language models](https://arxiv.org/pdf/2501.13397), Jan. 23 2025.
- [Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection](https://arxiv.org/pdf/2502.06042), Feb. 9 2025.
- [LM2: Large Memory Models](https://arxiv.org/pdf/2502.06049), Feb. 9 2025.
- [Economics of sourcing human data](https://arxiv.org/pdf/2502.07732), Feb. 11 2025.
- [Auditing prompt caching in language model APIs](https://arxiv.org/pdf/2502.07776), Feb. 11 2025.
- [Turning up the heat: Min-p sampling for creative and coherent LLM outputs](https://arxiv.org/pdf/2407.01082), Oct. 13 2024.
- [Automated capability discovery via foundation model self-exploration](https://arxiv.org/pdf/2502.07577), Feb. 11 2025.
  - [Quality-diversity through AI feedback](https://openreview.net/pdf?id=owokKCrGYr), ICLR 2024.
- [The hyperfitting phenomenon: Sharpening and stabilizing LLMs for open-ended text generation](https://arxiv.org/pdf/2412.04318), Dec. 5 2024.
- [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/pdf/2502.09604), Feb. 13 2025.
- [Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs](https://arxiv.org/pdf/2502.08640), Feb. 12 2025.
- [Rewind-to-Delete: Certified machine unlearning for nonconvex functions](https://arxiv.org/pdf/2409.09778), Jan. 31 2025.
  - Related theoretic works on machine unlearning
    - [Remember What You Want to Forget: Algorithms for Machine Unlearning](https://arxiv.org/abs/2103.03279), 2021.
- [Discrepancies are virtue: Weak-to-strong generalization through lens of intrinsic dimensions](https://arxiv.org/pdf/2502.05075), Feb. 7 2025.
  - [Scalable oversight and weak-to-strong generalization: Compatible approaches to the same problem](https://www.alignmentforum.org/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization), Dec. 2023.
  - [A guide to iterated amplification & debate](https://www.alignmentforum.org/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate), Nov. 16 2020.
- [Self-regulation and requesting interventions](https://arxiv.org/pdf/2502.04576), Feb. 7 2025.
- [Active task disambiguation with LLMs](https://arxiv.org/pdf/2502.04485), Feb. 6 2025.
- [Aligning black-box language models with human judgements](https://arxiv.org/pdf/2502.04997), Feb. 7 2025.
- [Large language diffusion models](https://arxiv.org/pdf/2502.09992), Feb. 14 2025. [code](https://github.com/ML-GSAI/LLaDA).
- [Independence tests for language models](https://arxiv.org/pdf/2502.12292), Feb. 17 2025.
- [From RAG to memory: Non-parametric continual learning for large language models](https://arxiv.org/pdf/2502.14802), Feb. 20 2025. [code](https://github.com/OSU-NLP-Group/HippoRAG).
- [Towards an ai co-scientist](https://arxiv.org/pdf/2502.18864), Feb. 26 2025.
- [An overview of large language models for statisticians](https://arxiv.org/pdf/2502.17814), Feb. 25 2025.
- [Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction](https://arxiv.org/pdf/2502.17541), Feb. 24 2025. [code](https://github.com/MichalBravansky/dataset-featurization).
- [Build a Minimal Transformer from Scratch](https://www.k-a.in/llm3.html), `blogpost`.
- [SuperBPE: Space travel for language models](https://arxiv.org/pdf/2503.13423), Mar. 17 2025. [code](https://superbpe.github.io/).
- [LLMs can generate a better answer by aggregating their own responses](https://arxiv.org/pdf/2503.04104), Mar. 6 2025.
- [Empirical privacy variance](https://arxiv.org/pdf/2503.12314), Mar. 26 2025.
- [Decision tree induction through LLMs via semantically-aware evolution](https://arxiv.org/pdf/2503.14217), Mar. 18 2025.
- [Value profiles for encoding human variation](https://arxiv.org/pdf/2503.15484), Mar. 19 2025.
  - _"Modelling human variation in rating tasks is crucial for enabling AI systems for personalization, pluralistic model alignment, and computational social science."_
  - _"We propose representing individuals using value profiles - natural language descriptions of underlying values compressed from in-context demonstrations - alogn with a steerable decoder model to estiamte ratings conditioned on a value profile or other rater information."_
  - This value profiles remind me of constitutions, though constitutions mean common principles that guides majority of people and value profile are principles guiding individual behavior.
- [Cost-efficient collaboration between on-device and cloud language models](https://openreview.net/pdf?id=uRPdQAaWBG), ICLR 2025 workshop.
- [Emergent abilities in large language models: A survey](https://arxiv.org/pdf/2503.05788), Feb. 28 2025.
- [UNDO: UNderstanding Distillation as Optimization](https://arxiv.org/pdf/2504.02521), Arp. 3 2025.
- [Anthropic Education Report: How University Students Use Claude](https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude), Apr. 8 2025.
- [Weak-to-strong generalization even in random feature networks, provably](https://arxiv.org/pdf/2503.02877), Mar. 4 2025.
- [(How) do language models track state?](https://arxiv.org/pdf/2503.02854), Mar. 11 2025. [code](https://github.com/belindal/state-tracking).
- [WildVis: Open source visualizer for million-scale chat logs in the wild](https://arxiv.org/pdf/2409.03753), Sep. 9 2024.
- [Attention mechanism, max-affine partition and universal approximation](https://arxiv.org/pdf/2504.19901), Apr. 28 2025.
- [From retrieval to reasoning: Advancing ai agents for knowledge discovery and collaboration](http://i.stanford.edu/~jure/pub/talks2/leskovec-relational-www_keynote-apr25v2.pdf), 2025.
- [AI supply chains: An emerging ecosystem of AI actors, products, and services](https://arxiv.org/pdf/2504.20185), Apr. 28 2025.
- [Lawma: The power of specialization for legal annotation](https://arxiv.org/pdf/2407.16615), Apr. 23 2025.
- [A statistical physics of language model reasoning](https://arxiv.org/pdf/2506.04374), Jun. 4 2025.
- [ENTP: Encoder-only next token prediction](https://arxiv.org/pdf/2410.01600), Feb. 4 2025.
- [Expertise is what we want](https://arxiv.org/pdf/2502.20335), Feb. 27 2025.
  - _"We share an application architecture, the Large Language Expert (LLE), that combines the flexibility and power of LLMs with the interpretability, explainability, and reliability of Expert Systems."_
- [Teasing apart architecture and initial weights as sources of inductive bias in neural networks](https://arxiv.org/pdf/2502.20237), Feb. 27 2025.
- [NeoBERT: A next-generation BERT](https://arxiv.org/pdf/2502.19587), Jun. 6 2025.
- [Correlated errors in large language models](https://arxiv.org/pdf/2506.07962), Jun. 9 2025.
- [Future of work with AI Agents: Auditing automation and augmentation potential across the U.S. workforce](https://arxiv.org/pdf/2506.06576), Jun. 11 2025.
- [Naturalistic computational cognitive science: Towards generalizable models and theories that capture the full range of natural behavior](https://arxiv.org/pdf/2502.20349), Jun. 12 2025.
- [From bytes to ideas: Language modeling with autoregressive U-Nets](https://arxiv.org/pdf/2506.14761), Jun. 17 2025.
- [Should we still pretrain encoders with masked language modeling](https://arxiv.org/pdf/2507.00994), Jul. 1 2025. [code](https://huggingface.co/MLMvsCLM).
- [The landscape of memorization in LLMs: Mechanisms, measurement, and mitigation](https://arxiv.org/pdf/2507.05578), Jul. 8 2025.
- [Generalized linear mode connectivity for transformers](https://arxiv.org/pdf/2506.22712), Jun. 28 2025.

### LLM-based active learning

- [Cold-start data selection for better few-shot language model fine-tuning: A prompt-based uncertainty propagation approach](https://aclanthology.org/2023.acl-long.141.pdf), ACL 2023. [code](https://github.com/yueyu1030/Patron).
- [FreeAL: Towards human-free active learning in the era of large language models](https://aclanthology.org/2023.emnlp-main.896.pdf), EMNLP 2023. [code](https://github.com/Justherozen/FreeAL).
- [Enhancing text classification through LLM-driven active learning and human annotation](https://aclanthology.org/anthology-files/pdf/law/2024.law-1.10.pdf), 2024.
- [ActiveLLM: Large language model-based active learning for textual few-shot scenarios](https://arxiv.org/pdf/2405.10808v1), May 17 2024.
- [From selection to generation: A survey of LLM-based active learning](https://arxiv.org/pdf/2502.11767), May 31 2025.

### LLM pruning

- [Instruction-following pruning for large language models](https://arxiv.org/pdf/2501.02086), Jun. 2 2025.
- [On the creation of narrow AI: Hierarchy and nonlocality of neural network skills](https://arxiv.org/pdf/2505.15811), May 21 2025. [code](https://github.com/ejmichaud/narrow).

### LLMs for traditional nlp tasks

- [Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study](https://arxiv.org/pdf/2304.04339), Feb. 17 2024.
- [Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT](https://arxiv.org/pdf/2302.10198), Mar. 2 2023.
- [Whitening Not Recommended for Classification Tasks in LLMs](https://arxiv.org/pdf/2407.12886), Jul. 16 2024.

### Efficient training and inference

- [Fitting larger networks into memory](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9), Jan. 14 2018. `gradient checkpointing`. [github](https://github.com/cybertronai/gradient-checkpointing). [pytorch](https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb).
- [Triton Puzzles](https://github.com/srush/Triton-Puzzles), Triton tutorial.
- [Flora: Low-Rank Adapters Are Secretly Gradient Compressors](https://arxiv.org/abs/2402.03293), Feb. 5 2024. `efficient training`. [github](https://github.com/BorealisAI/flora-opt). [blogpost](https://www.borealisai.com/research-blogs/pre-training-multi-billion-parameter-llms-on-a-single-gpu-with-flora/#Incorporating_FLORA_into_your_code).
- [A visual guide to quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7common-data-types), Jul. 22 2024.
- [Foundation of large language model compression - Part 1: Weight quantization](https://arxiv.org/pdf/2409.02026), Sep. 2024.

### Multi-modality

- [An introduction to vision-language modeling](https://arxiv.org/pdf/2405.17247), May 27 2024. `tutorial`.
- [A Practitioner’s Guide to Continual Multimodal Pretraining](https://arxiv.org/pdf/2408.14471), Aug. 26 2024.
- [Visual agents as fast and slow thinker](https://arxiv.org/pdf/2408.08862), Aug. 16 2024.
- [NVLM: Open Frontier-Class Multimodal LLMs](https://nvlm-project.github.io/), Sep. 17 2024.
- [From generalist to specialist: Adapting vision language models via task-specific visual instruction tuning](https://arxiv.org/pdf/2410.06456), Oct. 9 2024.


