### Survey

- [A Survey of Pretrained Language Models Based Text Generation](https://arxiv.org/pdf/2201.05273.pdf), Jan. 14 2022.
- [A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models](https://arxiv.org/pdf/2201.05337.pdf), Jan. 14 2022.


### Method

- [Ensembles and Cocktails: Robust Finetuning for Natural Language Generation](https://openreview.net/pdf?id=qXucB21w1C3), `nips2021`
- [A Plug-and-Play Method for Controlled Text Generation](https://arxiv.org/pdf/2109.09707.pdf), `emnlp2021` `text generation` `plug-and-play`
  - given a topic or keyword, a `shift distribution` is added to the language model's step-wise distribution
  - there are many details ignored here, and should be reading into the paper itself
- [Step-Unrolled Denoising Autoencoder for Text Generation](https://arxiv.org/pdf/2112.06749.pdf), Dec. 13 2021.

### Sampling and Searching

- [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate), Mar. 18 2020 `blogpost` from HuggingFace
- [Sampling from Discrete Energy-Based Models with Quality/Efficiency Trade-offs](https://arxiv.org/abs/2112.05702), Dec. 2021 `nipst2021` `workshop`


### Evaluation

- [Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand](https://arxiv.org/pdf/2106.10800.pdf), Dec. 8 2021.
- [Rethinking and Refining the Distinct Metric](https://arxiv.org/pdf/2202.13587.pdf), Feb. 28 2022.
