
### Survey

- [Which BERT? A Survey Organizing Contextualized Encoders](https://arxiv.org/pdf/2010.00854.pdf), Oct. 2 2020. `emnlp2020`
- [Ethical and social risks of harm from Language Models](https://arxiv.org/pdf/2112.04359.pdf), Dec. 8 2021 `deepmind`

### BERTology

- [ELMO] [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365), EMNLP 2018.
- [BERT] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,](https://aclanthology.org/N19-1423.pdf) ACL 2019, and first published at Oct. 11 2018.
- [XLNet] [Cross-lingual Language Model Pretraining](https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf), NIPS 2019.
- [RoBERTa] [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf), ICLR 2020 rejected.
- [ALBERT] [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf), ICLR 2020.
- [ELECTRA] [ELECTRA: Pre-training Text Encoders as Discriminators Rather than Generators](https://arxiv.org/pdf/2003.10555.pdf), ICLR 2020.
- [DeBERTa] [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/pdf/2006.03654.pdf), arXiv Oct. 2021, ICML 2021.

### Hyper-parameter/Training revisited

- [The MultiBERTs: BERT Reproductions for Robustness Analysis](https://openreview.net/forum?id=K0E_F0gFDgA), `iclr2022` submitted

### Few-shot learning

- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf), Oct. 2021, `T0++`
- [MetaICL: Learning to Learn In Context](https://arxiv.org/pdf/2110.15943.pdf), Oct. 29 2021. [code](https://github.com/facebookresearch/MetaICL).
- [ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](https://openreview.net/forum?id=Vzh1BFUCiIX), `iclr22 submit`
- [Quantifying Adaptability in Pre-trained Language Models with 500 Tasks](https://arxiv.org/pdf/2112.03204.pdf), Dec. 6 2021. Jacob Andreas et al.
  - I read this paper [Analysis and Prediction of NLP models via Task Embeddings](https://arxiv.org/pdf/2112.05647.pdf) and I guess there might be some connection that links the task *geography* represented and revealed by task embedding to task composition studied in the `quantifying` paper
- [Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://neurips2021-nlp.github.io/papers/24/CameraReady/NullPrompts___ENLSP_Workshop_Version.pdf), `nips2021`

### Language-specific LLMs or BERT

- [GottBERT: a pure German Language Model](https://arxiv.org/abs/2012.02110), Dec. 3 2020.
- [ERNIE 3.0 TITAN: EXPLORING LARGER-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION](https://arxiv.org/pdf/2112.12731.pdf), Dec. 23 2021 `chinese`

### Prompt-based learning

- [Memory-assisted prompt editing to improve GPT-3 after deployment](https://arxiv.org/pdf/2201.06009.pdf), Jan. 16 2022.
- [ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization](https://arxiv.org/pdf/2201.06910.pdf), Jan. 8 2022. `genetic search`
