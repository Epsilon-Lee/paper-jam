
This is a collection of papers and their summaries about the topic *pre-training explanation*.
The related papers are listed below:
- [Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models](https://openreview.net/forum?id=luO6l9cP6b6), 2021.
- [The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design](https://openreview.net/forum?id=lnEaqbTJIRz), Oct. 9 2021.
- [An Explanation of In-context Learning as Implicit Bayesian Inference](https://arxiv.org/abs/2111.02080), Nov. 3 2021.
- [Impact of Pretraining Term Frequencies on Few-Shot Reasoning](https://arxiv.org/abs/2202.07206), Feb. 15 2022.
- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837), Feb. 22 2022.
- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html), Mar. 8 2022. `anthropic interpretability team`
- [Insights into Pre-training via Simpler Synthetic Tasks](https://arxiv.org/abs/2206.10139), Jun. 21 2022.
- [What Can Transformers Learn In-Context? A Case Study of Simple Function Classes](https://arxiv.org/pdf/2208.01066.pdf), Aug. 1 2022. [Tweet thread](https://twitter.com/tsiprasd/status/1555302289824366592).
