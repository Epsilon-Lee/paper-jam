
This is a collection of papers and their summaries about the topic *pre-training explanation*.
The related papers are listed below:
- [Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models](https://openreview.net/forum?id=luO6l9cP6b6), 2021.
- [The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design](https://openreview.net/forum?id=lnEaqbTJIRz), Oct. 9 2021.
- [An Explanation of In-context Learning as Implicit Bayesian Inference](https://arxiv.org/abs/2111.02080), Nov. 3 2021.
- [Impact of Pretraining Term Frequencies on Few-Shot Reasoning](https://arxiv.org/abs/2202.07206), Feb. 15 2022.
- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837), Feb. 22 2022.
- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html), Mar. 8 2022. `anthropic interpretability team`
- [Insights into Pre-training via Simpler Synthetic Tasks](https://arxiv.org/abs/2206.10139), Jun. 21 2022.
- [What Can Transformers Learn In-Context? A Case Study of Simple Function Classes](https://arxiv.org/pdf/2208.01066.pdf), Aug. 1 2022. [Tweet thread](https://twitter.com/tsiprasd/status/1555302289824366592).
- [Exploring Length Generalization in Large Language Models](https://arxiv.org/pdf/2207.04901.pdf), Jul. 11 2022.
- [Data Distributioonal Properties Drive Emergent In-Context Learning in Transformers](https://arxiv.org/abs/2205.05055), Nov. 17 2022.
- [What learning algorithm is in-context learning? Investigations with linear models](https://arxiv.org/pdf/2211.15661.pdf), Nov. 29 2022. [jax](https://github.com/ekinakyurek/google-research/tree/master/incontext).
- [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846), Mar. 7 2023. [tweet](https://arxiv.org/abs/2303.03846).
- [Identifiability results for multimodal contrastive learning](https://arxiv.org/pdf/2303.09166.pdf), Mar. 16 2023.
- [The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces](https://arxiv.org/pdf/2303.14334.pdf), Mar. 25 2023. `education`.

---

### General LLMs generalization ability understanding

- [Distinguishing rule- and exemplar-based generalization in learning systems](https://proceedings.mlr.press/v162/dasgupta22b/dasgupta22b.pdf), `icml2022`.
- [Transformers generalize differently from information stored in context vs in weights]([Transformers generalize differently from information stored in context vs in weights](https://arxiv.org/abs/2210.05675)), Oct. 11 2022. [tweet](https://twitter.com/FelixHill84/status/1580566903168282624)

---

### Empirical understandings

- xxx

## Appendix

### Interpreting transformers

- [How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding](https://arxiv.org/pdf/2303.04245.pdf), Mar. 7 2023.

### Grokking

- [A tale of two circuits: Grokking as competition of sparse and dense subnetworks](https://arxiv.org/pdf/2303.11873.pdf), Mar. 21 2023.
- [A Simple Explanation for the Phase Transition in Large Language Models with List Decoding](https://arxiv.org/pdf/2303.13112.pdf), Mar. 23 2023.
